%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% STM 2007-04-13  split from previous version
% STM 2007-04-15  remove tools and start rewrite
% STM 2007-04-19  start main update
% STM 2007-05-14  major rewriting
% STM 2007-05-30  some further changes
% STM 2007-06-15  start to bring up to Alpha Patch 1 level
% STM 2007-06-16  add NGC5921 example
% STM 2007-06-29  update for Alpha Patch 1
% STM 2007-08-24  update for Alpha Patch 2
% STM 2007-09-20  start beta update
% STM 2007-10-02  beta version
% STM 2007-10-10  add Jupiter example
% STM 2007-10-10  spell-checked
% STM 2007-10-12  GMs corrections
% STM 2007-10-14  add caltable flow figure
% GvM 2008-02-21  add hanningsmooth text
% GvM 2008-02-22  add uvsub text
% STM 2008-02-25  add polcal text
% STM 2008-03-18  more patch 1 updating
% STM 2008-04-29  fix error re: importvla post-modcomp
% STM 2008-06-11  Patch 2, combine
% STM 2008-09-30  Patch 3 editing start
% STM 2008-10-20  Patch 3 editing start
% STM 2009-01-21  Patch 3.1 
% STM 2009-05-28  Patch 4.0 
% STM 2009-11-16  Release 3.0.0 start, gencal
% STM 2009-12-15  Release 3.0.0 cvel
% GM  2009-12-22  Release 3.0.0 gencal (and other edits)
% JO 2010-03-05 Release 3.0.1 edits
% JO 2010-04-16 Release 3.0.2 editing starts
% JO 2010-10-11 Release 3.1.0 edits
% GM 2010-11-19 Release 3.1.0 edits
% JO 2011-04-?? Release 3.2.0 edits
% GM 2011-05-10 Release 3.2.0 edits
% JO 2011-06-07 added negative channel widths
% JO 2011-10-06 edits for Release 3.3.0
% JO 1012-04-23 edits for Release 3.4.0
% DP 2012-09-21 edits for improved wvrgcal section
% JO 2012-10-18 Release 4.0.0 edits
% GM 2012-10-30 Further r4.0.0 edits, incl lin feed poln
% JO 2013-01-16 fixed evlagains
% JO 2013-04-23 Release 4.1.0 edits
% GM 2013-12-16 Initial v4.2.0 edits
% JO 2013-12-10 release 4.2.0 edits



\chapter{Synthesis Calibration}
\label{chapter:cal}

\begin{wrapfigure}{r}{2.5in}
  \begin{boxedminipage}{2.5in}
     \centerline{\bf Inside the Toolkit:}
     The workhorse for synthesis calibration is the {\tt cb} tool.
  \end{boxedminipage}
\end{wrapfigure}

This chapter explains how to calibrate interferometer
data within the CASA task system.  Calibration is the process
of determining the net complex correction factors that must be 
applied to each visibility in order to make them as close as
possible to what an idealized interferometer would measure, such
that when the data is imaged an accurate picture of the sky
is obtained.  This is not an arbitrary process, and there is
a philosophy behind the CASA calibration methodology (see
\S~\ref{section:cal.flow.philo} for more on this).  For the most part,
calibration in CASA using the tasks is not too different than
calibration in other packages such as AIPS or Miriad, so the user
should not be alarmed by cosmetic differences such as task and
parameter names!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Calibration Tasks}
\label{section:cal.tasks}

{\bf Alert:} The calibration table format changed in CASA
3.4.   CASA 4.2 is the last version that will support the 
{\tt caltabconvert} function that provides conversions from
the pre-3.4 caltable format to the modern format; it will be
removed for CASA 4.3.  In general, it is best to recalculate
calibration using CASA 3.4 or later.

{\bf Alert:} In CASA 4.2 the {\tt gaincurve} and {\tt opacity} 
parameters have been removed from all calibration tasks (as advertised
in 4.1).  These calibration types are supported via the 
{\tt gencal} task.

{\bf Alert:} As part of continuing development of a more flexible and
improved interface for specifying calibration for apply, a new
parameter has been introduced in {\tt applycal} and the solving
tasks: {\tt docallib}.   This parameter toggles between use of the
traditional calibration apply parameters ({\tt gaintable}, 
{\tt gainfield}, {\tt interp}, {\tt spwmap}, and {\tt calwt}),
and a new {\tt callib} parameter which currently provides 
access to the {\em experimental} cal library mechanism, wherein
calibration instructions are stored in a file. 
The default is {\tt docallib=False} in 4.3, and this reveals the 
traditional apply parameters which continue to work as always. 
Since {\tt callib} is still under development and testing, and 
does not yet support all calibration types, general users should 
continue to use the traditional parameters via {\tt docallib=False}.  The
rest of this chapter is written assuming {\tt docallib=False}.


The standard set of {\tt calibration} solving tasks (to produce
calibration tables) are:
\begin{itemize}
   \item {\tt bandpass} --- complex bandpass (B) calibration solving,
      including options for channel-binned or polynomial solutions
      (\S~\ref{section:cal.solve.band}),
   \item {\tt gaincal} --- complex gain (G,T) and delay (K)
     calibration solving, including options for time-binned or spline
     solutions.  
      (\S~\ref{section:cal.solve.gain}),
   \item {\tt polcal} --- polarization calibration including leakage
      and angle
      (\S~\ref{section:cal.solve.pol}),
   \item {\tt blcal} --- {\it baseline-based} complex gain or bandpass
      calibration
      (\S~\ref{section:cal.solve.blcal}).
\end{itemize}

There are helper tasks to create, manipulate, and explore calibration 
tables:
\begin{itemize}
   \item {\tt accum} --- Accumulate incremental calibration solutions
      into a cumulative cal table (\S~\ref{section:cal.tables.accum})
      ({\bf ALERT:} The {\tt accum} task is generally no longer recommended
      for most calibration scenarios.  Please write to the NRAO CASA
      helpdesk  if you need support using {\tt accum}.),
   \item {\tt applycal} --- Apply calculated calibration solutions
      (\S~\ref{section:cal.correct.apply}),
   \item {\tt clearcal} --- Re-initialize the calibration for a
     visibility dataset (\S~\ref{section:cal.correct.clearcal}),
   \item {\tt fluxscale} --- Bootstrap the flux density scale from
      standard calibration sources (\S~\ref{section:cal.solve.fluxscale}), 
   \item {\tt listcal} --- List calibration solutions 
      (\S~\ref{section:cal.tables.listcal}),
   \item {\tt plotcal} --- Plot calibration solutions 
      (\S~\ref{section:cal.tables.plotcal}),
    \item {\tt setjy} --- Compute model visibilities with the correct
      flux density for a specified source
      (\S~\ref{section:cal.prior.models}),
   \item {\tt smoothcal} --- Smooth calibration solutions derived from
      one or more sources (\S~\ref{section:cal.tables.smooth}),
   \item {\tt split} --- Write out new MS containing calibrated data
      from a subset of the original MS (\S~\ref{section:cal.other.split}).
\end{itemize}

There are some development versions of calibration and utility
tasks that are recently added to the suite:
\begin{itemize}
   \item {\tt calstat} --- Statistics of calibration solutions 
      (\S~\ref{section:cal.tables.calstat}),
   \item {\tt cvel} --- Regrid a spectral MS onto a new frequency
      channel system
      (\S~\ref{section:cal.other.cvel}),
   \item {\tt gencal} --- Create a calibration tables from metadata
     such as antenna position offsets, gaincurves and opacities 
      (\S~\ref{section:cal.prior.gencal}),
   \item {\tt wvrgcal} --- Generate a gain table based on Water Vapour
     Radiometer data (for ALMA use - \S~\ref{section:cal.prior.wvrgcal}),
   \item {\tt hanningsmooth} --- Apply a Hanning smoothing filter to
      spectral-line uv data
      (\S~\ref{section:cal.other.hanningsmooth}),
       \item {\tt mstransform} --- {\it experimental} Task to combine
         {\tt cvel, hanningsmooth, split} operations in a single step
      (\S~\ref{section:cal.other.mstransform}),
   \item {\tt uvcontsub} --- Carry out uv-plane continuum fitting and subtraction 
      (\S~\ref{section:cal.other.uvcontsub}),
   \item {\tt uvmodelfit} --- Fit a component source model to
     the uv data (\S~\ref{section:cal.other.uvmodelfit}),
   \item {\tt uvsub} --- Subtract the transform of a model image from
     the uv data (\S~\ref{section:cal.other.uvsub}),
   \item {\tt statwt} --- Recalcuate the data weights based on their
     scatter (\S~\ref{section:cal.other.statwt}),
   \item {\tt conjugatevis} --- Change the signs of visibility phases 
     (\S~\ref{section:cal.other.conjugatevis}).

.  
\end{itemize}
These are not yet full-featured, and may have only rudimentary
controls and options.

The following sections outline the use of these tasks in standard calibration
processes.

Information on other useful tasks and parameter setting can be found in:
\begin{itemize}
   \item {\tt listobs} --- summary of a MS (\S~\ref{section:io.list}),
   \item {\tt listvis} --- list data in a MS (\S~\ref{section:io.vis.listvis}),
   \item {\tt plotms} --- prototype next-generation X-Y plotting and editing 
      (\S~\ref{section:edit.plot.plotms}),
   \item {\tt plotxy} --- previous generation X-Y plotting and editing 
      (\S~\ref{section:edit.plot.plotxy}),
    \item {\tt plotweather} --- plot the weather information of an MS
      and calculate atmospheric opacities (\S~\ref{section:cal.prior.opacity.evla}),
   \item {\tt flagdata} --- non-interactive data flagging
      (\S~\ref{section:edit.flagdata}),
   \item data selection --- general data selection syntax
      (\S~\ref{section:io.selection}).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Calibration Process --- Outline and Philosophy}
\label{section:cal.flow}

A work-flow diagram for CASA calibration of interferometry data is
shown in Figure~\ref{fig:casacalflow}.  This should help you chart
your course through the complex set of calibration steps.  In the
following sections, we will detail the steps themselves and explain
how to run the necessary tasks and tools.

\begin{figure}[h!]
\begin{center}
\pngname{casa_calib}{6.25}
\caption{\label{fig:casacalflow}
Flow chart of synthesis calibration operations. Not shown are
use of table manipulation and plotting tasks {\tt accum}, 
{\tt plotcal}, and {\tt smoothcal} 
(see Figure~\ref{fig:casacaltables}).  }
\hrulefill
\end{center}
\end{figure}

This can be broken down into a number of discrete phases:
\begin{itemize}
   \item {\bf Calibrator Model Visibility Specification} --- set
      model visibilities for calibrators, either unit point source
      visibilities for calibrators with unknown flux density or
      structure (generally, sources used for calibrators are
      approximately point-like), or visibilities derived from {\em a
      priori} images and/or known or standard flux density values.
   \item {\bf Prior Calibration} --- set up previously known
      calibration quantities that need to be pre-applied, such
      antenna
      gain-elevation curves, atmospheric models, delays, and
      antenna position offsets. Use the
      {\tt setjy} task (\S~\ref{section:cal.prior.models}) for
      calibrator flux
      densities and models, and use {\tt gencal} (\S~\ref{section:cal.prior.gencal})
      for antenna position offsets, gaincurves, antenna efficiencies,
      and opacities;
   \item {\bf Bandpass Calibration} --- solve
      for the relative gain of the system over the frequency channels 
      in the dataset (if needed), having pre-applied the prior
      calibration. Use the {\tt bandpass} task 
      (\S~\ref{section:cal.solve.band});
   \item {\bf Gain Calibration} --- solve for the gain variations of
      the system as a function of time, having pre-applied the 
      bandpass (if needed) and prior calibration. Use the 
      {\tt gaincal} task (\S~\ref{section:cal.solve.gain});
   \item {\bf Polarization Calibration} --- solve for 
      polarization leakage terms and linear polarization position angle
      (\S~\ref{section:cal.solve.pol});
   \item {\bf Establish Flux Density Scale} --- if only some of the
      calibrators have known flux densities, then rescale gain
      solutions and derive flux densities of secondary calibrators.
      Use the {\tt fluxscale} task (\S~\ref{section:cal.solve.fluxscale});
   \item {\bf Manipulate, Accumulate, and Iterate} --- if necessary,
      accumulate different calibration solutions (tables), smooth,
      and interpolate/extrapolate onto different sources, bands, and
      times. Use the {\tt accum} (\S~\ref{section:cal.tables.accum}) and
      {\tt smoothcal} (\S~\ref{section:cal.tables.smooth})
      tasks;
   \item {\bf Examine Calibration} --- at any point, you can (and 
      should) use {\tt plotcal} (\S~\ref{section:cal.tables.plotcal}) 
      and/or {\tt listcal} (\S~\ref{section:cal.tables.listcal})
      to look at the calibration tables that you have created;
   \item {\bf Apply Calibration to the Data} --- this can be forced
      explicitly by using the {\tt applycal} task
      (\S~\ref{section:cal.correct.apply}), and can be undone using
      {\tt clearcal} (\S~\ref{section:cal.correct.clearcal});
   \item {\bf Post-Calibration Activities} --- this includes the
      determination and subtraction of continuum signal from line
      data, the splitting of data-sets into subsets (usually
      single-source), and other operations (such as model-fitting).
      Use the {\tt uvcontsub} (\S~\ref{section:cal.other.uvcontsub}),
      {\tt split} (\S~\ref{section:cal.other.split}),
      and {\tt uvmodelfit} (\S~\ref{section:cal.other.uvmodelfit})
      tasks.
\end{itemize}

The flow chart and the above list are in a suggested order.  However,
the actual order in which you will carry out these operations is
somewhat fluid, and will be determined by the specific data-reduction
use cases you are following.  For example, you may need to do an
initial {\bf Gain Calibration} on your bandpass calibrator before
moving to the {\bf Bandpass Calibration} stage.  Or perhaps the
polarization leakage calibration will be known from prior service 
observations, and can be applied as a constituent of Prior Calibration.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Philosophy of Calibration in CASA}
\label{section:cal.flow.philo}

Calibration is not an arbitrary process, and there is
a methodology that has been developed to carry out synthesis
calibration and an algebra to describe the various corruptions
that data might be subject to: the Hamaker-Bregman-Sault Measurement
Equation (ME), described in Appendix~\ref{chapter:me}.
The user need not worry about the details of this mathematics
as the CASA software does that for you.  Anyway, it's just
matrix algebra, and your familiar scalar methods of calibration
(such as in AIPS) are encompassed in this more general approach.

There are a number of ``physical'' components to calibration in CASA:
\begin{itemize}
   \item {\bf data} --- in the form of the Measurement Set
      (\S~\ref{section:io.ms}).  The MS includes a number of
      columns that can hold calibrated data, model information,
      and weights;
   \item {\bf calibration tables} --- these are in the form of
      standard CASA tables, and hold the calibration solutions
      (or parameterizations thereof);
   \item {\bf task parameters} --- sometimes the calibration
      information is in the form of CASA task parameters that
      tell the calibration tasks to turn on or off various
      features, contain important values (such as flux densities),
      or list what should be done to the data.
\end{itemize}

At its most basic level, Calibration in CASA is the process of taking
``uncalibrated'' {\bf data}, setting up the operation of calibration
tasks using {\bf parameters}, solving for new calibration {\bf
tables}, and then applying the calibration tables to form 
``calibrated'' {\bf data}.  Iteration can occur as necessary, with
the insertion of other non-calibration steps
(e.g. imaging to generate improved source models for
``self-calibration'').

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Keeping Track of Calibration Tables}
\label{section:cal.flow.tables}

\begin{figure}[h!]
\begin{center}
\pngname{casa_caltables}{6.25}
\caption{\label{fig:casacaltables} Chart of the table flow during
  calibration. The parameter names for input or output of the tasks
  are shown on the connectors.  Note that from the output solver
  through the accumulator only a single calibration type (e.g. {\tt
    'B'}, {\tt 'G'}) can be smoothed, interpolated or accumulated at a
  time.  {\tt accum} is optional (and not recommended as of v4.0). The final
  set of cumulative calibration tables of all types (accumulated or
  as a list of caltables) are then input to {\tt applycal} as shown in
  Figure~\ref{fig:casacalflow}. }
\hrulefill
\end{center}
\end{figure}

The calibration tables are the currency that is exchanged between
the calibration tasks.  The ``solver'' tasks ({\tt gaincal},
{\tt bandpass}, {\tt blcal}, {\tt polcal}) take in the MS
(which may have a calibration model attached) and previous calibration
tables, and will output an ``incremental'' calibration table
(it is incremental to the previous calibration, if any).  This table
can then be smoothed using {\tt smoothcal} if desired.

You can optionally accumulate the incremental calibration onto
previous calibration tables with {\tt accum}, which will then output a
cumulative calibration table.  This task will also interpolate onto a
different time scale.  See \S~\ref{section:cal.tables.accum} for more
on accumulation and interpolation.

Figure~\ref{fig:casacaltables} graphs the flow of these tables
through the sequence
\small
\begin{verbatim}
      solve   =>   smooth   =>   accumulate
\end{verbatim}
\normalsize
Note that this sequence applied to separate {\em types} of tables
(e.g. {\tt 'B'}, {\tt 'G'}) although tables of other types can
be previous calibration input to the solver.

The final set of cumulative calibration tables is what is applied
to the data using {\tt applycal}.  You will have to keep track of
which tables are the intermediate incremental tables, and which
are cumulative, and which were previous to certain steps so that
they can also be previous to later steps until accumulation.  This
can be a confusing business, and it will help if you adopt a
consistent table naming scheme (see Figure~\ref{fig:casacaltables})
for an example naming scheme).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Calibration of traditional VLA data in CASA}
\label{section:cal.flow.vla}

CASA supports the calibration of traditional VLA data that is 
imported from the Archive through the {\tt importvla} task.
See \S~\ref{section:io.import.vla} for more information.

{\bf ALERT:} Data taken both before and after the Modcomp turn-off in
late June 2007 will be handled automatically by {\tt importvla}.  You
do not need to set special parameters to do so, and it will obey the
scaling specified by {\tt applytsys}.

You can also import VLA data in UVFITS format with the 
{\tt importuvfits} task (\S~\ref{section:io.import.uvfits.import}).
However, in this case, you must be careful during calibration in
that some prior or previous calibrations (see below) may or may not
have been done in AIPS and applied (or not) before export.

For example, the default settings of AIPS {\tt FILLM} will apply
VLA gaincurve and approximate (weather-based) atmospheric optical
depth corrections when it generates the extension table {\tt CL 1}.
If the data is exported immediately using {\tt FITTP}, then this 
table is included in the UVFITS file.  However, CASA is not able
to read or use the AIPS {\tt SN} or {\tt CL} tables, so that 
prior calibration information is lost and must be applied during
calibration here (i.e. using {\tt gaincurve=True} and setting the
{\tt opacity} parameter).  

On the other hand, if you apply calibration in AIPS by using the
{\tt SPLIT} or {\tt SPLAT} tasks to apply the {\tt CL} tables before
exporting with {\tt FITTP}, then this calibration will be in the
data itself.  In this case, you do not want to re-apply these
calibrations when processing in CASA.

\subsection{Loading Jansky VLA data in CASA}
\label{section:cal.flow.evla}

Jansky VLA data can be loaded into CASA either via {\tt importevla} or by
using the task {\tt importasdm}. Both tasks will convert ASDM raw data
files into measurement sets. {\tt importasdm} will convert the data
itself and the majority of the metadata. {\tt importevla} will run
{\tt importasdm} followed by Jansky VLA-specific corrections, like the
application of the on-line flags (e.g. times when the subreflector was
not in place or the an antenna was not on source), an option to clip
values that are exactly zero (as of 2010, such values still may appear
in the VLA raw data), and flagging for shadowing.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preparing for Calibration}
\label{section:cal.prior}

There are a number of ``a priori'' calibration quantities that
may need to be applied to the data before further calibration
is carried out.  These include
\begin{itemize}
   \item {\bf system temperature correction} --- turn correlation
      coefficient into correlated flux density (necessary for some
      telescopes),
   \item {\bf gain curves} --- antenna gain-elevation dependence,
   \item {\bf atmospheric optical depth} --- attenuation of the signal
      by the atmosphere, including correcting for its elevation dependence.
   \item {\bf flux density models} --- establish the flux density
      scale using ``standard'' calibrator sources, with models for
      resolved calibrators,
   \item {\bf delays} --- antenna-based delay offsets,
   \item {\bf antenna position errors} --- offsets in the positions of
      antennas assumed during correlation.
\end{itemize}
These are pre-determined effects and should be applied (if known) before
solving for other calibration terms.  If unknown, then they will
need to be solved for (or subsumed in other calibration such as 
bandpass or gains).

We now deal with these in turn.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{System Temperature and Switched-Power Corrections}
\label{section:cal.prior.tsys}

Some telescopes, including the old VLA, ALMA, and the VLBA, record the
visibilities in the form of raw {\it correlation coefficient} with
weights proportional to the number of bits correlated.  The
correlation coefficient is the fraction of the total signal that is
correlated, and thus multiplication by the system temperature
($T_{sys}$) and the antenna gain (in Jy/K) will produce visibilities 
with units of correlated flux density.  ALMA records $T_{sys}(K)$ 
information in the MS which can be extracted as a caltable using 
{\tt gencal} with {\tt calmode='tsys'}, and applied to data to yield
units of $K$.  Calibration to flux density in $Jy$ is achieved
via reference to sources of known power.

{\bf Alert:}  Note that the old VLA system did this initial calibration on-line.
The modern VLA does not record normalized visibilities.  Instead,
the correlations are delivered in raw engineering units that are 
proportional to power.  The actual total power received is 
continuously monitored during the observation, with a calibration
signal of known temperature (K) switched in at a rate of 10 Hz.
This is the so-called ``switched-power'' calibration system on 
the VLA. This enables a continuous record of the $T_{sys}(K)$, as well as 
net electronic gain variation of each antenna's receiving system.
The correlator requantizer gain is also monitored. 
These data are recorded in MS subtables and appropriate calibration
factors can be derived from them by {\tt gencal} with 
{\tt caltype='swpow'}, and stored in a caltable for application.
This calibration is {\em not} a ``$T_{sys}$'' calibration of the traditional
sort; the switched-power gain is used to correct the visibility
amplitude, and the $T_{sys}$ is used to set the weights. This system
is still being commissioned (as of early 2014). Observations using
8-bit sampling are usually reasonably calibrated; 3-bit-sampled
switched-power data are subject to compression effects that are
not yet completely understood, and the switched power calibration is
not recommended (instead, correction only by the requantizer
gain is recommended, using {\tt caltype='rq'}).

See \S~\ref{section:cal.prior.gencal} for more information on use
of {\tt gencal}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Antenna Gain-Elevation Curve Calibration}
\label{section:cal.prior.curves}

Large antennas (such as the 25-meter antennas used
in the VLA and VLBA) have a forward gain and efficiency that changes with
elevation. Gain curve calibration involves compensating for the effects of
elevation on the amplitude of the received signals at each antenna.
Antennas are not absolutely rigid, and so their effective collecting
area and net surface accuracy vary with elevation as gravity deforms
the surface.  This calibration is especially important at higher
frequencies where the deformations represent a greater fraction of the
observing wavelength.  By design, this effect is usually minimized
(i.e., gain maximized) for elevations between 45 and 60 degrees, with
the gain decreasing at higher and lower elevations.  Gain curves are
most often described as 2nd- or 3rd-order polynomials in zenith angle.

Gain curve calibration has been implemented in CASA for the modern VLA
and old VLA (only), with gain curve polynomial coefficients available
directly from the CASA data repository.  To make gain curve and
antenna efficiency corrections for VLA data, use {\tt gencal} with
{\tt caltable='gceff'}. See \S~\ref{section:cal.prior.gencal} for more information on use
of {\tt gencal}.



{\bf ALERT:} 
If you are not using VLA data, do not use gaincurve corrections.
A general mechanism for incorporating gaincurve information for
other arrays will be made available in future releases.
The gain-curve information available for the VLA is
time-dependent (on timescales of months to years, at least for the 
higher frequencies), and CASA will automatically select 
the date-appropriate gain curve information.  Note, however, that 
the time-dependence was poorly sampled prior to 2001, and so gain 
curve corrections prior to this time should be considered with caution.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Atmospheric Optical Depth Correction}
\label{section:cal.prior.opacity}

The troposphere is not completely transparent.  At high radio
frequencies ($>$15 GHz), water vapor and molecular oxygen begin to
have a substantial effect on radio observations. According to the
physics of radiative transmission, the effect is threefold.  First,
radio waves from astronomical sources are absorbed (and therefore
attenuated) before reaching the antenna.  Second, since a good absorber
is also a good emitter, significant noise-like power will be added to
the overall system noise.  Finally, the optical path length through
the troposphere introduces a time-dependent phase error.  In all
cases, the effects become worse at lower elevations due to the
increased air mass through which the antenna is looking.  In CASA,
the opacity correction described here compensates only for the first
of these effects, tropospheric attenuation, using a plane-parallel
approximation for the troposphere to estimate the elevation
dependence.

To make
opacity corrections in CASA, an estimate of the zenith opacity is
required (see observatory-specific chapters for how to measure zenith
opacity).  This is then supplied to the {\tt caltype='opac'} parameter
in {\tt gencal} which creates a calibration table with all the
information. E.g. for data with two spectral windows, the inputs are
like: 
\small
\begin{verbatim}
gencal(vis='dataset.ms',
       caltable='opacity.cal',
       caltype='opac',
       spw='0,1',
       parameter=[0.0399,0.037])
\end{verbatim}
\normalsize


If you do not have an externally supplied value for {\tt opacity}, for
example from a VLA tip procedure, then you should either use an
average value for the telescope, or leave it at zero and let
your gain calibration compensate as best it can (e.g. that your 
calibrator is at the same elevation as your target at approximately 
the same time. As noted above, there are no facilities yet to estimate this from the
data (e.g. by plotting $T_{sys}$ vs. elevation).

Below, we give instructions for determining {\tt opacity} for Jansky VLA
data from weather statistics and VLA observations where tip-curve data
is available.  It is beyond the scope of this cookbook to provide
information for other telescopes.

%%%%%
\subsubsection{Determining opacity corrections for {\em modern} VLA data}
\label{section:cal.prior.opacity.evla}

For the VLA site, weather statistics and/or seasonal models that
average over many years of weather statistics prove to be reasonable good
ways to estimate the opacity at the time of the observations. The task
{\tt plotweather} calculates the opacity as a mix of both actual
weather data and seasonal model. It has the following inputs: 

\small
\begin{verbatim}
#  plotweather :: Plot elements of the weather table; estimate opacity.
vis                 =         ''        #  MS name
seasonal_weight     =        0.5        #  weight of the seasonal model
doPlot              =       True        #  set this to True to create a plot
async               =      False        #  If true the taskname must
                                        #  be started using plotweather(...)
\end{verbatim}
\normalsize

The task plots the weather statistics if {\tt doPlot=T}, like shown in
Figure\,\ref{fig:plotweather}. The bottom panel displays the
calculated opacities for the run as well as a seasonal model. The
parameter {\tt seasonal\_weight} can be adjusted to calculate the
opacities as a function of the weather data alone {\tt
  seasonal\_weight=0}, only the seasonal model {\tt
  seasonal\_weight=1}, or a mix of the two (values between 0 and
1). Calculated opacities are shown in the logger output, one for each
spectral window. {\tt plotweather} can also assign a python variable
to a list of calculated opacities (one entry for each spw) when run as:

\small
\begin{verbatim}
myTau = plotweather(vis='myvladata.ms')
\end{verbatim}
\normalsize

In this example, {\tt myTau} will be returned with a list of per-spw
opacities, e.g. {\tt myTau=[0.02,0.03]} and can later be used as input for 
{\tt gencal} in {\tt caltype='opac'} in the {\tt parameter} setting, 
e.g.,

\small
\begin{verbatim}
# opac for spws 0,1 in myTau
gencal(vis='myvladata.ms',caltype='opac',spw='0,1',parameter=myTau)  
\end{verbatim}
\normalsize

Note that it is important to explicitly specify the spws that are
covered by the opacity values stored in {\tt myTau}.  For most modern
VLA data there will be more than two spws, probably.

See \S~\ref{section:cal.prior.gencal} for more information on use
of {\tt gencal}.

\begin{figure}[h!]
\begin{center}
\pngname{PlotWX}{6.25}
\caption{\label{fig:plotweather}
The weather information for a MS as plotted by the task {\tt plotweather}.}
\hrulefill
\end{center}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%
\subsubsection{Determining opacity corrections for VLA data}
\label{section:cal.prior.opacity.vla}

For VLA data, zenith opacity can be measured at the frequency
and during the time observations are made using a VLA tipping scan in
the observe file.  Historical tipping data are available at:
\begin{quote}
   \url{http://www.vla.nrao.edu/astro/calib/tipper}
\end{quote}
Choose a year, and click {\tt Go} to get a list of all tipping scans
that have been made for that year.

If a tipping scan was made for your observation, then select the
appropriate file.  Go to the bottom of the page and click on the
button that says {\tt Press here to continue.}.  The results of the
tipping scan will be displayed.  Go to the section called 'Overall Fit
Summary' to find the fit quality and the fitted zenith opacity in
percent.  If the zenith opacity is reported as 6\%, then the actual
zenith optical depth value is {\tt opacity=0.060} for {\tt gaincal}
and other calibration tasks.

If there were no tipping scans made for your observation, then look
for others made in the same band around the same time and weather
conditions.  If nothing is available here, then at K and Q bands
you might consider using an average value (e.g.\ 6\% in reasonable
weather).  See the VLA memo
\begin{quote}
   \url{http://www.vla.nrao.edu/memos/test/232/232.pdf}
\end{quote}
for more on the atmospheric optical depth correction at the VLA,
including plots of the seasonal variations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Setting the Flux Density Scale using ({\tt setjy})}
\label{section:cal.prior.models}

When solving for visibility-plane calibration, CASA calibration
applications compare the observed {\tt DATA} column with the Fourier
transform of calibrator model when it is provided (if no model is
specified, a point source at the phase center is assumed).  

The {\tt setjy} task is used to set the proper flux density and
attaches a model image (if specified) of the calibrator to the MS. For
sources that are recognized flux calibrators (listed in Tables
\ref{table:fluxcal-table} and \ref{table:fluxcal-table-solarsystem},
see also \S\,\ref{section:conventions.fluxdensity}),
{\tt setjy} can calculate the flux densities as a function of
frequency (and time, for Solar System objects).  Otherwise, the flux
densities should be manually specified ({\tt standard='manual'}).  

For the VLA, the default source models are customarily point sources
defined by the 'Baars', 'Perley 90', 'Perley-Taylor 99',
'Perley-Butler 2010', time-variable 'Perley-Butler 2013', or 'Scaife-Heald 2012' flux density scales
(\S\,\ref{section:conventions.longwavelength}; 'Perley-Butler 2010' is
the current standard by default), or point sources of unit flux
density if the flux density is unknown. In fact, the model can be any
image in Jy/pixel units (models typically generated by the {\tt clean}
task).


Optionally, the MODEL column can be filled with the Fourier transform
of (option {\tt usescratch=T} is {\tt setjy}, {\tt ft}, and {\tt
  clean}). But for most measurement sets, the performance and data
storage requirements are less demanding without the MODEL\_DATA column.

The inputs for {\tt setjy} are:
\small
\begin{verbatim}
#  setjy :: Fills the model column with the visibilities of a calibrator
vis                 =         ''        #  Name of input visibility file
field               =         ''        #  Field name(s)
spw                 =         ''        #  Spectral window identifier (list)
selectdata          =      False        #  Other data selection parameters
modimage            =         ''        #  File location for field model
listmodels          =      False        #  List the available modimages for VLA
                                        #   calibrators or Tb models for Solar System
                                        #   objects
scalebychan         =       True        #  scale the flux density on a per channel basis
                                        #   or else on a per spw basis
fluxdensity         =         -1        #  Specified flux density [I,Q,U,V]; -1 will
                                        #   lookup values
standard            = 'Perley-Butler 2010' #  Flux density standard
usescratch          =      False        #  Will create if necessary
                                        #   and use the MODEL_DATA
async               =      False        #  If true the taskname must be started using
                                        #   setjy(...)
\end{verbatim}
\normalsize 

\vspace{5mm}
\begin{table}[h!]
\caption{Recognized Flux Density Calibrators. Note that the VLA uses
J2000 calibrator names. CASA accepts all strings that contain the
names below. E.g. 'PKS 1934-638' will be recognized
        {\label{table:fluxcal-table}}}
\begin{center}
\begin{tabular}{|ccccc|} \hline
 {\bf 3C Name}  & {\bf B1950 Name}& {\bf J2000 Name} & {\bf Alt. J2000
   Name} & {\bf Standards}\\
3C48  &  0134+329 &  0137+331 &  J0137+3309 & 1,3,4,5,6,7  \\
3C123 &  0433+295 &  0437+296 &  J0437+2940 &      2 \\
3C138 &  0518+165 &  0521+166 &  J0521+1638 &      1,3,4,5,6\\
3C147 &  0538+498 &  0542+498 &  J0542+4951 &      1,3,4,5,6,7\\
3C196 &  0809+483 &  0813+482 &  J0813+4813 &      1,2,7 \\
3C286 &  1328+307 &  1331+305 &  J1331+3030 &      1,2,3,4,5,6,7\\
3C295 &  1409+524 &  1411+522 &  J1411+5212 &      1,2,3,4,5,6,7\\
 --   &  1934-638 &    --     &  J1939-6342 &      1,3,4,5,6\\
3C380&   1828+487&   1829+487 &  J1829+4845&       7\\
\hline
\end{tabular}
\end{center}
Standards are: (1) Perley-Butler 2010, (2) Perley-Butler 2013, (3) Perley-Taylor 99, (4)
Perley-Taylor 95, (5) Perley 90, (6) Baars (Baars, J. W. M., et
al. 1977, A\&A, 61, 99); (7) Scaife-Heald 2012, see
\S\,\ref{section:conventions.longwavelength} for details.
\end{table}



\vspace{5mm}
\begin{table}[h!]
\caption{'Butler-JPL-Horizons 2012' recognized Solar System Objects for Flux Calibration
        {\label{table:fluxcal-table-solarsystem}}}
\begin{center}
\begin{tabular}{|l|} \hline
\hline
\multicolumn{1}{|c|}{Planets} \\

Venus\footnotemark[1], Mars\footnotemark[2], Jupiter\footnotemark[3], Uranus\footnotemark[4], Neptune\footnotemark[5]\\
\hline
\multicolumn{1}{|c|}{Moons}\\
{\it Jupiter:}  Io, Europa, Ganymede, Callisto\\
{\it Saturn:}  Titan\footnotemark[7] \\
%{\it Neptune:} Triton \\
%{\it Mars moons:} Phobos, Deimos\\
%{\it Jupiter moons:} Io, Europa, Ganymede, Callisto\\
%{\it Saturn moons:} Mimas, Enceladus, Tethys, Dione, Rhea, Titan, Hyperion\\
%Iapetus, Phoebe, Janus, Epimetheus, Helene, Telesto, Calypso, Atlas\\
%Prometheus, Pandora, Pan\\
%{\it Uranus moons:} Ariel, Umbriel, Titania, Oberon, Miranda, Cordelia\\
%Ophelia, Bianca, Cressida, Desdemona, Juliet, Portia, Rosalind\\
%Belinda, Puck\\
%{\it Neptune moons:} Triton, Nereid, Naiad, Thalassa, Despina, Galatea\\
%Larissa, Proteus\\
%{\it Pluto moon:} Charon\\
\hline
\multicolumn{1}{|c|}{Asteroids}\\
Ceres, Pallas\footnotemark[8], Vesta\footnotemark[8],
Juno\footnotemark[8]\\
%, Victoria\footnotemark[2], Davida\footnotemark[2]\\
%Ceres, Pallas, Juno, Vesta, Astraea, Hygiea, Parthenope, Victoria\\
%Davida, Interamnia\\
\hline
\end{tabular}
\end{center}
\footnotesize
$^1$ Venus: model for $\sim300$\,MHz to 350\,GHz, no atmospheric lines
(CO,H$_2$O,HDO, etc.)\\
$^2$ Mars: tabulated as a function of time and frequency (30 - 1000GHz) based on 
               Rudy et al. (1988), no atmospheric lines (CO, H20, H$_2$O$_2$, HDO, etc.)\\
$^3$ Jupiter: model for 30-1020GHz, does not include synchrotron
emission\\
$^4$ Uranus: model for 60-1800GHz, contains no rings or synchrotron.\\
$^5$ Neptune: model for 2-2000GHz, the broad CO absorption line
               is included, but contains no rings or synchrotron. \\

$^7$ Titan: model for 53.3-1024.1GHz, include many spectral lines\\
$^8$ not recommended (The temperature is not yet adjusted for
                varying distance from the Sun.  The model data can be scaled
                after running setjy, but it is an involved process.)\\
Details are described in ALMA Memo
594 available on
\url{https://science.nrao.edu/facilities/alma/aboutALMA/Technology/ALMA_Memo_Series/alma594/abs594}.

\normalsize

\end{table}



% Butler-JPL-Horizons 2010:
%
%\vspace{5mm}
%\begin{table}[h!]
%\caption{Recognized Solar System Objects for Flux Calibration
%        {\label{table:fluxcal-table-solarsystem}}}
%\begin{center}
%\begin{tabular}{|l|} \hline
%\hline
%\multicolumn{1}{|c|}{Planets} \\
%
%Venus, Mars, Jupiter, Uranus, Neptune, Pluto\footnotemark[1]\\
%\hline
%\multicolumn{1}{|c|}{Moons}\\
%{\it Jupiter:} Io, Europa, Ganymede, Callisto\\
%{\it Saturn:}  Titan \\
%{\it Neptune:} Triton \\
%%{\it Mars moons:} Phobos, Deimos\\
%%{\it Jupiter moons:} Io, Europa, Ganymede, Callisto\\
%%{\it Saturn moons:} Mimas, Enceladus, Tethys, Dione, Rhea, Titan, Hyperion\\
%%Iapetus, Phoebe, Janus, Epimetheus, Helene, Telesto, Calypso, Atlas\\
%%Prometheus, Pandora, Pan\\
%%{\it Uranus moons:} Ariel, Umbriel, Titania, Oberon, Miranda, Cordelia\\
%%Ophelia, Bianca, Cressida, Desdemona, Juliet, Portia, Rosalind\\
%%Belinda, Puck\\
%%{\it Neptune moons:} Triton, Nereid, Naiad, Thalassa, Despina, Galatea\\
%%Larissa, Proteus\\
%%{\it Pluto moon:} Charon\\
%\hline
%\multicolumn{1}{|c|}{Asteroids}\\
%Ceres, Pallas\footnotemark[2], Vesta\footnotemark[2], Juno\footnotemark[2], Victoria\footnotemark[2], Davida\footnotemark[2]\\
%%Ceres, Pallas, Juno, Vesta, Astraea, Hygiea, Parthenope, Victoria\\
%%Davida, Interamnia\\
%\hline
%\end{tabular}
%\end{center}
%\footnotesize
%$^1$ Pluto alone.  No allowance is made for the possibility of Charon
%  being in the field.\\
%$^2$ not recommended for this release (The temperature is not yet adjusted
%   for varying distance from the Sun.  The model data can be scaled after
%   running setjy, but it is an involved process.)
%\normalsize
%
%\end{table}




By default the {\tt setjy} task will cycle through all
fields spectral windows and channels, (one solution per spw with {\tt
  scalebychan = False}) , setting the flux density either to 1 Jy
(unpolarized), or if the source is recognized as one of the
calibrators in the above table, to the flux density (assumed
unpolarized) appropriate to the observing frequency.  For example, to
run {\tt setjy} on a measurement set called {\tt data.ms}: \small
\begin{verbatim}
  setjy(vis='data.ms')                # This will set all fields and spectral windows
\end{verbatim}
\normalsize

Models of available calibrator sources can be listed by setting {\it
  listmodels=True}. {\tt setjy} will then come up with all images
that are in the paths where calibrator models for known telescopes are
stored. It will also show all images in the working directory - any
image there could potentially be a calibrator model. If the calibrator
model is found by {\it listmodels} it can be used in the {\it
  modimage} parameter without a path.

The {\tt fluxdensity} parameter can be used to specify the flux of the
calibrator in all Stokes parameters. It it thus a list of values
[I,Q,U,V], e.g. ['12Jy','13mJy','0Jy','0Jy']. In addition, a spectral
index can be specified via {\it spix} and a reference frequency {\it
  reffreq} (using the definition: $S = fluxdensity\times\frac{
  freq}{reffreq}^{spix}$).


Most calibrator sources are based on radio emission from quasars and
jets. The spectral indices of these sources are such that at (sub)mm
wavelengths the majority of these sources become too weak and variable
to be reliable flux estimators. Alternatives are thermal objects such
as planets, moons, and asteroids. Those sources, however, are all
Solar System objects, which implies that they move and may be
(strongly) resolved. The {\tt standard='Butler-JPL-Horizons 2010'} and
the recommended {\tt standard='Butler-JPL-Horizons 2012'} (for more
information on the implemented models, see ALMA Memo 594 soon
available on
\url{https://science.nrao.edu/facilities/alma/aboutALMA/Technology/ALMA_Memo_Series/alma594/abs594}.)
option of {\tt setjy} includes flux density calibration using Solar
System objects. For 'Butler-JPL-Horizons 2012' CASA currently supports
the objects listed in Table~\ref{table:fluxcal-table-solarsystem} to
be applied to ALMA data. These names are recognized when they are used
in the 'field' parameter in {\tt setjy}.  In that case, {\tt setjy}
will obtain the geocentric distance and angular diameter at the time
of the observation from a (JPL--Horizons) ephemeris and calculate
model visibilities.  Currently the objects are modeled as uniform
temperature disks, but effects like primary beam attenuation and limb
darkening will be accounted for soon. Note that this model may
oversimplify the real structure, in particular asteroids.

An example, using data from the M99 tutorial in 
\url{http://casaguides.nrao.edu/index.php?title=CARMA_spectral_line_mosaic_M99}:

{\tt setjy(vis='c0104I', field='MARS', spw='0~2', standard='Butler-JPL-Horizons 2012')}

Tip: Running casalog.filter('INFO1') before running setjy with a Solar
System object may send the logger a reference to the temperature
measurement.  Use casalog.filter('INFO') to restore the normal logging
level.

The source model will be attached to the MS and applied to all
calibration steps when {\tt usescratch=False}. {\tt usescratch=True}
  fills the {\tt MODEL\_DATA} column with the Fourier transform of the
  model. As of CASA 3.4. we found that under some circumstances,
  creation of the MODEL column may prevent memory issues and if tasks
  fail, we recommend to set {\tt usescratch=True}. Note that currently
  {\tt setjy} will not transform a full-Stokes model image such that
  all polarizations are applied correctly.  You need to use {\tt ft}
  for this.


To limit this operation to certain fields and spectral windows, use
the {\tt field} and/or {\tt spw} parameters, which take the usual
data selection strings (\S~\ref{section:io.selection}). For example, 
to set the flux density of the first field (all spectral windows)
\small
\begin{verbatim}
  setjy(vis='data.ms',field='0')
\end{verbatim}
\normalsize
or to set the flux density of the second field in spectral window 17
\small
\begin{verbatim}
  setjy(vis='data.ms',field='1',spw='17')
\end{verbatim}
\normalsize
The full-polarization flux density (I,Q,U,V) may also be explicitly provided:
\small
\begin{verbatim}
  setjy(vis='data.ms',
       field='1',spw='16',               # Run setjy on field id 1, spw id 17
       fluxdensity=[3.5,0.2,0.13,0.0])   # and set I,Q,U,V explicitly
\end{verbatim}
\normalsize


{\bf ALERT:} The apparent brightness of objects in the Solar System
will vary with time because of the Earth's varying distance to them,
if nothing else.  If the field {\it index} of a flux calibrator spans
several days, you should run setjy more than once, limiting each run
to a suitable timerange by using the timerange, scan, and/or
observation selection parameters.  Note that it is the field index
that matters, not the name.  Typically {\tt concat} assigns moving
objects a new field index for each observation, so usually it is not
necessary to select a time range in setjy.  However, it is worth
checking with {\tt listobs}, especially for planets.

%\begin{figure}
%\label{fig:reldelfdperday}
%\pngname{reldelfdperday}{6}
%\caption{The relative change of apparent brightness per day for some
%popular Solar System flux density calibrators.  Note that when Mars is
%varies fastest near opposition, when it is closest to us and thus probably
%too resolved to use as a calibrator anyway.}  
%\end{figure}


\subsubsection{Using Calibration Models for Resolved Sources}
\label{section:cal.prior.models.resolved}

For observations of solar system objects using the
'Butler-JPL-Horizons 2010' and 'Butler-JPL-Horizons 2012' models
(\S\,\ref{section:cal.prior.models}) {\tt setjy} will know and apply
the flux distribution across the extended structure of the calibrators. 

For other sources, namely VLA calibrator sources, a flux density
calibrator can be resolved at the observing frequency and the point
source model generated by {\tt setjy} will not be appropriate.  If
available, a model image of the resolved source at the observing
frequency may be used to generate the appropriate visibilities using
the {\tt modimage} parameter (or in older versions explicitly with the
{\tt ft} task).  To use this, provide {\tt modimage} with the path to
the model image.  Remember, if you just give the file name, it will
assume that it is in the current working directory.  Note also that
{\tt setjy} using a model image will only operate on that single
source, thus you would run it multiple times (with different {\tt
  field} settings) for different sources.

Otherwise, you may need to use the {\tt uvrange} selection
(\S~\ref{section:cal.solve.pars.select}) in the calibration solving
tasks to exclude the baselines where the resolution effect is
significant.  There is not hard and fast rule for this, though you
should consider this if your calibrator is shows a drop of more than
10\% on the longest baselines (use {\tt plotxy},
\S~\ref{section:edit.plot.plotxy}, to look at this).  You may need to
do {\tt antenna} selection also, if it is heavily resolved and there
are few good baselines to the outer antennas.  Note that {\tt uvrange}
may also be needed to exclude the short baselines on some calibrators
that have extended flux not accounted for in the model.  {\bf Note:}
the calibrator guides for the specific telescopes usually indicate
appropriate min and max for {\tt uvrange}. For example, see the {\em
  VLA Calibration Manual} at:
\begin{quote}
   \url{http://www.vla.nrao.edu/astro/calib/manual/}
\end{quote}
for details on the use of standard calibrators for the E/VLA.

Model images for some flux density calibrators are provided with CASA:
\begin{itemize}
   \item Red Hat Linux RPMs 32bit (RHE4, Fedora 6): 
         located in\\ /usr/lib/casapy/data/nrao/VLA/CalModels
   \item Red Hat Linux RPMs 64bit (RHE4, Fedora 6): 
         located in\\ /usr/lib64/casapy/data/nrao/VLA/CalModels
   \item MAC OSX .dmg: located in\\ /Applications/CASA.app/Contents/Resources/casa-data/nrao/VLA/CalModels
   \item NRAO-AOC casapy-test:\\ /home/casa/data/nrao/VLA/CalModels
%  \item NRAO-AOC test: /home/ballista/casa/daily/data/nrao/VLA/CalModels
\end{itemize}
e.g., these are found in the {\tt data/nrao/VLA/CalModels}
sub-directory of the CASA installation.  For example, just point to the
repository copy, e.g.
\small
\begin{verbatim}
   modimage = '/usr/lib/casapy/data/nrao/VLA/CalModels/3C48_C.im'
\end{verbatim}
\normalsize
or if you like, you can copy the ones you wish to use to your working
directory.

The models available are:
\small
\begin{verbatim}

3C138_L.im  3C147_L.im  3C286_L.im  3C48_L.im
3C138_C.im  3C147_C.im  3C286_C.im  3C48_C.im
3C138_X.im  3C147_X.im  3C286_X.im  3C48_X.im
3C138_U.im  3C147_U.im  3C286_U.im  3C48_U.im
3C138_K.im  3C147_K.im  3C286_K.im  3C48_K.im
3C138_Q.im  3C147_Q.im  3C286_Q.im  3C48_Q.im
\end{verbatim}

(more calibrator models for the VLA are available at\\
\url{https://science.nrao.edu/facilities/vla/data-processing/models}
\normalsize
These are all un-reconvolved images of AIPS CC lists.  
It is important that the model image {\em not} be one
convolved with a finite beam; it must have units of Jy/pixel (not
Jy/beam).  

Note that {\tt setjy} will rescale the flux in the models for known
sources (e.g. those in Table~\ref{table:fluxcal-table}) to match those
it would have calculated.  It will thus extrapolated the flux out of
the frequency band of the model image to whatever spectral windows
in the MS are specified (but will use the structure of the source
in the model image).

{\bf ALERT:} The reference position in the {\tt modimage} is 
currently used by {\tt setjy} when it does the Fourier transform,
thus differences from the positions for the calibrator in the MS
will show up as phase gradients in the uv-plane.  If your model
image position is significantly different but you don't want this
to affect your calibration, then you can doctor either the image
header using {\tt imhead} (\S~\ref{section:analysis.imhead})
or in the MS (using the {\tt ms} tool) as appropriate.  In an upcoming
release we will put in a toggle to use or ignore the position of
the {\tt modimage}.  Note that this will not affect the flux scaling
(only put in erroneous model phases); in any event small position
differences, such as those arising by changing epoch from B1950 to
J2000 using {\tt regridimage} (\S~\ref{section:analysis.regrid}),
will be inconsequential to the calibration.

This illustrates the use of {\tt uvrange} for a slightly resolved 
calibrator:
\small
\begin{verbatim}
  # Import the data
  importvla(archivefiles='AS776_A031015.xp2', vis='ngc7538_XBAND.ms',
            freqtol=10000000.0, bandname='X')

  # Flag the ACs
  flagautocorr('ngc7538_XBAND.ms')

  # METHOD 1:  Use point source model for 3C48, plus uvrange in solve

  # Use point source model for 3C48
  setjy(vis='ngc7538_XBAND.ms',field='0');

  # Limit 3C48 (fieldid=0) solutions to uvrange = 0-40 klambda
  gaincal(vis='ngc7538_XBAND.ms', caltable='cal.G', field='0',
          solint=60.0, refant='10', selectdata=True, uvrange='0~40klambda', 
          append=False)

  # Append phase-calibrator's solutions (no uvrange) to the same table
  gaincal(vis='ngc7538_XBAND.ms', caltable='cal.G', field='2', 
          solint=60.0, refant='10', selectdata=True, uvrange='', 
          append=True)

  # Fluxscale
  fluxscale(vis='ngc7538_XBAND.ms', caltable='cal.G', reference=['0137+331'],
          transfer=['2230+697'], fluxtable='cal.Gflx', append=False)
\end{verbatim}
\normalsize
while the following illustrates the use of a model:
\small
\begin{verbatim}
  # METHOD 2: use a resolved model copied from the data repository
  #   for 3C48, and no uvrange
  # (NB: detailed freq-dep flux scaling TBD)

  # Copy the model image 3C48_X.im to the working directory first!

  setjy(vis='ngc7538_XBAND.ms', field='0', modimage='3C48_X.im')

  # Solutions on both calibrators with no uvrange
  gaincal(vis='ngc7538_XBAND.ms', caltable='cal.G2', field='0,2',
          solint=60.0, refant='10', 
          append=False)

  # Fluxscale
  fluxscale(vis='ngc7538_XBAND.ms', caltable='cal.G2', reference=['0137+331'],
          transfer=['2230+697'], fluxtable='cal.G2flx', append=False)

  # Both methods give 2230 flux densities ~0.7 Jy, in good agreement with
  #   AIPS
\end{verbatim}
\normalsize


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correction for delay and antenna position offsets 
   using {\tt gencal}}
\label{section:cal.prior.gencal}


The gencal task provides a means of specifying antenna-based
calibration values manually.  The values are put in designated tables
and can be applied to the data on-the-fly in solving tasks and 
using applycal.

The {\tt gencal} task has the inputs:
\small
\begin{verbatim}
#  gencal :: Specify Calibration Values of Various Types
vis                 =         ''        #  Name of input visibility file
caltable            =         ''        #  The new/existing calibration table
caltype             =         ''        #  The calibration type: 'amp','ph', 'sbd','mbd','
                                        #   antpos','antposvla','tsys','evlagain','opac','
                                        #   gc','gceff','eff'
spw                 =         ''        #  Calibration spw(s) selection
antenna             =         ''        #  Calibration antenna(s) selection
pol                 =         ''        #  Calibration polarizations(s) selection
parameter           =         []        #  The calibration values
async               =      False        #  If true the taskname must be started using
                                        #   gencal(...)

\end{verbatim}
\normalsize


Current antenna-based gencal options (caltype) are:
\begin{itemize}
   \item {\tt 'amp'} --- amplitude correction
   \item {\tt 'ph'} --- phase correction
   \item {\tt 'sbd'} --- single-band delay (phase-frequency slope for each spw)
   \item {\tt 'mbd'} --- multi-band delay (phase-frequency slope over all spw)
   \item {\tt 'antpos'} --- ITRF antenna position corrections for the
     Jansky VLA (automatic parameter lookup is supported)
   \item {\tt 'antposvla'} --- old VLA-centric antenna position
     corrections 
   \item {\tt 'tsys'} --- Tsys from the SYSCAL table (ALMA)
   \item {\tt 'evlagain'} --- VLA switched-power gains (experimental;
     equal to 'swpow')
   \item {\tt 'swpow'} --- VLA switched power (equal to 'evlagain')
   \item {\tt 'rq'} --- VLA requantizer gains {\it only} 
   \item {\tt swp/rq} --- VLA switched-power gains divided by requantizer gain
   \item {\tt 'opac'} --- Tropospheric opacity
    \item {\tt 'gc'} --- Gain curve (zenith-angle-dependent gain) (VLA
    only) (auto-lookup only)
    \item {\tt 'gceff'} --- Antenna efficiency (sqrt(K/Jy)) (VLA only) (auto-lookup only)
    \item {\tt 'eff'} --- Gain curve and efficiency (VLA only) (auto-lookup only)
\end{itemize}

The calibration parameter specifications cannot be time-variable in the present
implementation (though some of them will introduce implicit
time-dependence upon evaluation in the apply).  Calibration values can be assigned to each {\tt spw},
{\tt antenna} and {\tt pol} selection, where applicable.  The list 
of calibration values specified in {\tt parameter} must conform to
the range of spectral windows, antennas, and polarizations specified
in {\tt spw}, {\tt antenna} and {\tt pol}, with the values specified
in order of the specified polarizations (fastest), antennas, and spectral
windows (slowest).  If any of {\tt spw}, 
{\tt antenna}, or {\tt pol} are left unspecified (empty strings), the
values specified in {\tt parameter} will be assumed applicable to
all values of the unspecified data axes. The output caltable will
otherwise assume nominal calibration values for unspecified spectral
windows, antennas, and polarizations.  Note that antenna position
corrections formally do not have spectral-window or polarization
dependence; such specifications should not be used with 'antpos'.

The same caltable can be specified for multiple runs of gencal, in
which case the specified parameters will be incorporated cumulatively.
E.g., amplitude parameters ({\tt caltype='amp'}) multiply and
phase-like parameters ({\tt 'ph', 'sbd','mbd','antpos'}) add.
Parameters for {\tt 'amp'} and {\tt 'ph'} corrections can be
incorporated into the same caltable (in separate runs), but each of
the other types require their own unique caltable.  A mechanism for
specifying manual corrections via a text file will be provided in the
future.

Two kinds of delay corrections are supported.  For {\tt caltype='sbd'},
the specified delays (in nanoseconds) will be applied locally to 
each spectral window, referring the derived phase corrections to
each spectral window's reference frequency (where the phase correction
will be zero).  The phases in each spectral window will nominally be
flattened, but any phase offsets between spectral windows will remain.
(These can be corrected using {\tt caltype='phase'}, or via ordinary
spectral-window-dependent phase calibration.)  For {\tt caltype='mbd'},
the evaluated phase corrections are referred to zero frequency.  This
causes a correction that is coherent over many spectral windows. 
If the data are already coherent over many spectral windows and share
a common multi-band delay (e.g., VLA data, per baseband), {\tt caltype='mbd'} 
corrections will maintain this coherence and flatten the 
frequency-dependent phase.  Using {\tt caltype='sbd'} in this instance
will introduce phase offsets among spectral windows that reflect
the multi-band delay.  

For antenna position corrections ({\tt caltype='antpos'}), the antenna
position offsets are specified in the ITRF frame.  If the {\tt
  antenna} field is left empty, {\tt gencal} will try to look up the
appropriate antenna position offsets at the time of the observation
from the VLA baseline webpage
\url{http://www.vla.nrao.edu/astro/archive/baselines/}.

For VLA position corrections in the VLA-centric frame, use {\tt
  caltype='antposvla'}, and gencal will rotate them to ITRF before
storing them in the output caltable.

The sign and scale convention for {\tt gencal} corrections (indeed for
all CASA caltables) is such that the specified parameters (and as
stored in caltables) are the factors that {\em corrupt} ideal data to
yield the observed data.  Thus, when applied to correct the data,
their effective inverse will automatically be taken.  I.e., amplitude
factors will be divided into the data on correction.  Phase-like
parameters adopt the convention that the complex factor for the second
antenna in the baseline is conjugated, and then both antenna factors
are divided into the data on correction.  (These conventions differ
from AIPS in that {\tt multiplying} correction factors are stored in
AIPS calibration tables; however, the phase convention ends up being
the same since AIPS conjugates the complex factor for the {\em first}
antenna in the baseline.)

The following series of examples illustrate the use of {\tt gencal}.

For the dataset {\tt 'data.ms'}, the following sequence of {\tt
gencal} runs introduces, into a single caltable ({\tt 'test.G'}), (1)
an antenna-based amplitude scale correction of $3.0$ for all
polarizations, antennas, and spectral windows, (2) phase corrections
for all spectral windows and polarizations of 45 and 120 degrees to
antennas EA03 and EA04, respectively, (3) phase corrections for all
spectral windows of 63 and -34 in R (only) for antennas EA05 and EA06,
respectively, and (4) phase corrections for all spectral windows of
14, -23, -130, and 145 degrees for antenna/polarizations EA09/R,
EA09/L, EA10/R, and EA10/L, respectively:

\small
\begin{verbatim}
gencal(vis='data.ms',caltable='test.G',caltype='amp', \
       spw='',antenna='',pol='', \
       parameter=[3])

gencal(vis='data.ms',caltable='test.G',caltype='ph', \
       spw='',antenna='EA03,EA04',pol='', \
       parameter=[45,120])

gencal(vis='data.ms',caltable='test.G',caltype='ph', \
       spw='',antenna='EA05,EA06',pol='R', \
       parameter=[63,-34])

gencal(vis='data.ms',caltable='test.G',caltype='ph', \
       spw='',antenna='EA09,EA10',pol='R,L', \
       parameter=[14,-23,-130,145])
\end{verbatim}
\normalsize

In the following example, delay corrections in both polarizations will
be adjusted for antenna EA09 by 14 nsec in spw 2 and -130 nsec in spw
3, and for antenna EA10 by -23 nsec in spw 2 and 145 nsec in spw 3:

\small
\begin{verbatim}
gencal(vis='test.ms',caltable='test.sbd',caltype='sbd', \
       spw='2,3',antenna='EA09,EA10',pol='', \
       parameter=[14,-23,-130,145])
\end{verbatim}
\normalsize

In the following example, antenna position corrections in meters (in
ITRF) for antenna EA09 (dBx=0.01, dBy=0.02, dBz=0.03) and for antenna
EA10 (dBx=-0.03, dBy=-0.01, dBz=-0.02) are introduced.  Note that 
three parameters are required for each antenna. The antenna offsets
can be obtained for the 'Jansky VLA/ old VLA Baseline Corrections' web page:
{\url http://www.vla.nrao.edu/astro/archive/baselines}. The table
given on this webpage has a format like:\\ 

\begin{verbatim}
;                2010 BASELINE CORRECTIONS IN METERS
;ANT
;MOVED OBSDATE   Put_In_ MC(IAT)  ANT  PAD    Bx      By      Bz
;
JAN27   FEB12     FEB21  01:57    11   E04  0.0000  0.0000  0.0000
JAN27   FEB12     FEB21  01:57    26   W03 -0.0170  0.0204  0.0041
MAR24   MAR25     MAR26  18:28    17   W07 -0.0061 -0.0069 -0.0055
APR21   MAY02     MAY04  23:25    12   E08 -0.0072  0.0045 -0.0017
\end{verbatim}


If your observations fall in between the 'Antenna Moved' and
'Put\_In\_' dates of a given antenna, you may choose to apply the
offsets in that table; the 'Put\_In\_' time stamp marks the date where
the more accurate solution was introduced in the data stream directly
and no correction is required anymore. In {\tt gencal} the offsets
will be inserted as:



\small
\begin{verbatim}
gencal(vis='test.ms',caltable='test.antpos',caltype='antpos', \
       antenna='EA09,EA10', \
       parameter=[0.01,0.02,0.03, -0.03,-0.01,-0.02])
\end{verbatim}
\normalsize


In the following example, antenna position corrections (in the
traditional VLA-centric frame) will be introduced in meters for
antenna EA09 (dBx=0.01, dBy=0.02, dBz=0.03) and for antenna EA10
(dBx=-0.03, dBy=-0.01, dBz=-0.02) These offsets will be rotated to the
ITRF frame before storing them in the caltable.

\small
\begin{verbatim}
gencal(vis='test.ms',caltable='test.antposvla',caltype='antposvla', \
       antenna='EA09,EA10', \
       parameter=[0.01,0.02,0.03, -0.03,-0.01,-0.02])
\end{verbatim}
\normalsize

{\tt gencal} is also the task to generate gaincurve, antenna
efficiency, and opacity tables. The first two items can be determined
together with {\tt caltype='gceff'} and the latter with {\tt
  caltype='opac'}. These tables are treated just like any other
calibration table and will be carried through the calibration
steps. This method replaces the older method where 'gaincurve' and
'opacity' keywords were present in calibration tasks such as {\tt
  gaincal}, {\tt bandpass}, or {\tt applycal}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applying Jansky VLA switched power  or ALMA Tsys
   using {\tt gencal}}
\label{section:cal.prior.gencalevlagains}
Noise diodes in the Jansky VLA antennas can be used to pre-calibrate the
data. The diodes follow an ON-OFF cycle and the power for both states is
measured and recorded. This is called the 'VLA switched power'
calibration.  To apply the switched power data, one needs to 
create a calibration table with {\tt gencal} using {\tt
  caltype='evlagain'}, like 

\small
\begin{verbatim}
gencal(vis='test.ms',caltable='VLAswitchedpower.cal',caltype='evlagain')
\end{verbatim}
\normalsize

For ALMA the calibration of system temperature is done via hot loads
and the data recorded similar to the VLA in the measurement set (ALMA
will provide measurement sets where these data are available. To
derive the calibration table from it, use {\tt caltype='tsys'}:

\small
\begin{verbatim}
gencal(vis='test.ms',caltable='ALMAtsys.cal',caltype='tsys')
\end{verbatim}
\normalsize


This calibration tables created for ALMA or VLA are then carried
along all further calibration steps in the {\tt gaintable} parameter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ Generate a gain table based on Water Vapor Radiometer data {\tt wvrgcal}}
\label{section:cal.prior.wvrgcal}

\small
\begin{verbatim}
#  wvrgcal :: Generate a gain table based on Water Vapour Radiometer data
vis                 =         ''        #  Name of input visibility file
caltable            =         ''        #  Name of output gain calibration table
toffset             =          0        #  Time offset (sec) between
                                        #   interferometric and WVR data
segsource           =       True        #  Do a new coefficient calculation for each source
     tie            =         []        #  Prioritise tieing the phase
                                        #   of these sources as well
                                        #   as possible (requires segsource=True)
     sourceflag     =         []        #  Flag the WVR data for these
                                        #   source(s) as bad and do
                                        #   not produce corrections
                                        #   for it (requires segsource=True)
disperse            =      False        #  Apply correction for dispersion
wvrflag             =       ['']        #  Flag the WVR data for these
                                        #    antenna(s) as bad and
                                        #    replace its data with interpolated values
statfield           =         ''        #  Compute the statistics
                                        #    (Phase RMS, Disc) on this field only
statsource          =         ''        #  Compute the statistics
                                        #   (Phase RMS, Disc) on this source only
smooth              =         ''        #  Smooth calibration solution on the given timescale
scale               =        1.0        #  Scale the entire phase correction by this factor
reversespw          =         ''        #  Reverse the sign of the
                                        #   correction for the listed
                                        #   SPWs (only needed for early ALMA data before Cycle 0)
cont                =      False        #  Estimate the continuum (e.g., due to clouds) (experimental)
maxdistm            =      500.0        #  maximum distance (m) of an
                                        #   antenna used for interpolation for a flagged antenna
minnumants          =          2        #   minimum number of near
                                        #   antennas (up to 3) required for interpolation
mingoodfrac         =        0.8        #  If the fraction of
                                        #   unflagged data for an
                                        #   antenna is below this
                                        #   value (0. to 1.), the antenna is flagged.
async               =      False        #  If true the taskname must be started using wvrgcal(...)


\end{verbatim}
\normalsize

The task {\tt wvrgcal} generates a gain table based on Water Vapor
Radiometer (WVR) data and is used for ALMA data reduction. It is an
interface to the executable ``wvrgcal'' which is part of the CASA 4.0
distribution and can also be called from outside CASA. The wvrgcal
software is based on the libair and libbnmin libraries which were
developed by Bojan Nikolic at the University of Cambridge as part of
EU FP6 ALMA Enhancement program.

CASA 4.0 contains version 1.2.1 of wvrgcal. Source code of the
stand-alone package and links to documentation can be found at
\url{http://www.mrao.cam.ac.uk/\~bn204/alma/wvrsoft.html}. In
particular, there are three ALMA memos (number 587, 588, and 593
(submitted)) which describe the algorithms implemented in the
software. They can be found at \url{http://www.alma.cl/almamemos}.
The only recently submitted memo 593 can be found at
\url{http://xxx.lanl.gov/pdf/1207.6069}.

Briefly, wvrgcal follows a Bayesian approach to calculate the
coefficients that convert the outputs of the ALMA 183 GHz water-vapor
radiometers (mounted on each antenna) into estimates of path
fluctuations which can then be used to correct the observed
interferometric visibilities.

The CASA task interface to wvrgcal follows closely the interface of
the shell executable at the same time staying within the CASA task
parameter conventions.

In ALMA data, the WVR measurements belonging to a given observation
are contained in the ASDM for that observation. After conversion to an
MS using {\tt importasdm}, the WVR information can be found in
separate spectral windows (as of September 2012, it is still spectral
window id 0).  This spectral window must be present in the MS for
wvrgcal to work.

The various features of wvrgcal are then controlled by a number of
task parameters (see the list above).  They have default values which
will work for ALMA data.  An example for a typical wvrgcal call can be
found in the ALMA CASA guide for the NGC 3256 analysis:

\small
\begin{verbatim}
wvrgcal(vis='uid___A002_X1d54a1_X5.ms', caltable='cal-wvr-uid___A002_X1d54a1_X5.W',  
        toffset=-1, segsource=True, tie=["Titan,1037-295,NGC3256"], statsource="1037-295")
\end{verbatim}
\normalsize

Here, {\tt vis} is the name of input visibility file (which as
mentioned above also contains the WVR data in spectral window 0) and
{\tt caltable} is the name of the output gain calibration table.

{\tt toffset} is the known time offset in seconds between the WVR
measurements and the visibility integrations they are valid for. For
ALMA, this offset is presently -1~s (which is also the default value).

The parameter {\tt segsource} (segregate source) controls whether
separate coefficients are calculated for each source. The default
value True is the recommended one for ALMA.  When {\tt segsource} is
True, the subparameter {\tt tie} is available. It permits to form
groups of sources for which common coefficients are calculated as well
as possible. The {\tt tie} parameter ensures best possible phase
transfer between a group of sources. In general it is recommended to
tie together all of the sources in a single Science Goal (in ALMA
speak) and their phase calibrator(s).  The recommended maximum angular
distance up to which two sources can be tied is 15$^\circ$.

The parameter {\tt statsource} controls for which sources statistics are calculated and
displayed in the logger. This has no influence on the generated calibration table.

{\tt wvrgcal} respects the flags in the Main and ANTENNA table of the
MS. The parameter {\it mingoodfrac} lets the user set a requirement on
the minimum fraction of good measurements for accepting the WVR data
from an antenna.  If antennas are flagged, their WVR solution is
interpolated from the three nearest neighbouring antennas.  This
process can be controlled with the new parameters {\it maxdistm} and
{\it minnumants}. The former sets the maximum distance an antenna used
for interpolation may have from the flagged one. And {\it minnumants} sets
how many near antennas there have to be for interpolation to take
place.


\subsubsection{Statistical parameters shown in the logger output of {\tt wvrgcal}}

{\tt wvrgcal} writes out a variety of information to
the logger, including various statistical measures of the performance.
This allows the user to judge whether WVR correction is appropriate
for the ms, to check whether any antennas have problematic WVR values,
and to examine the predicted performance of the WVR correction when
applied.

For each set of correction coefficients which are calculated (the
number of coefficient sets are controlled by the parameters
{\tt nsol}, {\tt segsource} and {\tt tie}), the wvrgcal output to the logger
first of all shows the
time sample, the individual temperatures of each of the four WVR
channels and the elevation of the source in question at that time. 

For each of these coefficient sets, it then gives the evidence of the
bayesian parameter estimation, the calculated precipitable water
vapour (PWV) and its error in mm, and the correction coefficients
found for each WVR channel (dTdL).

The output then shows the statistical information about the
observation. First of all it gives the start and end times for the
parts of the observation used to calculate these statistics
(controlled by {\tt segsource}).  It then shows a break down for each
of the antennas in the data set. This gives the antenna name and
number; whether or not it has a WVR (column {\tt WVR}); whether or not
it has been flagged (column {\tt FLAG}); the RMS of the path length
variation with time towards that antenna (column {\tt RMS}); and the
discrepancy between the RMS path length calculated separately for
different WVR channels (column {\tt Disc.}). These values allow the
user to see if an individual WVR appears to have been suffering from
problems during the observation, and to flag that antenna using {\tt
  wvrflag} if necessary.

This discrepancy value, {\tt Disc.}, can in addition be used as a simple
diagnostic tool to evaluate whether or not the WVR correction caltable
created by {\tt wvrgcal} should be applied. In the event of the WVR
observations being contaminated by strong cloud emission in the
atmosphere, the attempt by {\tt wvrgcal} to fit the water vapour line
may not be successful, and applying the produced calibration table can
in extreme cases reduce the quality of the data. However, these
weather conditions should identified by a high value in the
discrepancy column produced when running {\tt wvrgcal}.

Although there have not currently been enough cases checked to give
definitive limits, the available data sets as of summer 2012 suggest
that discrepancy values of greater than a 1000 microns usually indicate
strong cloud contamination of the WVR data, and the output calibration
table should probably not be applied. If the values are between 100
and 1000 microns, then the user should manually examine the
phases before and after applying the caltable to decide if WVR
correction is appropriate.

After the antenna-by-antenna statistics, the output then displays some
estimates of the performance of the {\tt wvrgcal} correction. 
These are the thermal contribution from the water vapour to the path
fluctuations per antenna (in microns), the largest path fluctuation
found on a baseline (in microns), and the expected error on the path
length calculated for each baseline due to the error in the
coefficients (in microns). 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other {\it a priori} Calibrations and Corrections}
\label{section:cal.prior.other}

Other {\it a priori} calibrations will be added to the 
{\tt calibrater} ({\tt cb}) tool 
in the near future.  These will include
instrumental line-length corrections, ionospheric corrections, etc.  Where
appropriate, solving capabilities for these effects will also be
added.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solving for Calibration --- Bandpass, Gain, Polarization}
\label{section:cal.solve}

The {\tt gaincal}, {\tt bandpass}, {\tt polcal}, and {\tt blcal}
tasks actually solve for the unknown calibration parameters from the
visibility data obtained on calibrator sources,
placing the results in a calibration table.  They take as input
an MS, and a number of parameters that specify any prior calibration
or previous calibration tables to pre-apply before computing the
solution.  These are placed in the proper sequence of the Measurement
Equation automatically.

We first discuss the parameters that are in common between many
of the calibration tasks.  Then we describe each solver in turn.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Common Calibration Solver Parameters}
\label{section:cal.solve.pars}

There are a number of parameters that are in common between 
the calibration ``solver'' tasks.  These also appear in some
of the other calibration manipulation and application tasks.

%%%%%%
\subsubsection{Parameters for Specification : {\tt vis} and
{\tt caltable} }
\label{section:cal.solve.pars.spec}

The input measurement set and output table are
controlled by the following parameters:
\small
\begin{verbatim}
vis          =         ''   #   Name of input visibility file
caltable     =         ''   #   Name of output calibration table
\end{verbatim}
\normalsize

The MS name is input in {\tt vis}.  If it is highlighted red
in the inputs (\S~\ref{section:intro.tasks.setpar.inp}) then it 
does not exist, and the task will not execute.  Check the name and
path in this case. 

The output table name is placed in {\tt caltable}.  Be sure to give a
unique name to the output table, or be careful.  If the table exists,
then what happens next will depend on the task and the values of other
parameters (e.g.  \S~\ref{section:cal.solve.pars.action}).  The task
may not execute giving a warning that the table already exists, or
will go ahead and overwrite the solutions in that table, or append
them.  Be careful.

%%%%%%
\subsubsection{Selection: {\tt field}, {\tt spw}, {\tt selectdata},
  {\tt intent}, and {\tt observation} }
\label{section:cal.solve.pars.select}

Selection is controlled by the parameters:
\small
\begin{verbatim}
field        =         ''   #   field names or index of calibrators: ''==>all
spw          =         ''   #   spectral window:channels: ''==>all 
intent       =         ''   #  Select observing intent
selectdata   =      False   #   Other data selection parameters
\end{verbatim}
\normalsize

Field and spectral window selection are so often used, that we have
made these standard parameters {\tt field} and {\tt spw}
respectively. {\tt intent} is the scan intent that was specified when
the observations were set up. They typically describe what was
intended with a specific scan, i.e. a flux or phase calibration, a
bandpass, a pointing, an observation of your target, or something else
or a combination. The format for the scan intents of your observations
are listed in the logger when you run {\tt listobs}. Minimum matching
with wildcards will work, like '*BANDPASS*'. This is especially useful
when multiple intents are attached to scans.  Finally, {\tt
  observation} is an identifier to distinguish between different
observing runs, mainly used for ALMA.

The {\tt selectdata} parameter expands as usual, uncovering other
selection sub-parameters:
\small
\begin{verbatim}
selectdata          =       True        #  data selection parameters
     field          =         ''        #  field names or field index
                                        #   numbers (blank for all)
     spw            =         ''        #  spectral windows:channels (blank for all)
     timerange      =         ''        #  time range (blank for all)
     uvrange        =         ''        #  uv range (blank for all)
     antenna        =         ''        #  antenna/baselines (blank for all)
     scan           =         ''        #  scan numbers (blank for all)
     correlation    =         ''        #  correlations (blank for all)
     array          =         ''        #  (sub)array numbers (blank for all)
     observation    =         ''        #  Select by observation ID(s)
     msselect       =         ''        #  MS selection (blank for all)
\end{verbatim}
\normalsize
Note that if {\tt selectdata=False} these parameters are not used when
the task is executed, even if set underneath.

The most common {\tt selectdata} parameter to use is {\tt uvrange},
which can be used to exclude longer baselines if the calibrator is
resolved, or short baselines of the calibrator contains extended flux
not accounted for in the model 
(e.g. \S~\ref{section:cal.prior.models.resolved}).

See \S~\ref{section:io.selection} for more on the selection parameters.

%%%%%%
\subsubsection{Prior Calibration and Correction: {\tt parang} }
\label{section:cal.solve.pars.prior}

These parameters control the on-the-fly application of various
calibration or effect-based corrections prior to the solving process.

The {\tt parang} parameter turns on the application of the
antenna-based parallactic angle correction ({\tt 'P'}) in the
measurement equation.  This is necessary for polarization calibration
and imaging, or for cases where the parallactic angles are different
for geographically spaced antennas and it is desired that the ordinary
gain calibration not absorb the inter-antenna parallactic angle phase.  
When dealing with only the parallel-hand data (e.g. RR, LL, XX, YY),
and an unpolarized calibrator model
for a co-located array (e.g. the VLA or ALMA), you can set {\tt  parang=False} 
and save some computational effort.  Otherwise, set {\tt parang=True} to apply
this correction.

See \S~\ref{section:cal.prior} for more on {\bf Prior Calibration},
including
how to invoke {\tt gaincurve} and {\tt opacity} correction using
{\tt gencal}.

%%%%%%
\subsubsection{Previous Calibration: {\tt gaintable},
{\tt gainfield}, {\tt interp} and {\tt spwmap} }
\label{section:cal.solve.pars.previous}

Calibration tables that have already been determined can also be
applied before solving for the new table:
\small
\begin{verbatim}
docallib        =  False        #  Use traditional cal apply parameters
     gaintable  =     []        #  Gain calibration table(s) to apply on the fly
     gainfield  =     []        #  Select a subset of calibrators from gaintable(s)
     interp     =     []        #  Interpolation mode (in time) to use for each gaintable
     spwmap     =     []        #  Spectral windows combinations to form for gaintable(s)
\end{verbatim}
\normalsize


This is controlled by the {\tt gaintable} parameter, which takes 
a string or list of strings giving one or more calibration tables 
to pre-apply.  For example,

\small
\begin{verbatim}
   gaintable = ['ngc5921.bcal','ngc5921.gcal']
\end{verbatim}
\normalsize
specifies two tables, in this case bandpass and gain calibration tables
respectively.

The other parameters key off {\tt gaintable}, taking single values or
lists, with an entry for each table in {\tt gaintable}.  The order is
given by that in {\tt gaintable}.

The {\tt gainfield} parameter specifies which fields from the
respective {\tt gaintable} to select for apply.  This is a list,
with each entry a string or list of strings.  The default 
{\tt ''} for an entry means to use all in that table.  For
example,
\small
\begin{verbatim}
   gaintable = ['ngc5921.bcal','ngc5921.gcal']
   gainfield = [ '1331+305', ['1331+305','1445+099'] ]
\end{verbatim}
\normalsize
or using indices
\small
\begin{verbatim}
   gainfield = [ '0', ['0','1'] ]
\end{verbatim}
\normalsize
to specify the field {\tt '1331+305'} from the table 
{\tt 'ngc5921.bcal'} and fields {\tt '1331+305'} and 
{\tt '1445+099'} from the second table 'ngc5921.gcal'.
We could also have wildcarded the selection, e.g.
\small
\begin{verbatim}
   gainfield = [ '0', '*' ]
\end{verbatim}
\normalsize
taking all fields from the second table.  And of course we could have
used the default
\small
\begin{verbatim}
   gainfield = [ '0', '' ]
\end{verbatim}
\normalsize
or even
\small
\begin{verbatim}
   gainfield = [ '0' ]
\end{verbatim}
\normalsize
which is to take all for the second table in {\tt gaintable}. In addition, {\tt gainfield} can be specified by
\small
\begin{verbatim}
   gainfield = [ 'nearest' ]
\end{verbatim}
\normalsize
which selects the calibrator that is the spatially closest (in sky
coordinates) to each of the selected
MS fields specified in the {\tt field} parameter. This is particularly
useful for running {\tt applycal} with a number of different sources
to be calibrated in a single run.


The {\tt interp} parameter chooses the interpolation scheme to be used
when pre-applying the solution in the tables.  Interpolation in both
time and frequency (for channel-dependent calibrations) are supported.
The choices are currently {\tt 'nearest'} and {\tt 'linear'}, and 
{\tt 'nearest'}, {\tt 'linear'}, {\tt cubic}, and {\tt spline} for 
frequency-dependent interpolation.   Frequency-dependent interpolation
is only relevant for channel-dependent calibration tables (like
bandpasses)
that are undersampled in frequency relative to the data.

\begin{itemize}
\item {\tt 'nearest'} just picks the entry nearest in time or freq to the
   visibility in question;

\item {\tt 'linear'} interpolation calibrates each datum with
   calibration phases and amplitudes linearly 
   interpolated from neighboring values. In the case of phase,
   this mode will assume that phase never jumps more than $180^\circ$
   between neighboring points, and so undersampled cycle-slips will
   not be corrected for.  Solutions will not be {\em extrapolated}
   arbitrarily in time or frequency for data before the first solution or after
   the last solution; such data will be calibrated using {\tt
   'nearest'} to avoid unreasonable extrapolations.

\item {\tt 'cubic'} interpolation forms a 3rd-order polynomial that
   passes through the nearest 4 calibration samples (separately in
   phase and amplitude

\item {\tt 'spline'} interpolation forms a cubic spline that
   passes through the nearest 4 calibration samples (separately in
   phase and amplitude

\end{itemize}

For each gaintable, specify the interpolation style in quotes, with
the frequency-dependent interpolation style specified after a comma,
if relevant.

If the uncalibrated phase is changing rapidly, a {\tt 'nearest'}
interpolation is not desirable. Usually, {\tt interp='linear'} is the
best choice. For example,
\small
\begin{verbatim}
   gaintable=['gain','bandpass']
   interp = [ 'nearest', 'linear,spline' ]
\end{verbatim}
\normalsize
uses nearest ``interpolation'' on the first table, and linear (in
time) and spline (in freq) on the second.

The {\tt spwmap} parameter sets the spectral window combinations to
form for the {\tt gaintable}(s).  This is a list, or a list of lists,
of integers giving the {\tt spw} IDs to map.  There is one list for
each table in {\tt gaintable}, with an entry for each ID in the MS.
For example,
\small
\begin{verbatim}
   spwmap=[0,0,1,1]                # apply from spw=0 to 0,1 and 1 to 2,3
\end{verbatim}
\normalsize
for an MS with {\tt spw=0,1,2,3}.  For multiple {\tt gaintable}, use
lists of lists, e.g.
\small
\begin{verbatim}
   spwmap=[ [0,0,1,1], [0,1,0,1] ] # 2nd table spw=0 to 0,2 and 1 to 1,3
\end{verbatim} 
\normalsize

%%%%%%
\subsubsection{Solving: {\tt solint}, {\tt combine},
{\tt preavg}, {\tt refant}, {\tt minblperant}, {\tt minsnr} }
\label{section:cal.solve.pars.solving}

The parameters controlling common aspects of the solution are:
\small
\begin{verbatim}
solint              =      'inf'        #  Solution interval: egs. 'inf', '60s' (see help)
combine             =     'scan'        #  Data axes which to combine for solve (obs, scan, 
                                        #   spw, and/or field)
preavg              =       -1.0        #  Pre-averaging interval (sec) (rarely needed)
refant              =         ''        #  Reference antenna name(s)
minblperant         =          4        #  Minimum baselines _per antenna_ required for solve
minsnr              =        3.0        #  Reject solutions below this SNR
\end{verbatim} 
\normalsize

The time and frequency (if relevant) solution interval is given by {\tt solint}. Optionally a
frequency interval for each solution can be added after a comma,
e.g. {\tt solint='60s,300Hz'}. Time units are in seconds unless
specified differently. Frequency units can be either channels or Hz
and only make sense for bandpass of frequency dependent polarization
calibration.  The special values {\tt 'inf'} and {\tt -1} specify an
``infinite'' solution interval encompassing the entire dataset, while
{\tt 'int'} or zero specify a solution every integration. You can
use time quanta in the string, e.g. {\tt solint='1min'} and {\tt
  solint='60s'} both specify solution intervals of one minute.  Note
that 'm' is a unit of distance (meters); 'min' must be used to specify
minutes.  The {\tt solint} parameter interacts with {\tt combine} to
determine whether the solutions cross scan or field boundaries.

The parameter controlling the scope of the solution is {\tt combine}.
For the default {\tt combine=''} solutions will break at obsId, scan,
field, and spw boundaries.  Specification of any of these in {\tt combine} will
extend the solutions over the boundaries (up to the {\tt solint}). 
For example, {\tt combine='spw'} will combine spectral windows
together for solving, while {\tt combine='scan'} will cross scans, and
{\tt combine='obs,scan'} will use data across different observation
IDs and scans (usually, obsIds consist of many scans, so it is
not meaningful to combine obsIds without also combining scans).  
Thus, to do scan-based solutions (single solution for each scan), set
\small
\begin{verbatim}
   solint = 'inf'
   combine = ''
\end{verbatim} 
\normalsize
while
\small
\begin{verbatim}
   solint = 'inf'
   combine = 'scan'
\end{verbatim} 
\normalsize
will make a single solution for the entire dataset (for a given field
and spw).  
\small
\begin{verbatim}
   solint = 'inf,30ch'
\end{verbatim}
will calculate a bandpass solution for each scan, averaging over 30
channels.

You can specify multiple choices for combination:
\small
\begin{verbatim}
   combine = 'scan,spw'
\end{verbatim} 
\normalsize
for example.

The reference antenna is specified by the {\tt refant} parameter. A
list of antennas can be provided to this parameter and if the first
antenna is not present in the data, the next antenna in the list will
be used, etc.  It is useful to ``lock'' the solutions with time,
effectively rotating (after solving) the phase of the gain solutions
for all antennas such that the reference antenna's phase is constant
at zero.  If the selected antenna drops out, another antenna will be
selected for ongoing consistency in time (at its ``current'' value)
until the refant returns, usually at a new value (not zero), which
will be kept fixed thenceforth.  You can also run without a reference
antenna, but in this case the solutions will formally float with time;
in practice, the first antenna will be approximately constant near
zero phase.  It is usually prudent to select an antenna in the center
of the array that is known to be particularly stable, as any gain
jumps or wanders in the {\tt refant} will be transferred to the other
antenna solutions.  Also, it is best to choose a reference antenna
that never drops out.



Setting a {\tt preavg} time (only needed in {\tt polcal}) will let you
average data over periods shorter than the solution interval first
before solving on longer timescales.

The minimum signal-to-noise ratio allowed for an acceptable solution
is specified in the {\tt minsnr} parameter.  Default is {\tt minsnr=3}.
The {\tt minblperant} parameter sets the minimum number of baselines
to other antennas that must be preset for each antenna to be
included in a solution.  This enables control of the constraints
that a solution will require for each antenna.

%%%%%%
\subsubsection{Action: {\tt append} and {\tt solnorm} }
\label{section:cal.solve.pars.action}

The following parameters control some things that happen after
solutions are obtained:
\small
\begin{verbatim}
solnorm      =      False   #   Normalize solution amplitudes post-solve.
append       =      False   #   Append solutions to (existing) table.
                            #    False will overwrite.
\end{verbatim} 
\normalsize

The {\tt solnorm} parameter toggles on the option to normalize the
solution after the solutions are obtained.  The exact
effect of this depends upon the type of solution.  Not all tasks
use this parameter.  

One should be aware when using {\tt solnorm} that if this is done
in the last stage of a chain of calibration, then the part of 
the calibration that is ``normalized'' away will be lost.  It is
best to use this in early stages (for example in a first bandpass
calibration) so that later stages (such as final gain calibration)
can absorb the lost normalization scaling.  It is not strictly
necessary to use {\tt solnorm=True} at all, but is sometimes helpful
if you want to have a normalized bandpass for example.

The {\tt append} parameter, if set to {\tt True}, will append the
solutions from this run to existing solutions in {\tt caltable}.
Of course, this only matters if the table already exists.  If
{\tt append=False} and {\tt caltable} exists, it will overwrite.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Spectral Bandpass Calibration ({\tt bandpass})}
\label{section:cal.solve.band}

For channelized data, it is usually desirable to solve for the gain
variations in frequency as well as in time.  Variation in frequency
arises as a result of non-uniform filter passbands or other frequency-dependent
effects in signal transmission.  It is usually the case that these
frequency-dependent effects vary on timescales much longer than the
time-dependent effects handled by the gain types 'G' and 'T'.  
Thus, it makes sense to solve for them as a separate term: 'B', using the
{\tt bandpass} task.

The inputs to {\tt bandpass} are:
\small
\begin{verbatim}
#  bandpass :: Calculates a bandpass calibration solution
vis                 =         ''        #  Name of input visibility file
caltable            =         ''        #  Name of output gain calibration table
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channels
intent              =         ''        #  Select observing intent
selectdata          =       True        #  Other data selection parameters
     timerange      =         ''        #  Select data based on time range
     uvrange        =         ''        #  Select data within uvrange (default units meters)
     antenna        =         ''        #  Select data based on antenna/baseline
     scan           =         ''        #  Scan number range
     observation    =         ''        #  Select by observation ID(s)
     msselect       =         ''        #  Optional complex data selection (ignore for now)

solint              =      'inf'        #  Solution interval in time[,freq]
combine             =     'scan'        #  Data axes which to combine
                                        #   for solve (obs, scan, spw, and/or field)
refant              =         ''        #  Reference antenna name(s)
minblperant         =          4        #  Minimum baselines _per
                                        #   antenna_ required for solve
minsnr              =        3.0        #  Reject solutions below this
                                        #   SNR (only applies for bandtype = B)
solnorm             =      False        #  Normalize average solution amplitudes to 1.0
bandtype            =        'B'        #  Type of bandpass solution (B or BPOLY)
     fillgaps       =          0        #  Fill flagged solution channels by interpolation

smodel              =         []        #  Point source Stokes parameters for source model.
append              =      False        #  Append solutions to the (existing) table
docallib            =      False        #  Use callib or traditional cal apply parameters
     gaintable      =         []        #  Gain calibration table(s) to apply on the fly
     gainfield      =         []        #  Select a subset of calibrators from gaintable(s)
     interp         =         []        #  Interpolation mode (in
                                        #   time) to use for each gaintable
     spwmap         =         []        #  Spectral windows
                                        #   combinations to form for gaintable(s)

parang              =      False        #  Apply parallactic angle correction
async               =      False        #  If true the taskname must
                                        #   be started using bandpass(...)
\end{verbatim}
\normalsize

Many of these parameters are in common with the other calibration
tasks and are described above in \S~\ref{section:cal.solve.pars}.

The {\tt bandtype} parameter selects the type of solution used for the
bandpass.  The choices are {\tt 'B'} and {\tt 'BPOLY'}.  The former 
solves for a complex gain in each channel in the selected part of the
MS. See \S~\ref{section:cal.solve.band.b} for more on {\tt 'B'}.
The latter uses a polynomial as a function of channel to fit the
bandpass, and expands further to reveal a number of sub-parameters
See \S~\ref{section:cal.solve.band.bpoly} for more on {\tt 'BPOLY'}.

It is usually best to solve for the bandpass in channel data before
solving for the gain as a function of time.  However, if the gains of
the bandpass calibrator observations are fluctuating over the
timerange of those observations, then it can be helpful to first solve
for the gains of that source with {\tt gaincal} , and input these to
{\tt bandpass} via {\tt gaintable}.  See more below on this strategy.

We now describe the issue of bandpass normalization, followed by
a description of the options {\tt bandtype='B'} and {\tt bandtype='BPOLY'}.

%%%%%%
\subsubsection{Bandpass Normalization}
\label{section:cal.solve.band.solnorm}

The {\tt solnorm} parameter (\S~\ref{section:cal.solve.pars.action})
deserves more explanation in the context of the bandpass.  Most users
are used to seeing a normalized bandpass, where the mean amplitude
is unity and fiducial phase is zero. 
The toggle {\tt solnorm=True} allows this.  However, the parts of the
bandpass solution normalized away will be still left in the data,
and thus you should not use {\tt solnorm=True} if the {\tt bandpass}
calibration is the end of your calibration sequence (e.g. you have
already done all the gain calibration you want to).  Note that
setting {\tt solnorm=True} will NOT rescale any previous calibration
tables that the user may have supplied in {\tt gaintable}.

You can safely use {\tt solnorm=True} if you do the bandpass first
(perhaps after a throw-away initial gain calibration) as we suggest above in
\S~\ref{section:cal.flow}, as later gain calibration stages will deal with this
remaining calibration term.  This does have the benefit of isolating
the overall (channel independent) gains to the following {\tt gaincal}
stage.  It is also recommended for the case where you have multiple
scans on possibly different bandpass calibrators.  It may also be 
preferred when applying the bandpass before doing {\tt gaincal} and 
then {\tt fluxscale} (\S~\ref{section:cal.solve.fluxscale}), 
as significant variation of bandpass among antennas could otherwise 
enter the gain solution and make (probably subtle) adjustments to the
flux scale.

We finally note that {\tt solnorm=False} at the bandpass step in the
calibration chain will still in the end produce the correct results.  It
only means that there will be a part of what we usually think of the
gain calibration inside the bandpass solution, particularly if
{\tt bandpass} is run as the first step.

%%%%%%
\subsubsection{B solutions}
\label{section:cal.solve.band.b}

Calibration type {\tt 'B'} differs from {\tt 'G'} only in that it is
determined for each channel in each spectral window.  It is possible
to solve for it as a function of time, but it is most efficient to
keep the {\tt 'B'} solving timescale as long as possible, and use {\tt
'G'} or {\tt 'T'} for frequency-independent rapid  time-scale variations.

The {\tt 'B'} solutions are limited by the signal-to-noise ratio
available per channel, which may be quite small.  It is therefore
important that the data be coherent over the time-range of the {\tt
'B'} solutions.  As a result, {\tt 'B'} solutions are almost always
preceded by an initial {\tt 'G'} or {\tt 'T'} solve using {\tt
gaincal} (\S~\ref{section:cal.solve.gain}).  In turn, if the {\tt 'B'}
solution improves the frequency domain coherence significantly, a {\tt
'G'} or {\tt 'T'} solution following it will be better than the
original.

For example, to solve for a {\tt 'B'} bandpass using a single short
scan on the calibrator, then
\small
\begin{verbatim}
default('bandpass')

vis = 'n5921.ms'
caltable = 'n5921.bcal'
gaintable = ''                   # No gain tables yet
gainfield = ''
interp = ''
field = '0'                      # Calibrator 1331+305 = 3C286 (FIELD_ID 0)
spw = ''                         # all channels
selectdata = False               # No other selection
bandtype = 'B'                   # standard time-binned B (rather than BPOLY)
solint = 'inf'                   # set solution interval arbitrarily long
refant = '15'                    # ref antenna 15 (=VLA:N2) (ID 14)

bandpass()
\end{verbatim}
\normalsize

On the other hand, we might have a number of scans on the bandpass
calibrator spread over time, but we want a single bandpass solution.
In this case, we could solve for and then pre-apply an initial gain
calibration, and let the bandpass solution cross scans:
\small
\begin{verbatim}
gaintable = 'n5921.init.gcal'    # Our previously determined G table
gainfield = '0'
interp = 'linear'                # Do linear interpolation
solint = 'inf'                   # One interval over dataset
combine = 'scan'                 # Solution crosses scans
\end{verbatim}
\normalsize

Note that we obtained a bandpass solution for all channels in the MS.
If explicit channel selection is desired, for example some channels 
are useless and can be avoided entirely (e.g. edge channels or those
dominated by Gibbs ringing), then {\tt spw} can be set to select only
these channels, e.g.
\small
\begin{verbatim}
spw = '0:4~59'                   # channels 4-59 of spw 0
\end{verbatim}
\normalsize
This is not so critical for {\tt 'B'} solutions as for {\tt 'BPOLY'},
as each channel is solved for independently, and poor solutions at
edges can be ignored.

If you have multiple time solutions, then these will be applied using
whatever time interpolation scheme is specified in later tasks. 

The {\tt combine} parameter (\S~\ref{section:cal.solve.pars.solving}) 
can be used to combine data across spectral windows, scans, and fields.

%%%%%
\subsubsection{BPOLY solutions}
\label{section:cal.solve.band.bpoly}

For some observations, it may be the case that the SNR per channel is
insufficient to obtain a usable per-channel {\tt 'B'} solution.  In this
case it is desirable to solve instead for a best-fit functional form
for each antenna using the {\tt bandtype='BPOLY'} solver. 
The {\tt 'BPOLY'} solver naturally enough fits (Chebychev) polynomials to the
amplitude and phase of the calibrator 
visibilities as a function of frequency.  Unlike ordinary {\tt 'B'}, a
single common {\tt 'BPOLY'} solution will be determined for all spectral
windows specified (or implicit) in the selection.  As
such, it is usually most meaningful to select individual spectral
windows for {\tt 'BPOLY'} solves, unless groups of adjacent spectral windows
are known {\it a priori} to share a single continuous bandpass
response over their combined frequency range (e.g., PdBI data).

The {\tt 'BPOLY'} solver requires a number of unique sub-parameters:
\small
\begin{verbatim}
bandtype        =    'BPOLY'   #   Type of bandpass solution (B or BPOLY)
     degamp     =          3   #   Polynomial degree for BPOLY amplitude solution
     degphase   =          3   #   Polynomial degree for BPOLY phase solution
     visnorm    =      False   #   Normalize data prior to BPOLY solution
     maskcenter =          0   #   Number of channels in BPOLY to avoid in center of band
     maskedge   =          0   #   Percent of channels in BPOLY to avoid at each band edge
\end{verbatim}
\normalsize
The {\tt degamp} and {\tt degphase} parameters indicate the polynomial degree
desired for the amplitude and phase solutions.  The {\tt maskcenter}
parameter is used to indicate the number of channels in the center
of the band to avoid passing to the solution (e.g., to avoid Gibbs
ringing in central channels for PdBI data).  The {\tt maskedge} drops
beginning and end channels.  The {\tt visnorm} parameter turns on
normalization before the solution is obtained (rather than after for
{\tt solnorm}).

The {\tt combine} parameter (\S~\ref{section:cal.solve.pars.solving}) 
can be used to combine data across spectral windows, scans, and
fields.

Note that {\tt bandpass} will allow you to use multiple {\tt field}s,
and can determine a single solution for all specified fields using
{\tt combine='field'}.   If you want to use more than one
field in the solution it is prudent to use an initial {\tt gaincal}
using proper flux densities for all sources (not just 1Jy)
and use this table as an input to bandpass
because in general the phase towards two (widely separated) sources
will not be sufficiently similar to combine them, and you want the
same amplitude scale.  If you do not
include amplitude in the initial {\tt gaincal}, you probably want
to set {\tt visnorm=True} also to take out the amplitude normalization
change.  Note also in 
the case of multiple {\tt field}s, that the {\tt 'BPOLY'} solution 
will be labeled with the field ID of the first {\tt field} used in
the {\tt 'BPOLY'} solution, so if for example you point {\tt plotcal} at the
name or ID of one of the other fields used in the solution, 
plotcal does not plot.

For example, to solve for a {\tt 'BPOLY'} (5th order in amplitude, 7th order
in phase), using data from field 2, with {\tt G} corrections pre-applied:
\small
\begin{verbatim}
bandpass(vis='data.ms',          # input data set
         caltable='cal.BPOLY',   #
         spw='0:2~56',           # Use channels 3-57 (avoid end channels)
         field='0',              # Select bandpass calibrator (field 0)
         bandtype='BPOLY',       # Select bandpass polynomials
           degamp=5,             #   5th order amp
           degphase=7,           #   7th order phase
         gaintable='cal.G',      # Pre-apply gain solutions derived previously
         refant='14')            #   
\end{verbatim}
\normalsize

\subsubsection{What if the bandpass calibrator has a significant slope?}
The bandpass calibrator can have a spectral slope that will change the
spectral properties of the solutions. If the slope is significant, the
best way is to model the slope and store that model in the bandpass
calibrator MS. To do so, go through the normal steps of {\tt bandpass}
and the {\tt gaincal} runs on the bandpass and flux calibrators,
followed by {\tt setjy} of the flux calibrator. The next step would be
to use {\tt fluxscale} on the bandpass calibrator to derive the slope
of it. {\tt fluxscale} can store this information in a python
dictionary which is subsequently fed into a second {\tt setjy} run,
this time using the bandpass calibrator as the source and the derived
slope (the python dictionary) as input. This step will
create a source model with the correct overall spectral slope for the
bandpass. Finally, rerun {\tt bandpass} and all other
calibration steps again, making use of the newly created internal
bandpass model.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Complex Gain Calibration ({\tt gaincal})}
\label{section:cal.solve.gain}

The fundamental calibration to be done on your interferometer data
is to calibrate the antenna-based gains as a function of time. Some of
these calibrations are known beforehand (``a priori'') and others
must be determined from observations of calibrators, or from observations
of the target itself (``self-calibration'').

It is best to have determined a (constant or slowly-varying) ``bandpass'' from the
frequency channels by solving for the bandpass (see above).  Thus,
the {\tt bandpass} calibration table would be input to {\tt gaincal} via
the {\tt gaintable} parameter (see below).

The {\tt gaincal} task has the following inputs:
\small
\begin{verbatim}
#  gaincal :: Determine temporal gains from calibrator observations
vis                 =         ''        #  Name of input visibility file
caltable            =         ''        #  Name of output gain calibration table
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channels
intent              =         ''        #  Select observing intent
selectdata          =       True        #  Other data selection parameters
     timerange      =         ''        #  Select data based on time range
     uvrange        =         ''        #  Select data within uvrange (default units meters)
     antenna        =         ''        #  Select data based on antenna/baseline
     scan           =         ''        #  Scan number range
     observation    =         ''        #  Select by observation ID(s)
     msselect       =         ''        #  Optional complex data selection (ignore for now)

solint              =      'inf'        #  Solution interval: egs. 'inf', '60s' (see help)
combine             =         ''        #  Data axes which to combine
                                        #   for solve (obs, scan, spw, and/or field)
preavg              =       -1.0        #  Pre-averaging interval (sec) (rarely needed)
refant              =         ''        #  Reference antenna name(s)
minblperant         =          4        #  Minimum baselines _per antenna_ required for solve
minsnr              =        3.0        #  Reject solutions below this SNR
solnorm             =      False        #  Normalize average solution
                                        #   amplitudes to 1.0 (G, T only)
gaintype            =        'G'        #  Type of gain solution (G,T,GSPLINE,K,KCROSS)
smodel              =         []        #  Point source Stokes parameters for source model.
calmode             =       'ap'        #  Type of solution: ('ap', 'p', 'a')
append              =      False        #  Append solutions to the (existing) table
docallib            =      False        #  Use callib or traditional cal apply parameters
     gaintable      =         []        #  Gain calibration table(s) to apply on the fly
     gainfield      =         []        #  Select a subset of calibrators from gaintable(s)
     interp         =         []        #  Temporal interpolation for
                                        #   each gaintable (=linear)
     spwmap         =         []        #  Spectral windows
                                        #   combinations to form for gaintable(s)

parang              =      False        #  Apply parallactic angle correction on the fly
async               =      False        #  If true the taskname must
                                        #   be started using gaincal(...)
\end{verbatim}
\normalsize
Data selection is done through the standard {\tt field}, {\tt spw},
{\tt intent}, and 
{\tt selectdata} expandable sub-parameters (see \S~\ref{section:io.selection}).
The bulk of the other parameters are the standard solver parameters.  See
\S~\ref{section:cal.solve.pars} above for a description of these.

The {\tt gaintype} parameter selects the type of gain solution to
compute.  The choices are {\tt 'T'}, {\tt 'G'}, and {\tt 'GSPLINE'}.
The {\tt 'G'} and {\tt 'T'} options solve for independent complex
gains in each solution interval (classic AIPS style), with {\tt 'T'} 
enforcing a single polarization-independent gain for each co-polar
correlation (e.g. {\tt RR} and {\tt LL}, or {\tt XX} and {\tt YY})
and {\tt 'G'} having independent gains for these.  
See \S~\ref{section:cal.solve.gain.g} for a more detailed description
of {\tt 'G'} solutions, and \S~\ref{section:cal.solve.gain.t} for more
on {\tt 'T'}.  The {\tt 'GSPLINE'} fits cubic splines to the gain as
a function of time.  See \S~\ref{section:cal.solve.gain.gspline} for
more on this option.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Polarization-dependent Gain (G)}
\label{section:cal.solve.gain.g}

Systematic time-dependent complex gain errors are almost always the
dominant calibration effect, and a solution for them is almost always
necessary before proceeding with any other calibration.
Traditionally, this calibration type has been a catch-all for a
variety of similar effects, including: the relative amplitude and
phase gain for each antenna, phase and amplitude drifts in the
electronics of each antenna, amplitude response as a function of
elevation (gain curve), and tropospheric amplitude and phase effects.
In CASA, it is possible to handle many of these effects separately, as
available information and circumstances warrant, but it is still
possible to solve for the net effect using calibration type G.

Generally speaking, type G can represent any per-spectral window
multiplicative polarization- and time-dependent complex gain effect
downstream of the polarizers.  (Polarization- and time- {\it independent} effects
{\it upstream} of the polarizers may also be treated implicitly with G.)
Multi-channel data (per spectral window) will be averaged in frequency
before solving (use calibration type B to solve for
frequency-dependent effects within each spectral window).

To solve for G on, say, fields 1 \& 2, on a 90s timescale, and do so
relative to gaincurve corrections:
\small
\begin{verbatim}
gaincal('data.ms',
        caltable='cal.G',       # Write solutions to disk file 'cal.G'
        field='0,1',            # Restrict field selection
        solint=90.0,            # Solve for phase and amp on a 90s timescale
        gaintable=['cal.gc']    # a gain curve table from gencal
        refant='3')             #
			        
plotcal('cal.G','amp')          # Inspect solutions
\end{verbatim}
\normalsize

These G solution will be referenced to antenna 4.  Choose a
well-behaved antenna that is located near the center of the array and
is ever-present for the reference antenna.  For non-polarization
datasets, reference antennas need not be specified although you can if
you want.  If no reference antenna is specified, an effective phase
reference that is an average over the data will be calculated and
used.  For data that requires polarization calibration, you must
choose a reference antenna that has a constant phase difference
between the right and left polarizations (e.g. no phase jumps or
drifts).  If no reference antenna (or a poor one) is specified, the
phase reference may have jumps in the R--L phase, and the resulting
polarization angle response will vary during the observation, thus
corrupting the polarization imaging.

To apply this solution, along with the gain curve correction, to 
the calibrators (fields 0,1) and the target source (field
2):
\small
\begin{verbatim}
applycal('data.ms',
         field='0,1,2',                # Restrict field selection (cals + src)
         gaintable=['cal.gc','cal.G']) # Apply gc and G solutions to correct data
\end{verbatim}
\normalsize
The calibrated data is written to the {\tt CORRECTED\_DATA} column, with 
{\tt calwt=True} by default. This parameter can also be a list of
Boolean values for which each entry then controls the calculation of
weights based on each individual input calibration table. {\tt
  calwt=False} will recompute the weights form the SIGMA column, thus
resetting the weights to their original value.

{\bf Alert:} {\it Current} (as of February 2014) Jansky VLA data has
no calibrated weights (unless they are computed from switched power
calibration). To avoid trouble, {\tt calwt=False} should be set for
those data sets. Older, pre-upgrade VLA data should still be calibrated with {\tt
  calwt=True}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Polarization-independent Gain (T)}
\label{section:cal.solve.gain.t}

At high frequencies, it is often the case that the most rapid
time-dependent gain errors are introduced by the troposphere, and are
polarization-independent.  It is therefore unnecessary to solve for
separate time-dependent solutions for both polarizations, as is the
case for {\tt 'G'}.  Calibration type {\tt 'T'} is available to calibrate such
tropospheric effects, differing from {\tt 'G'} only in that a single common
solution for both polarizations is determined.  In cases where only
one polarization is observed, type {\tt 'T'} is adequate to describe the
time-dependent complex multiplicative gain calibration.

In the following example, we assume we have a {\tt 'G'} solution obtained on
a longish timescale (longer than a few minutes, say), and we want a residual
{\tt 'T'} solution to track the polarization-independent variations on a
very short timescale:

\small
\begin{verbatim}
gaincal('data.ms',               # Visibility dataset
        caltable='cal.T',        # Specify output table name
        gaintype='T',            # Solve for T
        field='0,1',             # Restrict data selection to calibrators
        solint=3.0,              # Obtain solutions on a 3s timescale
        gaintable='cal120.G')    # Pre-apply prior G solution
\end{verbatim}
\normalsize

For dual-polarization observations, it will always be necessary to
obtain a {\tt 'G'} solution to account for differences and drifts between
the polarizations (which traverse different electronics), but
solutions for rapidly varying polarization-independent effects such as
those introduced by the troposphere will be optimized by using {\tt 'T'}.
Note that {\tt 'T'} can be used in this way for self-calibration purposes,
too.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{GSPLINE solutions}
\label{section:cal.solve.gain.gspline}

At high radio frequencies, where tropospheric phase fluctuates
rapidly, it is often the case that there is insufficient
signal-to-noise ratio to obtain robust {\tt 'G'} or {\tt 'T'}
solutions on timescales short enough to track the 
variation.  In this case it is desirable to solve for a best-fit
functional form for each antenna using the {\tt 'GSPLINE'} solver.  
This fits a time-series of cubic B-splines to the phase and/or
amplitude of the calibrator visibilities.  

The {\tt combine} parameter (\S~\ref{section:cal.solve.pars.solving}) 
can be used to combine data across spectral windows, scans, and
fields.  Note that if you want to use {\tt combine='field'},
then all fields used to obtain a {\tt 'GSPLINE'} amplitude solution must have
models with accurate relative flux densities.  Use of incorrect
relative flux densities will introduce spurious variations in the
{\tt 'GSPLINE'} amplitude solution.

The {\tt 'GSPLINE'} solver requires a number of unique additional parameters,
compared to ordinary {\tt 'G'} and {\tt 'T'} solving.  The sub-parameters are:
\small
\begin{verbatim}
gaintype         =  'GSPLINE'   #   Type of solution (G, T, or GSPLINE)
     splinetime  =     3600.0   #   Spline (smooth) timescale (sec), default=1 hours
     npointaver  =          3   #   Points to average for phase wrap (okay)
     phasewrap   =        180   #   Wrap phase when greater than this (okay)
\end{verbatim}
\normalsize

The duration of each spline segment is controlled by {\tt splinetime}.
The actual splinetime will be adjusted such that an integral number of
equal-length spline segments will fit within the overall range of
data.

Phase splines require that cycle ambiguities be resolved prior to the
fit; this operation is controlled by {\tt npointaver} and {\tt
phasewrap}.  The {\tt npointaver} parameter controls how many
contiguous points in the time-series are used to predict the cycle
ambiguity of the next point in the time-series, and {\tt phasewrap} sets
the threshold phase jump (in degrees) that would indicate a cycle
slip.  Large values of {\tt npointaver} improve the SNR of the cycle
estimate, but tend to frustrate ambiguity detection if the phase rates
are large.  The {\tt phasewrap} parameter may be adjusted to influence
when cycles are detected.  Generally speaking, large values
($>180^\circ$) are useful when SNR is high and phase rates are
low. Smaller values for {\tt phasewrap} can force cycle slip detection
when low SNR conspires to obscure the jump, but the algorithm becomes
significantly less robust.  More robust algorithms for phase-tracking
are under development (including fringe-fitting).

For example, to solve for {\tt 'GSPLINE'} phase and amplitudes, with
splines of duration 600 seconds, 
\small
\begin{verbatim}
gaincal('data.ms',
        caltable='cal.spline.ap',
        gaintype='GSPLINE'       #   Solve for GSPLINE
        calmode='ap'             #   Solve for amp & phase
        field='0,1',             #   Restrict data selection to calibrators
        splinetime=600.)         #   Set spline timescale to 10min
\end{verbatim}
\normalsize

{\bf ALERT':} The {\tt 'GSPLINE'} solutions cannot yet be
used in {\tt fluxscale}.  You should do at least some {\tt 'G'}
amplitude solutions to establish the flux scale, then do 
{\tt 'GSPLINE'} in phase before or after to fix up the short 
timescale variations.  Note that the ``phase tracking'' algorithm
in {\tt 'GSPLINE'} needs some improvement.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Antenna Delays --- 'K' solutions}
\label{section:cal.solve.gain.k}
{\tt gaintype='K'} solves for simple antenna-based delays
via Fourier transforms of the spectra on baselines to the
reference antenna. This is not a global fringe fit
but will be useful for deriving delays from data of
reasonable snr.  If {\tt combine} includes {\tt 'spw'},
multi-band delays solved jointly from all selected spectral 
windows will be determined, and will be identified with the
first spectral window id in the output caltable.  When applying
a multi-band delay table, {\tt spwmap} is required to distribute
the solutions to all spectral windows.

After solving for delays, a subsequent {\tt bandpass} is 
recommended to describe higher-order channel-dependent
variation in the phase (and amplitude).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Cross-Hand  Delays --- 'KCROSS' solutions}
\label{section:cal.solve.gain.kcross}
{\tt gaintype='KCROSS'} solves for a global cross-hand
delay.  Use {\tt parang=T} and apply prior gain and
bandpass solutions.   
{\bf Alert:} Multi-band delays are not yet supported for {\tt KCROSS}
solutions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Establishing the Flux Density Scale ({\tt fluxscale}) }
\label{section:cal.solve.fluxscale}

The {\tt 'G'} or {\tt 'T'} solutions obtained from calibrators for
which the flux 
density was unknown and assumed to be 1 Jansky are correct in a time- and
antenna- relative sense, but are mis-scaled by a factor equal to the
inverse of the square root of the true flux density.  This scaling can
be corrected by enforcing the constraint that mean gain amplitudes
determined from calibrators of unknown flux density should be the same
as determined from those with known flux densities.  The {\tt
fluxscale} task exists for this purpose.  

The inputs for fluxscale are:
\small
\begin{verbatim}
#  fluxscale :: Bootstrap the flux density scale from standard calibrators
vis                 =         ''        #  Name of input visibility file (MS)
caltable            =         ''        #  Name of input calibration table
fluxtable           =         ''        #  Name of output, flux-scaled calibration table
reference           =       ['']        #  Reference field name(s) (transfer flux scale
                                        #   FROM)
transfer            =       ['']        #  Transfer field name(s) (transfer flux scale
                                        #   TO), '' -> all
listfile            =         ''        #  Name of listfile that contains the fit
                                        #   information.  Default is  (no file).
append              =      False        #  Append solutions?
refspwmap           =       [-1]        #  Scale across spectral window boundaries.  See
                                        #   help fluxscale
incremental         =      False        #  incremental caltable
async               =      False        #  If true the taskname must be started using
                                        #   fluxscale(...)
\end{verbatim}
\normalsize

Before running {\tt fluxscale}, one must have first run {\tt setjy} for the
{\tt reference} sources and run a {\tt gaincal} that includes {\tt reference}
and {\tt transfer} fields.  After running {\tt fluxscale} the output
{\tt fluxtable} caltable will have been scaled such that the correct
scaling will be applied to the {\tt transfer} sources.

For example, given a {\tt 'G'} table, e.g. {\tt 'cal.G'},
containing solutions for a flux density calibrator (in this case 
{\tt '3C286'}) and for one or more gain calibrator sources with
unknown flux densities (in this example {\tt '0234+285'} and 
{\tt '0323+022'}):
\small
\begin{verbatim}
fluxscale(vis='data.ms',
          caltable='cal.G',                  # Select input table
          fluxtable= 'cal.Gflx',             # Write scaled solutions to cal.Gflx
          reference='3C286',                 # 3C286 = flux calibrator
          transfer='0234+258, 0323+022')     # Select calibrators to scale
\end{verbatim}
\normalsize


The output table, {\tt 'cal.Gflx'}, contains either the scaling
factors alone ({\tt incremental=T}) to be used alongside with the
input gain table {\tt 'cal.G'}, or a scaled version of the gain table
({\tt incremental=F}), that replaces it for the execution of {\tt applycal}.


Note that the assertion that the gain solutions are independent of the
calibrator includes the assumption that the gain amplitudes are
strictly not systematically time-dependent in any way.  While synthesis antennas
are designed as much as possible to achieve this goal, in practice, a
number of effects conspire to frustrate it.  When relevant, it is
advisable to pre-apply {\tt gaincurve} and {\tt opacity} 
corrections when solving
for the {\tt 'G'} solutions that will be flux-scaled (see 
\S~\ref{section:cal.prior} and \S~\ref{section:cal.solve.pars.prior}).
When the {\tt 'G'} solutions are essentially constant for each
calibrator separately, the fluxscale operation is likely to be robust.

{\tt fluxscale} will report the fluxes of each spw for each source. In
addition, it will attempt a fit across the spws of each source and
report a spectral index and curvature
($S\propto(\nu/\nu_0)^{\alpha+\beta*\log(\nu/\nu_0)}$)). This
information can be subsequently used to build up a model for the
spectral slope of a calibrator with the {\tt setjy} task if required. 

The {\tt fluxscale} task can be executed on either {\tt 'G'} or {\tt
'T'} solutions, but it should only be used on one of these types if
solutions exist for both and one was solved relative to the other (use
fluxscale only on the first of the two).  

{\bf ALERT:} The {\tt 'GSPLINE'} option is not yet supported in
{\tt fluxscale} (see \S~\ref{section:cal.solve.gain.gspline}).

If the {\tt reference} and {\tt transfer} fields were observed in different
spectral windows, the {\tt refspwmap} parameter may be used
to achieve the scaling calculation across spectral window boundaries.

The {\tt refspwmap} parameter functions similarly to the standard
{\tt spwmap} parameter (\S~\ref{section:cal.solve.pars.previous}),
and takes a list of indices
indicating the spectral window mapping for the reference fields,
such that {\tt refspwmap[i]=j} means that reference field amplitudes
from spectral window {\tt j} will be used for spectral window {\tt i}.

{\bf Note:} You should be careful when you have a dataset with
spectral windows with different bandwidths, and you
have observed the calibrators differently in the different {\tt spw}.
The flux-scaling will probably be different in windows with different
bandwidths.

For example,
\small
\begin{verbatim}
fluxscale(vis='data.ms',
          caltable='cal.G',                  # Select input table
          fluxtable= 'cal.Gflx',             # Write scaled solutions to cal.Gflx
          reference='3C286',                 # 3C286 = flux calibrator
          transfer='0234+258,0323+022'       # Select calibrators to scale
          refspwmap=[0,0,0])                 # Use spwid 0 scaling for spwids 1 & 2
\end{verbatim}
\normalsize
will use {\tt spw=0} to scale the others, while in
\small
\begin{verbatim}
fluxscale(vis='data.ms',
          caltable='cal.G',                  # Select input table
          fluxtable='cal.Gflx',              # Write scaled solutions to cal.Gflx
          reference='3C286',                 #  3C286 = flux calibrator,
          transfer='0234+285, 0323+022',     #  select calibrators to scale,
          refspwmap=[0,0,1,1])               #  select spwids for scaling,
\end{verbatim}
\normalsize
the reference amplitudes from spectral window 0 will be
used for spectral windows 0 and 1 and reference amplitudes from
spectral window 2 will be used for spectral windows 2 and 3.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Using Resolved Calibrators}
\label{section:cal.solve.fluxscale.resolved}

If the flux density calibrator is resolved, the assumption that it is
a point source will cause solutions on outlying antennas to be biased
in amplitude.  In turn, the {\tt fluxscale} step will be biased
on these antennas as well.  In general, it is best to use 
model for the calibrator, but if such a model is not available,
it is important to limit the solution on the flux density calibrator
to only the subset of antennas that have baselines short enough that
the point-source assumption is valid.  This can be done by using
{\tt antenna} and {\tt uvrange} selection when solving for the flux density
calibrator.  For example, if antennas 1 through 8 are the antennas
among which the baselines are short enough that the point-source
assumption is valid, and we want to be sure to limit the solutions to
the use of baselines shorter than 15000 wavelengths, then we can
assemble properly scaled solutions for the other calibrator as follows
(note: specifying both an antenna and a {\tt uvrange} constraint prevents
inclusion of antennas with only a small number of baselines within the
specified {\tt uvrange} from being included in the solution; such antennas
will have poorly constrained solutions):

As an example, we first solve for gain solutions for the flux density
calibrator (3C286 observed in field 0) using a subset of antennas
\small
\begin{verbatim}
gaincal(vis='data.ms',
        caltable='cal.G',        # write solutions to cal.G
        field='0'                # Select the flux density calibrator
        selectdata=True,         # Expand other selectors
        antenna='0~7',           #  antennas 0-7,
        uvrange='0~15klambda',   #  limit uvrange to 0-15klambda
        solint=90)               # on 90s timescales, write solutions
                                 # to table called cal.G
\end{verbatim}
\normalsize
Now solve for other calibrator (0234+285 in field 1) using all antennas
(implicitly) and append these solutions to the same table
\small
\begin{verbatim}
gaincal(vis='data.ms',
        caltable='cal.G',        # write solutions to cal.G
        field='1',
        solint=90,
        append=T)                # Set up to write to the same table
\end{verbatim}
\normalsize
Finally, run fluxscale to adjust scaling
\small
\begin{verbatim}
fluxscale(vis='data.ms',
          caltable='cal.G',      # Input table with unscaled cal solutions
          fluxtable='cal.Gflx',  # Write scaled solutions to cal.Gflx
          reference='3C286',     # Use 3c286 as ref with limited uvrange
          transfer='0234+285')   # Transfer scaling to 0234+285
\end{verbatim}
\normalsize

The {\tt fluxscale} calculation will be performed using only the
antennas common 
to both fields, but the result will be applied to all antennas on the
transfer field.  Note that one can nominally get by only with the
{\tt uvrange} selection, but you may find that you get strange
effects from some antennas only having visibilities to a subset of
the baselines and thus causing problems in the solving.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Instrumental Polarization Calibration (D,X)}
\label{section:cal.solve.pol}

Full support for instrumental polarization calibration for the 
circular feed basis (e.g., VLA) is provided in CASA.  Support for 
the linear feed basis (e.g., ALMA) is now practical (as of v4.0)
and is also described below.  The linear feed basis treatment
will continue to be expanded and streamlined for the v4.3 release.

The inputs to {\tt polcal} are:
\small
\begin{verbatim}
#  polcal :: Determine instrumental polarization calibrations
vis                 =         ''        #  Name of input visibility file
caltable            =         ''        #  Name of output gain calibration table
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channels
intent              =         ''        #  Select observing intent
selectdata          =       True        #  Other data selection parameters
     timerange      =         ''        #  Select data based on time range
     uvrange        =         ''        #  Select data within uvrange (default units meters)
     antenna        =         ''        #  Select data based on antenna/baseline
     scan           =         ''        #  Scan number range
     observation    =         ''        #  Select by observation ID(s)
     msselect       =         ''        #  Optional complex data selection (ignore for now)

solint              =      'inf'        #  Solution interval
combine             = 'obs,scan'        #  Data axes which to combine
                                        #   for solve (obs, scan, spw, and/or field)
preavg              =      300.0        #  Pre-averaging interval (sec)
refant              =         ''        #  Reference antenna name(s)
minblperant         =          4        #  Minimum baselines _per antenna_ required for solve
minsnr              =        3.0        #  Reject solutions below this SNR
poltype             =     'D+QU'        #  Type of instrumental
                                        #   polarization solution (see help)
smodel              =         []        #  Point source Stokes parameters for source model.
append              =      False        #  Append solutions to the (existing) table
docallib            =      False        #  Use callib or traditional cal apply parameters
     gaintable      =         []        #  Gain calibration table(s) to apply
     gainfield      =         []        #  Select a subset of calibrators from gaintable(s)
     interp         =         []        #  Interpolation mode (in
                                        #   time) to use for each gaintable
     spwmap         =         []        #  Spectral windows
                                        #   combinations to form for gaintable(s)

async               =      False        #  If true the taskname must
                                        #   be started using polcal(...)
\end{verbatim}
\normalsize
The {\tt polcal} task uses many of the standard calibration parameters
as described above in \S~\ref{section:cal.solve.pars}.

The key parameter controlling {\tt polcal} is {\tt poltype}.  The
choices are:
\begin{description}
\item{\tt 'D'} --- Solve for instrumental polarization (leakage D-terms),
using the transform of an IQU model; requires no
parallactic angle coverage, but if the source polarization is non-zero,
the gain calibration must have the correct R-L phase registration.
(Note: this is unlikely, so just use {\tt 'D+X'} to let the position
angle registration float.) This will produce a calibration table of
type {\bf D}.

\item{\tt 'D+X'} --- Solve for instrumental polarization D-terms and
the polarization position angle correction, using the transform of an
IQU model; this mode requires at least 2 distinct
parallactic angles to separate the net instrumental polarization and
the PA. This will produce a calibration table of
type {\tt 'D'}. {\bf ALERT:} no table of type {\tt 'X'} will be
produced, so you must follow this by a run of {\tt polcal} with
{\tt polmode='X'} (see below).

\item{\tt 'D+QU'} --- Solve for instrumental polarization and source 
$Q+iU$; requires at least 3 distinct parallactic angles to separate
the net instrumental polarization from the source Q and U.
Effectively sets the polarization PA to the value if the R-L phase
difference were $0^\circ$.  This will produce a calibration table of
type {\tt 'D'}. 

\item{\tt 'X'} --- Solve only for the position angle correction; best to use
this after getting the D-terms from one of the above modes.  Requires
the observation of a calibrator with known $Q+iU$ (or at least known $U/Q$).
This will produce a calibration table of type {\tt 'X'}. 

\item{\tt 'Dflls'} --- A specialized mode for instrumental
polarization solving for the linear feed basis.  This will probably
be consolidated with other options in a future release.

\end{description}

There are channelized solution modes for the above options.  For
example, substitute {\tt 'Df'} for {\tt 'D'} in the {\tt 'D*'} modes 
described above to get a channelized D-term solution; substitute 
{\tt 'Xf'} for {\tt 'X'} to get channelized position angle correction.

{\bf ALERT:} {\tt polcal} will obtain a separate D-term solution for
each {\tt field} supplied to it.  This limitation will be relaxed in 
the future, enabling more sensitive solutions.

%% gmoellen (2011Nov19) I don't understand the following...
%% as well as
%%flexibilities like solving for {\tt 'D+X'} using a single scan each of two or
%%more position angle calibrators.

%%%%%%
\subsubsection{Heuristics and Strategies for Polarization Calibration }
\label{section:cal.solve.pol.hstics}


{\bf ALERT:} This section concentrates on polarization calibration for
the circular feed basis.  It will be generalized to include the linear
feed basis for the v4.3 release.  See
\S~\ref{section:cal.solve.pol.example2} for the currently supported
processing steps for the linear feed basis.


Fundamentally, with good ordinary gain (and bandpass, if relevant)
calibration already in hand, good polarization calibration must
deliver both the instrumental polarization and position angle
calibration.  An unpolarized source can deliver only the first of
these, but does not require parallactic angle coverage.  A polarized
source can only deliver the position angle calibration also if its
polarization is known a priori.  Sources that are polarized, but with
unknown polarization, must always be observed with sufficient
parallactic angle coverage, where "sufficient" is determined by SNR
and the details of the solving mode.

These principles are stated assuming the instrumental polarization
solution is solved using the "linear approximation" where cross-terms
in more than a single product of the instrumental or source
polarizations are ignored in the Measurement Equation (see 
\S~\ref{chapter:me}).
A more general non-linearized solution, with sufficient SNR, may enable 
some relaxation of the requirements indicated here, and modes supporting
such an approach are currently under development.

For instrumental polarization calibration, there are 3 types of
calibrator choice:
\begin{center}
{\it CASA Polarization Calibration Modes}\\[5mm]
\begin{tabular}{|l|l|l|l|l|}
\hline
Cal Polarization & Parallactic Angles & model & 
    {\tt polmode} & Result \\
\hline
unpolarized & any & set $Q=U=0$ & {\tt 'D'} or {\tt 'Df'} & D-terms
  only \\
known non-zero & 2+ scans & set $Q,U$ & {\tt 'D+X'} or 
  {\tt 'Df+X'} & D-terms and PA \\
unknown & 2+ scans & ignored & {\tt 'D+QU'} or {\tt 'Df+QU'} &
  D-terms and source \\
\hline
\end{tabular}
\end{center}
Note that the parallactic angle ranges spanned by the scans in the
modes that require this should be large enough to give good separation
between the components of the solution.  In practice, $60^\circ$ is 
a good target.

Each of these solutions should be followed with a {\tt 'X'} solution
on a source with known polarization position angle (and correct $Q+iU$
in the model).  
{\bf ALERT:} {\tt polmode='D+X'} will soon be enhanced to 
deliver this automatically.
           
The {\tt polcal} task will solve for the {\tt 'D'} or {\tt 'X'} terms
using the model visibilities that are in the model attached to the MS.
Calibration of the parallel hands must have already been carried out
using {\tt gaincal} and/or {\tt bandpass} in order to align the phases
over time and frequency.  This calibration must be supplied through
the {\tt gaintable} parameters, but any cal-tables to be used in {\tt
  polcal} must agree (e.g. have been derived from) the data in the
{\tt DATA} column and the FT of the model.  Thus, for example, one
would not use the cal-table produced by {\tt fluxscale} as the
rescaled amplitudes would no longer agree with the contents of the model.

Be careful when using resolved calibrators for polarization
calibration.  A particular problem is if the structure in Q and U is
offset from that in I.  Use of a point model, or a resolved model for
I but point models for Q and U, can lead to errors in the {\tt 'X'} 
calibration.  Use of a {\tt uvrange} will help here.  The use of a
full-Stokes model with the correct polarization is the only way to 
ensure a correct calibration if these offsets are large.

\subsubsection{A Note on channelized polarization calibration}

When your data has more than one channel per spectral window, it is
important to note that the calibrator polarization estimate currently
assumes the source polarization signal is coherent across each
spectral window.  In this case, it is important to be sure there is no
large cross-hand delay still present in your data.  Unless the online
system has accounted for cross-hand delays (typically intended, but
not always achieved), the gain and bandpass calibration will only
correct for parallel-hand delay residuals since the two polarizations
are referenced independently.  Good gain and bandpass calibration will
typically leave a single cross-hand delay (and phase) residual from
the reference antenna.  Plots of cross-hand phases as a function of
frequency for a strongly polarized source (i.e., that dominates the
instrumental polarization) will show the cross-hand delay as a phase
slope with frequency.  This slope will be the same magnitude on all
baselines, but with different sign in the two cross-hand correlations.
This cross-hand delay can be estimated using the {\tt
gaintype='KCROSS'} mode of gaincal (in this case, using the strongly
polarized source 3C286):

\small
\begin{verbatim}
   default('gaincal')
   vis                 = 'polcal_20080224.cband.all.ms'
   caltable            = 'polcal.xdelcal'
   field               = '3C286'        
   spw                 =         ''        
   solint              =      'inf'    
   combine             =     'scan' 
   refant              =     'VA15'
   smodel              = [1.0,0.11,0.0,0.0]        
   gaintype            =     'KCROSS'        
   gaintable           = ['polcal.gcal','polcal.bcal']
   gaincal()
\end{verbatim}
\normalsize

Note that {\tt smodel} is used to specify that 3C286 is polarized; it is not
important to specify this polarization stokes parameters correctly, as only 
the delay will be solved for (not any absolute position angle or amplitude 
scaling).  The resulting solution should be carried forward and applied 
along with the gain (.gcal) and
bandpass (.bcal) solutions in subsequent polarization calibration steps.


%%%%%%
\subsubsection{A Polarization Calibration Example - Circular Feed
  Basis (e.g., VLA $\nu>1$ GHz)}
\label{section:cal.solve.pol.example}

In the following example, we do a standard {\tt 'D+QU'} solution on
the bright source BLLac ({\tt 2202+422}) which has been tracked
through a range in parallactic angle:
\small
\begin{verbatim}
   default('polcal')
   vis                 = 'polcal_20080224.cband.all.ms'
   caltable            = 'polcal.pcal'
   field               = '2202+422'        
   spw                 =         ''        
   solint              =      'inf'    
   combine             =     'scan' 
   preavg              =      300.0        
   refant              =     'VA15'        
   minsnr              =          3        
   poltype             =     'D+QU'        
   gaintable           = ['polcal.gcal','polcal.bcal','polcal.xdelcal]
   gainfield           =       ['']
   polcal()
\end{verbatim}
\normalsize This assumes {\tt setjy} and {\tt gaincal} have already
been run.  Note that the original gain-calibration table is used in
{\tt gaintable} so that what is in the model is in agreement with what
is in the {\tt gaintable}, rather than using the table resulting from
{\tt fluxscale}.

Now, we need to set the R-L phase using a scan on
3C48 ({\tt 0137+331}):
\small
\begin{verbatim}
   default('polcal')
   vis                 = 'polcal_20080224.cband.all.ms'
   caltable            = 'polcal.polx'
   field               = '0137+331'
   refant              =     'VA15'        
   minsnr              =          3        
   poltype             =        'X'
   smodel              = [1.0,-0.0348,-0.0217,0.0]  # the fractional Stokes for 3C48
   gaintable           = ['polcal.gcal','polcal.bcal','polcal.xdelcal','polcal.pcal']
   polcal()
\end{verbatim}
\normalsize

Note that the fractional polarization of 3C48 has been properly specified 
in {\tt smodel} here.


If, on the other hand, we had a scan on an unpolarized bright source,
for example 3C84 ({\tt 0319+415}), we could use this to calibrate the
leakages:
\small
\begin{verbatim}
   default('polcal')
   vis                 = 'polcal_20080224.cband.all.ms'
   caltable            = 'polcal.pcal'
   field               = '0319+415'
   refant              =     'VA15'        
   poltype             =     'D'        
   gaintable           = ['polcal.gcal','polcal.bcal','polcal.xdelcal]
   polcal()
\end{verbatim}
\normalsize
We would then do the {\tt 'X'} calibration as before (but using this
D-table in {\tt gaintable}).

%%%%%%
\subsubsection{A Polarization Calibration Example - Linear Feed
  Basis (e.g., ALMA, VLA $\nu<1$ GHz)}
\label{section:cal.solve.pol.example2}

CASA v4.0.0 introduces supports for instrumental polarization 
calibration for the linear feed basis at a level that is now
practical for the general user.  Some details remain to be
implemented with full flexibility, and much of what follows
will be streamlined for the v4.1 release.

Calibrating the instrumental polarization for the linear feed basis
is somewhat more complicated than the circular feed basis because
the polarization effects (source and instrument) appear in all
four correlations at first or zeroth order (whereas for circular
feeds, the polarization information only enters the parallel
hand correlations at second order).   As a result, e.g., the 
time-dependent gain calibration will be distorted by any non-zero 
source polarization, and some degree of iteration will be required to
isolate the gain calibration if the source polarization is not
initially known.  These complications can actually be used to 
advantage in solving for the instrumental calibration; in can
be shown, for example, that a significantly linearly polarized
calibrator enables a better instrumental polarization 
solution than an unpolarized calibrator.

In the following example, we show the processing steps for calibrating
the instrumental polarization using a strongly ($>$ 5\%) polarized
point-source calibrator (which is also the time-dependent gain
calibrator) that has been observed over a range of parallactic angle
(a single scan is not sufficient).  We assume that we have calibrated
the gain, bandpass, and cross-hand delay as described above, and that
the gain calibration ({\tt polcal.gcal}) was obtained assuming the
calibrator was unpolarized.

First, we import some utility functions from the CASA recipes
area:

\small
\begin{verbatim}
from recipes.almapolhelpers import *
\end{verbatim}

\normalsize 

Since the gain calibrator was assumed unpolarized, the time-dependent
gain solutions contain information about the source polarization.
This can be seen by plotting the amp vs. time for this table using
{\tt poln='/'}.  The antenna-based polarization amplitude ratios will
reveal the sinusoidal (in parallactic angle) of the source
polarization.  Run a utility method ({\tt qufromgain()}) to extract
the apparent source polarization estimates for each spw:

\small
\begin{verbatim}
qu=qufromgain('polcal.gcal')
\end{verbatim}

\normalsize

The source polarization reported for all spws should be reasonably
consistent.  This estimate is not as good as can be obtained from
the cross-hands (see below) since it relies on the gain amplitude 
polarization ratio being stable which may not be precisely true.
However, this estimate will be useful in resolving an ambiguity
that occurs in the cross-hand estimates.

Next we estimate both the XY-phase offset and source polarization from
the cross-hands.  The XY-phase offset is a spectral phase-only
bandpass relating the X and Y systems of the reference antenna.  The
cross-hand delay solved for above represents a systematic component
(linear phase in frequency).  If the XY-phase is solved for in a
channel-dependent manner (as below), it is strictly not necessary to
have solved for the cross-hand delay above, but it does not hurt (at
it allows reasonably coherent channel averages for data examination).
The source polarization occurs in the cross-hands as a sinusoidal
function of parallactic angle that is common to both cross-hands on
all baselines (for a point-source).  If the XY-phase bandpass is
uniformly zero, then the source linear polarization function will
occur entirely in the real part of the cross-hand visibilities.
Non-zero XY-phase has the effect of rotating the source linear
polarization signature partially into the imaginary part, where
circular (and instrumental) polarization occur (cf. the circular feed
basis where the cross-hand phase merely rotates the position angle of
linear polarization).  The following solve averages all baselines
together and first solves for a channelized XY-phase (the slope of the
source polarization function in the complex plane), then corrects the
slope and solves for a channel-averaged source polarization.  This
calibration is obtained using {\tt gaintype='XYf+QU'} in {\tt
gaincal}:

\small
\begin{verbatim}
   default('gaincal')
   vis                 = 'polcal_linfeed.ms'
   caltable            = 'polcal.xy0amb'   # possibly with 180deg ambiguity
   field               = '1'               # the calibrator 
   solint              =      'inf'    
   combine             =     'scan' 
   preavg              =      200.0        # minimal parang change
   smodel              = [1,0,1,0]         # non-zero U assumed
   gaintype             =     'XYf+QU'        
   gaintable           = ['polcal.gcal','polcal.bcal','polcal.xdelcal]
   gaincal()
\end{verbatim}
\normalsize 

Note that we imply non-zero Stokes U in {\tt smodel}; this is to
enforce the assumption of non-zero source polarization signature 
in the cross-hands in the ratio of data and model.  This solve
will report the center-channel XY-phase and apparent Q,U for
each spw.  The Q,U results should be recognizable in comparison
to that reported by {\tt qufromgain()} above.  However, since the 
XY-phase has a 180 degree ambiguity (you can rotate the source
polarization signature to lie entirely in the visibility
real part by rotating clockwise or counter-clockwise), some
or all spw QU estimates may have the wrong sign.  We correct
this using the {\tt xyamb()} utility method, using the {\tt qu}
obtained from {\tt qufromgain()} above (which is not
ambiguous):

\small
\begin{verbatim}
S=xyamb(xy='polcal.xy0amb',qu=qu,xyout='polcal.xy0')
\end{verbatim}
\normalsize 

The python variable {\tt S} now contains the mean
source model (Stokes I $=$ 1; fractional Q,U; V=0)
that can be used in a revision of the gain calibration
and instrumental polarization calibration.

Next we revise the gain calibration using the full polarization
source model:

\small
\begin{verbatim}
   default('gaincal')
   vis                 = 'polcal_linfeed.ms'
   caltable            = 'polcal.gcal1'   
   field               = '1'         
   solint              = 'int'        # or whatever was used previously
   smodel              = S            # obtained from xyamb
   gaintype            =     'G'        
   gaintable           = ['polcal.bcal']
   parang              = T            # so source poln properly rotated
   gaincal()
\end{verbatim}
\normalsize 

Note that {\tt parang=T} so that the supplied source linear
polarization is properly rotated in the parallel-hand visibility
model.  This new gain solution can be plotted with {\tt poln='/'} as
above to show that the source polarization is no longer distorting it.
Also, if {\tt qufromgain} is run on this new gain table, the reported
source polarization should be statistically indistinguishable from
zero.

Finally, we can now solve for the instrumental polarization:

\small
\begin{verbatim}
   default('polcal')
   vis                 = 'polcal_linfeed.ms'
   caltable            = 'polcal.dcal'
   field               = '1'
   solint              = 'inf'
   combine             = 'scan'
   preavg              = 200
   poltype             = 'Dflls'      # freq-dep LLS solver
   refant              = ''           # no reference antenna
   smodel              = S
   gaintable           = ['polcal.gcal1','polcal.bcal','polcal.xdelcal','polcal.xy0']
   polcal()
\end{verbatim}
\normalsize


Note that no reference antenna is used since this solve will produce
an absolute instrumental polarization solution that is registered
to the assumed source polarization ({\tt S}) and prior calibrations.
Applying a refant (referring all instrumental polarization terms
to a reference antenna's X feed, which would then be assumed perfect)
would, in fact, discard valid information about the imperfections
in the reference antenna's X feed.  (Had we used an unpolarized
calibrator, we would not have a valid xy-phase solution, nor would
we have had access to the absolute instrumental polarization solution
demonstrated here.)

A few points:

\begin{itemize}

\item Since the gain, bandpass, and XY-phase calibrations were obtained
prior to the instrumental polarization solution and maybe distorted by
it, it is generally desirable to resolve for them using the
instrumental polarization solution.  In effect, this means iterating
the sequence of calibration steps using all of the best of the
available information at each stage, including the source
polarization (and {\tt parang=T}).  This is a generalization
of traditional self-calibration.  For the CASA v4.1 release,
we expect to provide utility methods for iteration.

\item If the source linear polarization fraction and position angle is
known {\em a priori}, the processing steps outlined above can be
amended to use that source polarization assertion in the gain and
instrumental calibration solves.  The {\tt qufromgain()} method is not
needed (but can be used to verify assumptions), the {\tt
gaincal(...,gaintype='XYf+QU',...)} should not be altered (parallactic
angle coverage is still required!), and the {\tt
xyamb()} run should use the {\em a priori} polarization for {\tt qu}.
If there is likely to be a large systematic offset in the mean
feed position angle, iteration of the gain, bandpass, and instrumental
polarization terms is required to properly isolate the calibration
effects.

\item Note that the above process does not explicitly include a position
angle calibration.  In effect, the estimated source polarization sets
the mean feed position angle as the reference position angle, and this
is usually within a degree or so of optimal.  If your mean X feed
position angle is not $\sim$ 0 degrees, and your MS does not account
for the offset in its FEED subtable, be careful in your interpretation
of the final position angle. Currently, the circular feed-specific
position angle calibration modes of {\tt polcal} ({\tt poltype='X' or
'Xf'}) will not properly handle the linear feed basis; this will
be fixed in the CASA v4.1 release.

\end{itemize}


A full processing example for linear feed basis polarimetry is under
development and will be distributed with an upcoming CASA release.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Baseline-based Calibration ({\tt blcal})}
\label{section:cal.solve.blcal}

You can use the {\tt blcal} task to solve for baseline-dependent
(non-closing) errors.  {\bf WARNING:} this is in general a very dangerous
thing to do, since baseline-dependent errors once introduced are
difficult to remove.  You must be sure you have an excellent model
for the source (better than the magnitude of the baseline-dependent
errors).

The inputs are (note that {\tt blcal} does not yet use the 
{\tt docallib } parameter:
\small
\begin{verbatim}
#  blcal :: Calculate a baseline-based calibration solution (gain or bandpass)
vis                 =         ''        #  Name of input visibility file
caltable            =         ''        #  Name of output gain calibration table
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channels
intent              =         ''        #  Select observing intent
selectdata          =      False        #  Other data selection parameters
solint              =      'inf'        #  Solution interval
combine             =     'scan'        #  Data axes which to combine for solve (scan, spw,
                                        #   and/or field)
freqdep             =      False        #  Solve for frequency dependent solutions
calmode             =       'ap'        #  Type of solution" ('ap', 'p', 'a')
solnorm             =      False        #  Normalize average solution amplitudes to 1.0
gaintable           =       ['']        #  Gain calibration table(s) to apply on the fly
gainfield           =       ['']        #  Select a subset of
                                        #   calibrators from gaintable(s)
interp              =       ['']        #  Interpolation mode (in
                                        #   time) to use for each gaintable
spwmap              =         []        #  Spectral windows combinations to form for
                                        #   gaintable(s)
gaincurve           =      False        #  Apply internal VLA antenna
                                        #   gain curve correction
opacity             =         []        #  Opacity correction to apply (nepers), per spw
parang              =      False        #  Apply parallactic angle correction
async               =      False        #  If true the taskname must
                                        #   be started using blcal(...)

\end{verbatim}
\normalsize

The {\tt freqdep} parameter controls whether {\tt blcal} solves for 
``gain'' ({\tt freqdep=False}) or ``bandpass'' ({\tt freqdep=True})
style non-closing calibration.

Other parameters are the same as in other calibration tasks.
These common calibration parameters are described in
\S~\ref{section:cal.solve.pars}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Plotting and Manipulating Calibration Tables}
\label{section:cal.tables}

At some point, the user should examine (plotting or listing) the
calibration solutions.
Calibration tables can also be manipulated in various ways, such as
by interpolating between times (and sources), smoothing of solutions,
and accumulating various separate calibrations into a single 
table.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Plotting Calibration Solutions ({\tt plotcal})}
\label{section:cal.tables.plotcal}

The {\tt plotcal} task is available for examining solutions of all of
the basic solvable types (G, T, B, D, M, MF, K).  The inputs are:
\small
\begin{verbatim}
#  plotcal :: An all-purpose plotter for calibration results:

caltable     =         ''   #  Name of input calibration table
xaxis        =         ''   #  Value to plot along x axis (time,chan,amp,phase,real,imag,snr)
yaxis        =         ''   #  Value to plot along y axis (amp,phase,real,imag,snr)
poln         =         ''   #  Polarization to plot (RL,R,L,XY,X,Y,/)
field        =         ''   #  Field names or index: ''=all, '3C286,P1321*', '0~3'
antenna      =         ''   #  Antenna selection.  E.g., antenna='3~5'
spw          =         ''   #  Spectral window: ''=all, '0,1' means spw 0 and 1
timerange    =         ''   #  Time selection ''=all
subplot      =        111   #  Panel number on display screen (yxn)
overplot     =      False   #  Overplot solutions on existing display
clearpanel   =     'Auto'   #  Specify if old plots are cleared or not
iteration    =         ''   #  Iterate on antenna,time,spw,field
plotrange    =         []   #  plot axes ranges: [xmin,xmax,ymin,ymax]
showflags    =      False   #  If true, show flags
plotsymbol   =        '.'   #  pylab plot symbol
plotcolor    =     'blue'   #  initial plotting color
markersize   =        5.0   #  size of plot symbols
fontsize     =       10.0   #  size of label font
showgui      =       True   #  Show plot on gui
figfile      =         ''   #  ''= no plot hardcopy, otherwise supply name
\end{verbatim}
\normalsize

{\bf ALERT:} Currently, {\tt plotcal} needs to know the MS from
which {\tt caltable} was derived to get indexing information.  It does
this using the name stored inside the table, which does not include
the full path, but assumes the MS is in the {\tt cwd}.  Thus if you
are using a MS in a directory other than the current one, it will not
find it.  You need to change directories using {\tt cd} in
IPython (or {\tt os.chdir()} inside a script) to the MS location.

The controls for the {\tt plotcal} window are the same as for
{\tt plotxy} (see \S~\ref{section:edit.plot.plotxy.control}).

The {\tt xaxis} and {\tt yaxis} plot options available are:
\begin{itemize}
   \item {\tt 'amp'} --- amplitude,
   \item {\tt 'phase'} --- phase,
   \item {\tt 'real'} -- the real part,
   \item {\tt 'imag'} --- the imaginary part,
   \item {\tt 'snr'} -- the signal-to-noise ratio,
%   \item {\tt 'delay'} -- the phase delay,
%   \item {\tt 'delayrate'} --- the phase delay rate,
\end{itemize}
of the calibration solutions that are in the {\tt caltable}.
The {\tt xaxis} choices also include {\tt 'time'} and {\tt 'channel'}
which will be used as the sensible defaults (if {\tt xaxis=''}) for
gain and bandpass solutions respectively.

The {\tt poln} parameter determines what polarization or combination of
polarization is being plotted.  The {\tt poln='RL'} plots both
R and L polarizations on the same plot.  The respective XY options do
equivalent things.  The {\tt poln='/'} option
plots amplitude ratios or phase differences between whatever
polarizations are in the MS (R and L. or X and Y).  

The {\tt field}, {\tt spw}, and {\tt antenna} selection parameters are
available to obtain plots of subsets of solutions.  The syntax for 
selection is given in \S~\ref{section:io.selection}.

The {\tt subplot} parameter is particularly helpful in making 
multi-panel plots.  The format is  
{\tt subplot=yxn} where {\tt yxn} is an integer with digit
{\tt y} representing the number of plots in the y-axis, digit
{\tt x} the number of panels along the x-axis, and digit {\tt n}
giving the location of the plot in the panel array (where
{\tt n = 1, ..., xy}, in order upper left to right, then down).
See \S~\ref{section:edit.plot.plotxy.subplot} for more details on this
option.

The {\tt iteration} parameter allows you to select an identifier to
iterate over when producing multi-panel plots.  The choices
for {\tt iteration} are: {\tt 'antenna'}, {\tt 'time'}, 
{\tt 'spw'}, {\tt 'field'}.  For example, if per-antenna solution 
plots are desired, use {\tt iteration='antenna'}.  You can then use
{\tt  subplot} to specify the number of plots to appear on each page.
In this case, set the {\tt n} to {\tt 1} for {\tt subplot=yxn}.  
Use the {\bf Next} button on the plotcal window to advance to the next
set of plots.  Note that if there is more than one timestamp in a {\tt
'B'} table, the user will be queried to interactively advance the plot
to each timestamp, or if {\tt multiplot=True}, the antennas plots will
be cycled through for each timestamp in turn.  Note that 
{\tt iteration} can take more than one iteration choice (as a single
string containing a comma-separated list of the options).
{\bf ALERT:} the iteration order is fixed (independent of the
order specified in the {\tt iteration} string), for example:
\small
\begin{verbatim}
   iteration = 'antenna, time, field'
   iteration = 'time, antenna, field'
\end{verbatim}
\normalsize
will both iterate over each field (fastest) then time (next) and antenna
(slowest).  The order is:
\small
\begin{verbatim}
   iteration = 'antenna, time, field, spw'
\end{verbatim}
\normalsize
from the slowest (outer loop) to fastest (inner loop).

The {\tt markersize} and {\tt fontsize} parameters are especially
helpful in making the dot and label sizes appropriate for the
plot being made.  The screen shots in this section used this feature
to make the plots more readable in the cookbook.  Adjusting the
{\tt fontsize} can be tricky on multi-panel plots, as the labels
can run together if too large.  You can also help yourself by manually
resizing the Plotter window to get better aspect ratios on the plots.

{\bf ALERT:} Unfortunately, {\tt plotcal} has many of the same
problems that {\tt plotxy} does, as they use similar code underneath.
An overhaul is underway, so stay tuned.

%%%%%%
\subsubsection{Examples for {\tt plotcal}}
\label{section:cal.tables.plotcal.examples}

For example, to plot amplitude or phase as a function of time for 
{\tt 'G'} solutions (after rescaling by {\tt fluxscale} can look like 
\small
\begin{verbatim}
default('plotcal')
fontsize = 14.0     # Make labels larger
markersize = 10.0   # Make dots bigger

caltable = 'ngc5921.usecase.fluxscale'
yaxis = 'amp'
subplot = 211
plotcal()

yaxis = 'phase'
subplot = 212
plotcal()
\end{verbatim}
\normalsize
The results are shown in Figure~\ref{fig:plotcal_G_5921}.  This makes 
use of the {\tt subplot} option to make multi-panel displays.

\begin{figure}[h!]
\begin{center}
\pngname{plotcal_n5921_G_2panel}{6}
\caption{\label{fig:plotcal_G_5921} Display of the amplitude (upper)
and phase (lower) gain solutions for all antennas and polarizations 
in the {\tt ngc5921} post-{\tt fluxscale} table.} 
\hrulefill
\end{center}
\end{figure}

% \begin{figure}[h!]
% \gname{plotcal_G}{3.5}
% \gname{plotcal_Gp}{3.5}
% \caption{\label{fig:plotcal_Gall} plotcal: Display of the amplitude and
%   phase gain solutions (for all data).} 
% \hrulefill
% \end{figure}

Similarly, to plot amplitude or phase as a function of channel for
{\tt 'B'} solutions for {\tt NGC5921}:
\small
\begin{verbatim}
default('plotcal')
fontsize = 14.0     # Make labels larger
markersize = 10.0   # Make dots bigger

caltable = 'ngc5921.usecase.bcal'
antenna = '1'
yaxis = 'amp'
subplot = 311
plotcal()

yaxis = 'phase'
subplot = 312
plotcal()

yaxis = 'snr'
subplot = 313
plotcal()
\end{verbatim}
\normalsize
The results are shown in Figure~\ref{fig:plotcal_B_5921}.  This stacks
three panels with amplitude, phase, and signal-to-noise ratio.  We
have picked {\tt antenna='1'} to show.

\begin{figure}[h!]
\begin{center}
\pngname{plotcal_n5921_B_3panel}{6}
\caption{\label{fig:plotcal_B_5921} Display of the amplitude (upper),
phase (middle), and signal-to-noise ratio (lower) of the
{\tt bandpass} {\tt 'B'} solutions for {\tt antenna='0'} and both
polarizations for {\tt ngc5921}.  Note the falloff of the SNR at
the band edges in the lower panel.} 
\hrulefill
\end{center}
\end{figure}

% \begin{figure}[h!]
% \gname{plotcal_Ba}{3.5}
% \gname{plotcal_Bp}{3.5}
% \caption{\label{fig:plotcal_B} plotcal: Display of the amplitude and
%   phase bandpass solutions (for all data).} 
% \hrulefill
% \end{figure}

For example, to show 6 plots per page of {\tt 'B'} amplitudes on a 
$3 \times 2$ grid:
\small
\begin{verbatim}
   default('plotcal')
   fontsize = 12.0     # Make labels just large enough
   markersize = 10.0   # Make dots bigger

   caltable = 'ngc5921.usecase.bcal'
   yaxis = 'amp'
   subplot = 231
   iteration = 'antenna'

   plotcal()
\end{verbatim}
\normalsize
See Figure~\ref{fig:plotcal_B_5921_3x2} for this example.  This uses
the {\tt iteration} parameter.

\begin{figure}[h]
\begin{center}
\pngname{plotcal_n5921_B_6panel}{6}
\caption{\label{fig:plotcal_B_5921_3x2} Display of the amplitude
of the {\tt bandpass} {\tt 'B'} solutions.  Iteration over antennas
was turned on using {\tt iteration='antenna'}. The first page is shown.
The user would use the {\bf Next} button to advance to the next
set of antennas.} 
\hrulefill
\end{center}
\end{figure}

% \begin{figure}[h!]
% \gname{plotcal_Bmulti}{5}
% \caption{\label{fig:plotcal_Bmulti} plotcal: Display of a 3x2 grid of
%   bandpass solutions, iterating over antenna identifier index.} 
% \hrulefill
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Listing calibration solutions with ({\tt listcal})}
\label{section:cal.tables.listcal}

The {\tt listcal} task will list the solutions in a specified 
calibration table.

The inputs are:
\small
\begin{verbatim}
#  listcal :: List data set summary in the logger:

vis        =         ''   #  Name of input visibility file (MS)
caltable   =         ''   #  Input calibration table to list
field      =         ''   #  Select data based on field name or index
antenna    =         ''   #  Select data based on antenna name or index
spw        =         ''   #  Spectral window, channel to list
listfile   =         ''   #  Disk file to write, else to terminal
pagerows   =         50   #  Rows listed per page
async      =      False   
\end{verbatim}
\normalsize

An example listing is:
\small
\begin{verbatim}
Listing CalTable: jupiter6cm.usecase.split.ms.smoothcal2   (G Jones) 
---------------------------------------------------------------

SpwId = 0,  channel = 0.
Time                  Field      Ant       :   Amp    Phase      Amp    Phase    
--------------------- ---------- --------    ---------------   ---------------
1999/04/16/14:10:43.5 'JUPITER'  '1'       :  1.016   -11.5     1.016    -9.2    
                                 '2'       :  1.013    -5.3     0.993    -3.1    
                                 '3'       :  0.993    -0.8     0.990    -5.1    
                                 '4'       :  0.997   -10.7     0.999    -8.3    
                                 '5'       :  0.985    -2.7     0.988    -4.0    
                                 '6'       :  1.005    -8.4     1.009    -5.3    
                                 '7'       :  0.894    -8.7     0.897    -6.8    
                                 '8'       :  1.001    -0.1     0.992    -0.7    
                                 '9'       :  0.989   -12.4     0.992   -13.5    
                                 '10'      :  1.000F   -4.2F    1.000F   -3.2F   
                                 '11'      :  0.896    -0.0     0.890    -0.0    
                                 '12'      :  0.996   -10.6     0.996    -4.2    
                                 '13'      :  1.009    -8.4     1.011    -6.1    
                                 '14'      :  0.993   -17.6     0.994   -16.1    
                                 '15'      :  1.002    -0.8     1.002    -1.1    
                                 '16'      :  1.010    -9.9     1.012    -8.6    
                                 '17'      :  1.014    -8.0     1.017    -7.1    
                                 '18'      :  0.998    -3.0     1.005    -1.0    
                                 '19'      :  0.997   -39.1     0.994   -38.9    
                                 '20'      :  0.984    -5.7     0.986     3.0    
                                 '21'      :  1.000F   -4.2F    1.000F   -3.2F   
                                 '22'      :  1.003   -11.8     1.004   -10.4    
                                 '23'      :  1.007   -13.8     1.009   -11.7    
                                 '24'      :  1.000F   -4.2F    1.000F   -3.2F   
                                 '25'      :  1.000F   -4.2F    1.000F   -3.2F   
                                 '26'      :  0.992     3.7     1.000    -0.2    
                                 '27'      :  0.994    -5.6     0.991    -4.3    
                                 '28'      :  0.993   -10.7     0.997    -3.8    

\end{verbatim}
\normalsize

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calibration table statistics ({\rm calstat})}
\label{section:cal.tables.calstat}

The {\tt calstat} task will print the statistics of solutions in a specified 
calibration table.

The inputs are:
\small
\begin{verbatim}
#  calstat :: Displays statistical information on a calibration table
caltable            =         ''        #  Name of input calibration table
axis                =      'amp'        #  Which values to use
     datacolumn     =     'gain'        #  Which data column to use

useflags            =       True        #  Take flagging into account? 
                                        #   (not implemented)
async               =      False        #  If true the taskname must
                                        #   be started using calstat(...)
\end{verbatim}
\normalsize

For example:
\small
\begin{verbatim}
CASA <3>: calstat('ngc5921.demo.gcal',axis='amp',datacolumn='gain')
  Out[3]: 
{'GAIN': {'max': 1.6031942367553711,
          'mean': 1.4448433067117419,
          'medabsdevmed': 0.0086394548416137695,
          'median': 1.5732669830322266,
          'min': 0.99916577339172363,
          'npts': 280.0,
          'quartile': 0.020265340805053711,
          'rms': 1.4650156497955322,
          'stddev': 0.24271160321065546,
          'sum': 404.55612587928772,
          'sumsq': 600.95579999685287,
          'var': 0.058908922333086665}}

CASA <4>: calstat('ngc5921.demo.gcal',axis='phase',datacolumn='gain')
  Out[4]: 
{'GAIN': {'max': 0.091214209794998169,
          'mean': -0.015221830284565011,
          'medabsdevmed': 0.012778861448168755,
          'median': -0.012778861448168755,
          'min': -0.15903720259666443,
          'npts': 280.0,
          'quartile': 0.02537553571164608,
          'rms': 0.031241731718182564,
          'stddev': 0.027331476552707856,
          'sum': -4.2621124796782031,
          'sumsq': 0.27329283416317834,
          'var': 0.00074700961055121926}}
\end{verbatim}
\normalsize
The statistics can be captured as return variables from the task:
\small
\begin{verbatim}
CASA <7>: mystat = calstat('ngc5921.demo.gcal',axis='amp',datacolumn='gain')

CASA <8>: print 'Gain Amp = ',mystat['GAIN']['mean'],'+/-',mystat['GAIN']['stddev']
Gain Amp =  1.44484330671 +/- 0.242711603211
\end{verbatim}
\normalsize

{\bf ALERT:} This task is still under development and currently offers no
selection (e.g.\ by antenna) for the statistics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calibration Smoothing ({\rm smoothcal})}
\label{section:cal.tables.smooth}

The {\tt smoothcal} task will smooth calibration solutions 
(most usefully $G$ or $T$) over a longer time interval to reduce noise
and outliers.  The inputs are:
\small
\begin{verbatim}
#  smoothcal :: Smooth calibration solution(s) derived from one or more sources:

vis          =         ''   #  Name of input visibility file
tablein      =         ''   #  Input calibration table
caltable     =         ''   #  Output calibration table
field        =         ''   #  Field name list
smoothtype   =   'median'   #  Smoothing filter to use
smoothtime   =       60.0   #  Smoothing time (sec)
async        =      False   #  if True run in the background, prompt is freed
\end{verbatim}
\normalsize

Note that if no {\tt caltable} is specified as output, {\tt smoothcal}
will overwrite the input {\tt tablein} calibration table. 

The smoothing will use the {\tt smoothtime} and {\tt smoothtype}
parameters to determine the new data points which will replace the
previous points on the same time sampling grid as for the {\tt
tablein} solutions.  The currently supported {\tt smoothtype} 
options: 
\begin{itemize}
\item {\tt 'mean'} --- use the mean of the points within the window
defined by {\tt smoothtime} (a ``boxcar'' average),

\item {\tt 'median'} --- use the median of the points within the window
defined by {\tt smoothtime} (most useful when many points lie in the
interval).
\end{itemize}
Note that {\tt smoothtime} defines the width of the time window that
is used for the smoothing.

{\tt ALERT:} Note that {\tt smoothcal} currently smooths by
{\tt field} and {\tt spw}, and thus you cannot smooth solutions
from different sources or bands together into one solution.

\begin{figure}[h!]
\begin{center}
\pngname{smoothcal_n4826}{6}
\caption{\label{fig:smoothcal_4826} The {\tt 'amp'} of gain solutions
for {\tt NGC4826} before (top) and after (bottom) smoothing with
a 7200 sec {\tt smoothtime} and {\tt smoothtype='mean'}.  Note that
the first solution is in a different {\tt spw} and on a different
source, and is not smoothed together with the subsequent solutions.}
\hrulefill
\end{center}
\end{figure}

An example using the {\tt smoothcal} task to smooth an existing table:
\small
\begin{verbatim}
smoothcal('n4826_16apr.ms',
       tablein='n4826_16apr.gcal',
       caltable='n4826_16apr.smoothcal',
       smoothtime=7200.,
       smoothtype='mean')

# Plot up before and after tables
plotcal('n4826_16apr.gcal','','amp',antenna='1',subplot=211)
plotcal('n4826_16apr.smoothcal','','amp',antenna='1',subplot=212)
\end{verbatim}
\normalsize
This example uses 2 hours (7200 sec) for the smoothing time and
{\tt smoothtype='mean'}.  The {\tt plotcal} results are shown
in Figure~\ref{fig:smoothcal_4826}.

% \begin{figure}[h!]
% \gname{plotcal_05s}{3.5}
% \gname{plotcal_smoothed}{3.5}
% \caption{\label{fig:plotcal_smooth} Display of the amplitude
%   solutions for short solution interval table (0.5 seconds: top) and
%   the smoothed table using a smoothtime of 1000 seconds. }
% \hrulefill
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calibration Interpolation and Accumulation ({\tt accum})}
\label{section:cal.tables.accum}

{\bf ALERT:} The {\tt accum} task is generally no longer recommended
for most calibration scenarios.  Please write to the NRAO CASA
helpdesk  if you need support using {\tt accum}.

The {\tt accum} task is used to interpolate calibration solutions onto
a different time grid, and to {\it accumulate} incremental
calibrations into a {\it cumulative} calibration table.  The manual
accumulation of calibration is rarely required and can usually be
achieved implicitly simply by running {\tt applycal} with all the
calibration tables given as a list in the {\tt gaintable} parameter
(and using {\tt gainfield}, {\tt spwmap}, and {\tt interp}
appropriately.  However, sometimes it is desirable to see the
interpolated calibration prior to application, and this section
describes how this can be done.

Its inputs are:
\small
\begin{verbatim}
#  accum :: Accumulate incremental calibration solutions

vis             =         ''   #  Name of input visibility file
tablein         =         ''   #  Input (cumulative) calibration table; use '' on first run
     accumtime  =        1.0   #  Timescale on which to create cumulative table

incrtable       =         ''   #  Input incremental calibration table to add
caltable        =         ''   #  Output (cumulative) calibration table
field           =         ''   #  List of field names to process from tablein.
calfield        =         ''   #  List of field names to use from incrtable.
interp          =   'linear'   #  Interpolation mode to use for resampling incrtable solutions
spwmap          =       [-1]   #  Spectral window combinations to apply
\end{verbatim}
\normalsize
The {\it mapping} implied here is 
\small
\begin{verbatim}
   tablein + incrtable => caltable
\end{verbatim}
\normalsize
(mathematically the cal solutions are multiplied as complex numbers
as per the Measurement Equation).
The {\tt tablein} is optional (see below).
You must specify an {\tt incrtable} and a {\tt caltable}.

The {\tt tablein} parameter is used to specify the existing cumulative
calibration table to which an incremental table is to be applied.
Initially, no such table exists, and if {\tt tablein=''} then
accumulate will generate one from
scratch (on-the-fly), using the timescale (in seconds) specified by
the sub-parameter {\tt accumtime}. These nominal solutions will be
unit-amplitude, zero-phase calibration, ready to
be adjusted by accumulation according to the settings of other
parameters.  When {\tt accumtime} is negative (the default), the table
name specified in {\tt tablein} must exist and will be used.  If 
{\tt tablein} is specified, then the entries in that
table will be used.

The {\tt incrtable} parameter is used to specify the incremental table
that should be applied to {\tt tablein}. The calibration type of {\tt
incrtable} sets the type assumed in the operation, so {\tt tablein}
(if specified) must be of the same type. If it is not, {\tt accum}
will exit with an error message. (Certain combinations of types and
subtypes will be supported by {\tt accum} in the future.)

The {\tt caltable} parameter is used to specify the name of the output
table to write. If un-specified ({\tt ''}), then {\tt tablein} will be
overwritten. Use this feature with care, since an error here will
require building up the cumulative table from the most recent distinct
version (if any).

The {\tt field} parameter specifies those field names in {\tt tablein} to
which the incremental solution should be applied. The solutions for
other fields will be passed to {\tt caltable} unaltered. If the cumulative
table was created from scratch in this run of accumulate, then the
solutions for these other fields will be unit-amplitude, zero-phase,
as described above.

The {\tt calfield} parameter is used to specify the fields to select
from {\tt incrtable} to use when applying to {\tt tablein}. Together,
use of {\tt field} and {\tt calfield} permit completely flexible combinations
of calibration accumulation with respect to fields. Multiple runs of
{\tt accum} can be used to generate a single table with many combinations.
In future, a {\tt 'self'} mode will be enabled that will simplify the
accumulation of field-specific solutions.

The {\tt spwmap} parameter gives the mapping of the spectral windows
in the {\tt incrtable} onto those in {\tt tablein} and {\tt caltable}.
The syntax is described in \S~\ref{section:cal.solve.pars.previous}.

The {\tt interp} parameter controls the method used for interpolation.
The options are (currently): {\tt 'nearest'} and {\tt 'linear'} for
time-dependent interpolation, and {\tt 'nearest'}, {\tt 'linear'}, {\tt
  cubic}, and {\tt spline} for (optional) frequency-dependent interpolation.
These are described in \S~\ref{section:cal.solve.pars.previous}.
For most purposes, the {\tt 'linear'} option should suffice.

We now describe the two uses of {\tt accum}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Interpolation using ({\tt accum})}
\label{section:cal.tables.accum.interp}

{\bf ALERT:} The {\tt accum} task is generally no longer recommended
for most calibration scenarios.  Please write to the NRAO CASA
helpdesk  if you need support using {\tt accum}.

Calibration solutions (most notably $G$ or $T$) can be interpolated
onto the timestamps of the science target observations using {\tt accum}.  

The following example uses {\tt accum} to interpolate an existing
table onto a new time grid:
\small
\begin{verbatim}
accum(vis='n4826_16apr.ms',
      tablein='',
      accumtime=20.0,
      incrtable='n4826_16apr.gcal',
      caltable='n4826_16apr.20s.gcal',
      interp='linear',
      spwmap=[0,1,1,1,1,1])

plotcal('n4826_16apr.gcal','','phase',antenna='1',subplot=211)
plotcal('n4826_16apr.20s.gcal','','phase',antenna='1',subplot=212)
\end{verbatim}
\normalsize
See Figure~\ref{fig:accum_interp} for the {\tt plotcal} results.
The data used in this example is BIMA data (single polarization 
{\tt  YY}) where the calibrators were observed in single continuum
spectral windows ({\tt spw='0,1'}) and the target NGC4826 was observed
in 64-channel line windows ({\tt spw='2,3,4,5'}).  Thus, it is 
necessary to use {\tt spwmap=[0,1,1,1,1,1]} to map the bandpass
calibrator in {\tt spw='0'} onto itself, and the phase calibrator 
in {\tt spw='1'} onto the target source in {\tt spw='2,3,4,5'}.

\begin{figure}[h!]
\begin{center}
\pngname{accum_n4826_interp}{6}
\caption{\label{fig:accum_interp} The {\tt 'phase'} of gain solutions
for NGC4826 before (top) and after (bottom) {\tt 'linear'} interpolation onto
a 20 sec {\tt accumtime} grid.  The first scan was 3C273 in {\tt spw='0'} 
while the calibrator scans on 1331+305 were in {\tt spw='1'}.  The use of 
{\tt spwmap} was necessary to transfer the interpolation correctly
onto the NGC4826 scans.
}
\hrulefill
\end{center}
\end{figure}


% \begin{figure}[h!]
% \gname{plotcal_G}{3.5}
% \gname{plotcal_interp}{3.5}
% \caption{\label{fig:plotcal_G} plotcal: Display of the amplitude
%   solutions for NGC 5921; original (left), interpolated solutions-20s
%   sampling (right).} 
% \hrulefill
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Incremental Calibration using ({\tt accum})}
\label{section:cal.tables.accum.incr}

It is occasionally desirable to solve for and apply calibration
incrementally.  This is the case when a calibration table of a certain
type already exists (from a previous solve), a solution {\it of the
same type} and incremental {\it relative to the first} is required,
and it is not possible or convenient to recover the cumulative
solution by a single solve.

Much of the time, it is, in fact, possible to recover the cumulative
solution. This is because the equation describing the solution for the
incremental solution (using the original solution), and that describing
the solution for their product are fundamentally the same equation---the
cumulative solution, if unique, must always be the same no matter what
initial solution is.  One circumstance where an incremental solution is
necessary is the case of {\it phase-only} self-calibration relative to a
full amplitude and phase calibration already obtained (from a different
field).

For example, a phase-only {\tt 'G'} self-calibration on a target source may be
desired to tweak the full amplitude and phase {\tt 'G'} calibration already
obtained from a calibrator. The initial calibration (from the calibrator)
contains amplitude information, and so must be carried forward, yet the
phase-only solution itself cannot (by definition) recover this
information, as a full amplitude and phase self-calibration would. In this
case, the initial solution must be applied while solving for the
phase-only solution, then the two solutions combined to form a cumulative
calibration embodying the net effect of both. In terms of the Measurement
Equation, the net calibration is the product of the initial and
incremental solutions.

Cumulative calibration tables also provide a means of generating
carefully interpolated calibration, on variable user-defined
timescales, that can be examined prior to application to the data with
{\tt applycal}. The solutions for different fields and/or spectral
windows can be interpolated in different ways, with all solutions
stored in the same table.

\begin{wrapfigure}{r}{2.5in}
  \begin{boxedminipage}{2.5in}
     \centerline{\bf Other Packages:}
     The analog of {\tt accum} in classic AIPS is the use of {\tt
     CLCAL} to combine a series of (incremental) {\tt SN} calibration
     tables to form successive (cumulative) {\tt CL} calibration
     tables. AIPS {\tt SN/CL} tables are the analog of {\tt 'G'} 
     tables in CASA.
  \end{boxedminipage}
\end{wrapfigure}

The only difference between incremental and cumulative calibration
tables is that incremental tables are generated directly from the
calibration solving tasks ({\tt gaincal}, {\tt bandpass}, etc.), and
cumulative tables are generated from other cumulative and incremental
tables via {\tt accum}. In all other respects (internal format,
application to data with {\tt applycal}, plotting with {\tt plotcal},
etc.), they are the same, and therefore interchangeable. Thus,
accumulate and cumulative calibration tables need only be used when
circumstances require it.

The {\tt accum} task represents a generalization on the classic AIPS
{\tt CLCAL} (see sidebox) model of cumulative calibration in that its
application is not limited to accumulation of {\tt 'G'} solutions. 
In principle, any
basic calibration type can be accumulated (onto itself), as long as the
result of the accumulation (matrix product) is of the same type. This is
true of all the basic types, except {\tt 'D'}. Accumulation is currently
supported for {\tt 'B'}, {\tt 'G'}, and {\tt 'T'}, and, in future,
{\tt 'F'} (ionospheric Faraday rotation), delay-rate, and perhaps
others. Accumulation of certain specialized
types (e.g., {\tt 'GSPLINE'}, {\tt 'TOPAC'}, etc.) onto the basic types will be
supported in the near future. The treatment of various calibration from
ancillary data (e.g., system temperatures, weather data, WVR, etc.), as
they become available, will also make use of accumulate to achieve the net
calibration.

Note that accumulation only makes sense if treatment of a uniquely
incremental solution is required (as described above), or if a careful
interpolation or sampling of a solution is desired. In all other cases,
re-solving for the type in question will suffice to form the net
calibration of that type. For example, the product of an existing {\tt 'G'}
solution and an amplitude and phase {\tt 'G'} self-cal (solved with the
existing solution applied), is equivalent to full amplitude and phase
{\tt 'G'} self-cal (with no prior solution applied), as long as the timescale
of this solution is at least as short as that of the existing solution.

One obvious application is to calibrate the amplitudes and phases
on different timescales during self-calibration.
Here is an example:
\small
\begin{verbatim}
# Add clean model 
ft(vis='jupiter6cm.usecase.split.ms',
   model='jupiter6cm.usecase.clean1.model')

# Phase only self-cal on 10s timescales
gaincal(vis='jupiter6cm.usecase.split.ms',
        caltable='jupiter6cm.usecase.phasecal1',
        gaintype='G',
        calmode='p',
        refant='6',
        solint=10.0,
        minsnr=1.0)

# Plot up solution phase and SNR
plotcal('jupiter6cm.usecase.phasecal1','','phase',antenna='1',subplot=211)
plotcal('jupiter6cm.usecase.phasecal1','','snr',antenna='1',subplot=212)

# Amplitude and phase self-cal on scans
gaincal(vis='jupiter6cm.usecase.split.ms',
        caltable='jupiter6cm.usecase.scancal1',
        gaintable='jupiter6cm.usecase.phasecal1',
        gaintype='G',
        calmode='ap',
        refant='6',
        solint='inf',
        minsnr=1.0)

# Plot up solution amp and SNR
plotcal('jupiter6cm.usecase.scancal1','','amp',antenna='1',subplot=211)
plotcal('jupiter6cm.usecase.scancal1','','snr',antenna='1',subplot=212)

# Now accumulate these - they will be on the 10s grid
accum(vis='jupiter6cm.usecase.split.ms',
      tablein='jupiter6cm.usecase.phasecal1',
      incrtable='jupiter6cm.usecase.scancal1',
      caltable='jupiter6cm.usecase.selfcal1',
      interp='linear')

# Plot this up
plotcal('jupiter6cm.usecase.selfcal1','','amp',antenna='1',subplot=211)
plotcal('jupiter6cm.usecase.selfcal1','','phase',antenna='1',subplot=212)
\end{verbatim}
\normalsize
The final plot is shown in Figure~\ref{fig:accum_jupiter}

\begin{figure}[h!]
\begin{center}
\pngname{accum_jupiter}{6}
\caption{\label{fig:accum_jupiter} The final {\tt 'amp'} (top) and
{\tt 'phase'} (bottom) of the self-calibration gain solutions
for Jupiter.  An initial phase calibration on 10s {\tt solint} was
followed by an incremental gain solution on each scan.  These
were accumulated into the cumulative solution shown here.
}
\hrulefill
\end{center}
\end{figure}

{\bf ALERT:} Only interpolation is offered in {\tt accum},
no smoothing (as in {\tt smoothcal}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application of Calibration to the Data}
\label{section:cal.correct}

After the calibration solutions are computed and written to
one or more calibration tables, one then needs to apply them to the data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Application of Calibration ({\tt applycal})}
\label{section:cal.correct.apply}

After all relevant calibration types have been determined, they must
be applied to the target source(s) before splitting off to a new
MS or before imaging.  This is currently done by explicitly taking the
data in the {\tt DATA} column in the {\tt MAIN} table of the MS, 
applying the relevant calibration tables, and creating the 
{\tt CORRECTED\_DATA} scratch column.  The original {\tt DATA}
column is untouched.

The {\tt applycal} task does this.  The inputs are:
\small
\begin{verbatim}
#  applycal :: Apply calibrations solutions(s) to data
vis                 =         ''        #  Name of input visibility file
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channels
intent              =         ''        #  Select observing intent
selectdata          =       True        #  Other data selection parameters
     timerange      =         ''        #  Select data based on time range
     uvrange        =         ''        #  Select data within uvrange (default units meters)
     antenna        =         ''        #  Select data based on antenna/baseline
     scan           =         ''        #  Scan number range
     observation    =         ''        #  Select by observation ID(s)
     msselect       =         ''        #  Optional complex data selection (ignore for now)

docallib            =      False        #  Use callib or traditional cal apply parameters
     gaintable      =         []        #  Gain calibration table(s) to apply on the fly
     gainfield      =         []        #  Select a subset of calibrators from gaintable(s)
     interp         =         []        #  Interp type in time[,freq],
                                        #   per gaintable.  default=linear,linear
     spwmap         =         []        #  Spectral windows
                                        #   combinations to form for gaintable(s)
     calwt          =     [True]        #  Calibrate data weights per gaintable.

parang              =      False        #  Apply parallactic angle correction
applymode           =         ''        #  Calibration mode:
                                        #   ""="calflag","trial","flagonly", or "calonly"
flagbackup          =       True        #  Automatically back up the
                                        #   state of flags before the run?
async               =      False        #  If true the taskname must
                                        #   be started using applycal(...)
\end{verbatim}
\normalsize
As in other tasks, setting {\tt selectdata=True} will open up the
other selection sub-parameters (see
\S~\ref{section:io.selection}). In addition, you can also select data
based on the scan intents that were set during the observations (find
them through {\tt listobs}). 
Many of the other parameters are the common calibration parameters
that are described in \S~\ref{section:cal.solve.pars}.

The single non-standard parameter is the {\tt calwt} option to toggle
the ability to scale the visibility weights by the inverse of the 
products of the scale factors applied to the amplitude of the antenna
gains (for the pair of antennas of a given visibility).  
This should in {\em almost all cases} be set to its default ({\tt True}).
The weights should reflect the inverse noise variance of the
visibility, and errors in amplitude are usually also in the weights.


{\bf Alert:} {\it Current} (as of February 2014) Jansky VLA data has no
calibrated weights to the data (unless they are created from switched
power). To avoid trouble, {\tt calwt=False} should be set for those
data sets. Older, pre-Jansky VLA data should still be calibrated with
{\tt calwt=True}.

For {\tt applycal}, the list of final cumulative tables is given in
{\tt gaintable}.  In this case you will have run {\tt accum} if you
have done incremental calibration for any of the types, such as {\tt
  'G'}.  You can also feed {\tt gaintable} the full sets and rely on
use of {\tt gainfield}, {\tt interp} and {\tt spwmap} to do the
correct interpolation and transfer.  In particular, for frequency
interpolation, the interpolation methods ending in {\tt 'PD'}, {\tt
  nearestPD} and {\tt linearPD} also scale the phase by the frequency
ratio between the measured and interpolated values. It is often more
convenient to go through accumulation of each type with {\tt accum} as
described above (see \S~\ref{section:cal.tables.accum.incr}), as this
makes it easier to keep track of the sequence of incremental
calibration as it is solved and applied.  You can also do any required
smoothing of tables using {\tt smoothcal}
(\S~\ref{section:cal.tables.smooth}), as this is not yet available in
{\tt accum} or {\tt applycal}.


{\tt applycal} has different {\tt applymode}s: {\tt 'calflag'} will
apply all flags from a calibration table to the data and apply the
calibration itself to the remaining visibilities. {\tt 'trial'} will
only report on the calibration table flags but not manipulate the
data, {\tt 'flagonly'} applies the flags but not the calibration
itself, and {\tt 'calonly'} will apply the calibration and but not the
solution table flags. Data that would {\tt 'calflag'} would flag are
thus passed through uncalibrated. This option can be useful when
{\tt applycal} is executed in consecutive steps, one calibration table
at a time. Portions of the data that were not calibrated in the first
run can then be calibrated in a second run with a different
calibration table. This option should be used with care such that no
uncalibrated data remains in the final data product. 


{\tt applycal} will flag all data that have no calibration
solution. Flags will distribute into all of your scratch columns,
i.e. it will affect your uncalibrated visibilities, too. To be able to
restore the flags to the state before {\tt applycal} is starting its
duty, the task will make a backup of your current flags by default ({\tt
  flagbackup=True}). Restore them with {\tt flagmanager}, if you are
not happy with the {\tt applycal} results.



If you are not doing polarization calibration or imaging, then you can set 
{\tt parang=False} to make the calculations faster.  If you are
applying polarization calibration, or wish to make polarization
images, then set {\tt parang=True} so that the parallactic angle
rotation is applied to the appropriate correlations.  Currently,
you must do this in {\tt applycal} as this cannot be done on-the-fly
in {\tt clean} or {\tt mosaic}.  
See \S~\ref{section:cal.solve.pars.prior} for more on {\tt parang}.


For example, to apply the final bandpass and flux-scaled gain
calibration tables solutions to the NGC5921 data:
\small
\begin{verbatim}
default('applycal')

vis='ngc5921.usecase.ms'

# We want to correct the calibrators using themselves
# and transfer from 1445+099 to itself and the target N5921

# Start with the fluxscale/gain and bandpass tables
gaintable=['ngc5921.usecase.fluxscale','ngc5921.usecase.bcal']
         
# pick the 1445+099 (field 1) out of the gain table for transfer
# use all of the bandpass table
gainfield = ['1','*']

# interpolation using linear for gain, nearest for bandpass
interp = ['linear','nearest']

# only one spw, do not need mapping
spwmap = []

# all channels, no other selection
spw = ''
selectdata = False

# no prior calibration
gaincurve = False
opacity = 0.0

# select the fields for 1445+099 and N5921 (fields 1 and 2)
field = '1,2'

applycal()

# Now for completeness apply 1331+305 (field 0) to itself

field = '0'
gainfield = ['0','*']

applycal()

# The CORRECTED_DATA column now contains the calibrated visibilities
\end{verbatim}
\normalsize

In another example, we apply the final cumulative self-calibration 
of the Jupiter continuum data obtained in the example of
\S~\ref{section:cal.tables.accum.incr}:
\small
\begin{verbatim}
applycal(vis='jupiter6cm.usecase.split.ms',
         gaintable='jupiter6cm.usecase.selfcal1',
         selectdata=False)
\end{verbatim}
\normalsize

Again, it is important to remember the relative nature of each calibration
term.  A term solved for in the presence of others is, in effect,
residual to the others, and so must be used in combination with them
(or new versions of them) in subsequent processing.  At the same time,
it is important to avoid isolating the same calibration effects in
more than one term, e.g., by solving for both {\tt 'G'} and {\tt 'T'} 
separately (without applying the other), and then using them together.  

It is always a good idea to examine the corrected data after calibration
(using {\tt plotxy} to compare the raw ({\tt 'data'}) and corrected 
({\tt 'corrected'}) visibilities), as we describe next.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Examine the Calibrated Data}
\label{section:cal.correct.exam}

Once the source data is calibrated using {\tt applycal}, 
you should examine the $uv$ data and flag anything that looks bad.  If
you find source data that has not been flanked by calibration scans,
delete it (it will not be calibrated).  

For example, to look at the calibrated Jupiter data in the last
example given in the previous section:
\small
\begin{verbatim}
plotxy('jupiter6cm.usecase.split.ms','uvdist','amp','corrected',
       selectdata=True,correlation='RR LL',fontsize = 14.0)
\end{verbatim}
\normalsize
will show the {\tt CORRECTED\_DATA} column.  See 
Figure~\ref{fig:applycal_jupiter}.

\begin{figure}[h!]
\begin{center}
\pngname{applycal_jupiter}{6}
\caption{\label{fig:applycal_jupiter} The final {\tt 'amp'} versus
{\tt 'uvdist'} plot of the self-calibrated Jupiter data, as shown
in {\tt plotxy}.  The {\tt 'RR LL'} correlations are selected.
No outliers that need flagging are seen. }
\hrulefill
\end{center}
\end{figure}

See \S~\ref{section:edit.plot} for a description of how to display and edit 
data using {\tt plotms} or {\tt plotxy}, and \S~\ref{section:display.ms} for use of
the {\tt viewer} to visualize and edit a Measurement Set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Resetting the Calibration Models ({\tt delmod} and {\tt clearcal})}
\label{section:cal.correct.clearcal}

Whenever calibration tasks are run, the models associated with the MS
will be overwritten. Sometimes, however, one would like to completely
remove the model and the task {\tt delmod} can perform this
functionality:

\small
\begin{verbatim}
#  delmod :: Deletes model representations in the MS
vis                 =         ''        #  Name of input visibility file (MS)
otf                 =       True        #  Delete the on-the-fly model data keywords
     field          =         ''        #  Select field using field id(s) or field name(s)

scr                 =      False        #  Delete the MODEL_DATA scr col (if it exists)
async               =      False        #  If true the taskname must be started using delmod(...)
\end{verbatim} 

To do so, the parameter {\tt otf} should be set to {\tt True}. {\tt
  delmod} can also be used if for any reason a MODEL\_column was
created and should be removed to avoid confusion between the
on-the-fly model and the MODEL column (the MODEL\_DATA column was
required in CASA 3.3 and earlier). This can be achieved with {\tt scr=T}.


{\tt delmod} generally replaces the functionality of the older {\tt clearcal}
task. If one still decides to use the MODEL\_DATA columns, however,
{\tt clearcal} is still useful and will reset both the MODEL\_DATA and
CORRECTED\_DATA columns to unity:

\small
\begin{verbatim}
CASA <11>: inp clearcal
#  clearcal :: Re-initializes the calibration for a visibility data set
vis                 =         ''        #  Name of input visibility file (MS)
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channel.
intent              =         ''        #  Select observing intent
addmodel            =      False        #  Add MODEL_DATA scratch column
async               =      False        #  If true the taskname must be started using clearcal(...)
\end{verbatim}
\normalsize

with {\tt field}, {\tt spw}, and {\tt intent} being data selection
parameters. {\tt addmodel} can be used to opt in/out of formation
of the MODEL\_DATA column.


With the introduction of the on-the-fly calculation of the
MODEL visibilities, and the fact that {\tt applycal} overwrites any
previously existing CORRECTED\_DATA column, {\tt clearcal} is not
required anymore unless {\tt usescratch=True} is chosen in calibration
tasks, and it is also not recommended to use {\tt clearcal} to create
the scratch columns at the beginning of data calibration; all benefits
from the on-the-fly model would be made obsolete.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other Calibration and UV-Plane Analysis Options}
\label{section:cal.other}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Splitting out Calibrated uv data ({\tt split})}
\label{section:cal.other.split}

The {\tt split} task will apply calibration and output a new sub-MS
containing a specified list of sources (usually a single source).
The inputs are:
\small
\begin{verbatim}
#  split :: Create a visibility subset from an existing visibility set:
vis          =         ''   #  Name of input measurement set
outputvis    =         ''   #  Name of output measurement set
datacolumn   = 'corrected'  #  Which data column(s) to split out
field        =         ''   #  Select field using field id(s) or field name(s)
spw          =         ''   #  Select spectral window/channels
width        =          1   #  Number of channels to average to form one output channel
antenna      =         ''   #  Select data based on antenna/baseline
timebin      =       '0s'   #  Value for timeaveraging
timerange    =         ''   #  Select data based on time range
scan         =         ''   #  Select data based on scan numbers
array        =         ''   #  Select (sub)array by array ID number(s)
uvrange      =         ''   #  Select data based on uv distance range
async        =      False   #  If true the taskname must be started using split(...)
\end{verbatim}
\normalsize

Usually you will run {\tt split} with {\tt datacolumn='corrected'} as
previous operations (e.g. {\tt applycal}) will have placed the
calibrated data in the {\tt CORRECTED\_DATA} column of the MS.  This
will produce a new MS with this corrected data in its {\tt DATA} 
column.  The modes available in {\tt datacolumn} are:
\small
\begin{verbatim}
   'data', 'model', 'corrected',                       # produce MS with single DATA column
   'data,model', 'data,corrected', 'model,corrected',  # pairs of columns
   'all'                                               # all columns 'data,model,corrected'
\end{verbatim}
\normalsize
We recommend sticking to the simple single-column modes (e.g.\ 
{\tt 'data'} or {\tt 'corrected'}) or {\tt 'all'} if all columns are in
the MS.  Further processing may get confused by mismatched pairs of columns.

For example, to split out 46 channels (5-50) from {\tt spw} 1 of
our NGC5921 calibrated dataset:
\small
\begin{verbatim}
split(vis='ngc5921.usecase.ms',       
      outputvis='ngc5921.split.ms',    
      field='2',                      # Output NGC5921 data (field 2)
      spw='0:5~50',                   # Select 46 chans from spw 0
      datacolumn='corrected')         # Take the calibrated data column
\end{verbatim}
\normalsize

\subsubsection{Averaging in {\tt split}}
\label{section:cal.other.split.average}

Time and channel averaging are available using the {\tt timebin}
and {\tt width} parameters.

The {\tt timebin} parameter gives the averaging interval.  It takes a
quantity, e.g.
\small
\begin{verbatim}
   timebin = '30s'
\end{verbatim}
\normalsize
and will combine scans during averaging.

When time averaging, the {\tt ignorables} subparameter can be used to
specify that the bins should \emph{not} be split by changes in
{\tt SCAN\_NUMBER}, (sub){\tt ARRAY\_ID}, and/or {\tt STATE\_ID}.


The {\tt width} parameter defines the number of channels to average to
form a given output channel.  This can be specified globally for all
{\tt spw}, e.g.

\small
\begin{verbatim}
   width = 5
\end{verbatim}
\normalsize

or specified per {\tt spw}, e.g.

\small
\begin{verbatim}
   width = [2,3]
\end{verbatim}
\normalsize

to average 2 channels of 1st spectral window selected and 3 in the 
second one.


{\bf ALERT:} When averaging channels split will produce negative channel widths
(as reported by listobs) if frequency goes down with increasing channel
number, whether or not the input channel widths are negative.  The
\emph{band}widths and channel resolutions will still be positive."


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Recalculation of uvw values ({\tt fixvis})}
\label{section:cal.other.fixvis}

Sometimes the u,v,w coordinates of a measurement set are not recorded
correctly by the correlator. In those cases, it may be necessary to
recalculate them based on the antenna positions. {\tt fixvis} will
perform this task.


\small
\begin{verbatim}
#  fixvis :: Recalculates (u, v, w) and/or changes Phase Center 
vis                 =         ''        #  Name of the input visibility set.
outputvis           =         ''        #  Name of the output visibility set.  (Can be the same
                                        #   as vis.)
field               =         ''        #  Fields to operate on.   = all.
refcode             =         ''        #  reference frame to convert UVW coordinates to
reuse               =       True        #  base UVW calculation on the old values?
phasecenter         =         ''        #  use this direction as phase center
async               =      False        #  If true the taskname must be started using fixvis(...)
\end{verbatim}
\normalsize

A useful feature of {\tt fixvis} is that it can also change the phase
center of a measurement set. This can be done with absolute coordinates
or using offsets. An example is:
\small
\begin{verbatim}
fixvis(vis='Moon.ms',outpuvis='Moon-fixed.ms',field='Moon', phasedir='J2000 9h25m00s 05d12m00s')
\end{verbatim}
\normalsize

that will recalculate the u,v,w coordinates relative to the new phase
center for the field 'Moon'.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hanning smoothing of uv data ({\tt hanningsmooth})}
\label{section:cal.other.hanningsmooth}

For strong spectral line sources (like RFI sources), the Gibbs
phenomenon may cause ringing across the frequency channels of an
observation. This is called the Gibbs phenomenon and a proven remedy is
the Hanning smoothing algorithm. Hanning smoothing is a running mean
across the spectral axis with a triangle as a smoothing kernel. The
central channel is weighted by 0.5 and the two adjacent channels by
0.25 to preserve the flux. Hanning smoothing significantly reduces
Gibbs ringing but there's no gain without a penalty and here it is the
loss of a factor of two in spectral resolution.

In CASA, the {\tt hanningsmooth} task will apply Hanning smoothing to a
spectral line uv data measurement set.  The inputs are:

\small
\begin{verbatim}
#  hanningsmooth :: Hanning smooth frequency channel data to remove Gibbs ringing
vis                 =         ''        #  Name of input visibility file (MS)
datacolumn          =      'all'        #  the name of the MS column into which
                                        #   to write the smoothed data
outputvis           =         ''        #  name of the output visibility file
                                        #   (MS)
async               =      False        #  If true the taskname must be started
                                        #   using hanningsmooth(...)
\end{verbatim}
\normalsize

{\tt hanningsmooth} will operate on the input measurement set if no
{\tt outputvis} file name is provided. This option will keep the disk
usage of large datasets under control. But one should be aware that
the data is overwritten. If {\tt outputvis} is provided, the task will
copy the input MS to a new file with that name and operate there. The
{\tt datacolumn} parameter determines which of the data columns is to
be Hanning smoothed: {\tt 'all', 'corrected'} or {\tt 'data'}. {\tt
  'all'} refers to both, the {\tt CORRECTED\_DATA} and the {\tt DATA}
column. If {\tt 'corrected'} is specified but does not exist in the
MS, {\tt hanningsmooth} will create this column for your convenience.

{\bf ALERT}: We intend to make the kernel size a user supplied
parameter.  In the longer term we intend to offer other varieties of
spectral smoothing as well.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MStransform ({\tt mstransform})}
\label{section:cal.other.mstransform}

{\tt MStransform} is an {\it experimental} task. It combines the
functionality of {\tt split}, {\tt cvel}, {\tt hanningsmooth}, to
split an MS, combine/separate/regrid spws and do channel and time
averaging, all in a single step. 


Up to date documentation for {\tt mstransform} can be found in the
inline help of {\tt mstransform} or
at this web address:\\

\url{http://www.eso.org/~scastro/ALMA/casa/MST/MSTransformDocs/MSTransformDocs.html}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model subtraction from uv data ({\tt uvsub})}
\label{section:cal.other.uvsub}

The {\tt uvsub} task will subtract the Fourier transform of the
associated model of the MS (added to the MS with the tasks {\tt ft} or {\tt
  setjy}) from that in the {\tt
  CORRECTED\_DATA} column in the input MS and store the result in that
same {\tt CORRECTED\_DATA} column. 

The reverse operation is achieved by specifying {\tt reverse = True}:
in that case {\tt uvsub} will add the value of the Fourier transform of
the associated model to that in the {\tt CORRECTED\_DATA} column in
the input MS and store the result in that same {\tt CORRECTED\_DATA}
column.

The inputs are:

\small
\begin{verbatim}
#  uvsub :: Subtract/add model from/to the corrected visibility data.

vis          =         ''   #  Name of input visibility file (MS)
reverse      =      False   #  reverse the operation (add rather than subtract)
async        =      False   
\end{verbatim}
\normalsize

For example:
\small
\begin{verbatim}
   uvsub('ngc5921.split.ms')
\end{verbatim}
\normalsize

{\bf ALERT:} Currently, {\tt uvsub} operates on the {\tt CORRECTED\_DATA}
column in the MS {\tt vis}.  Eventually we will provide the option to
write out a new MS.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{UV-Plane Continuum Subtraction ({\tt uvcontsub})}
\label{section:cal.other.uvcontsub}

At this point, consider whether you are likely to need continuum
subtraction.  If there is significant continuum emission present in
what is intended as a spectral line observation, continuum subtraction
may be desirable.  You can estimate and subtract continuum emission in
the $uv$-plane prior to imaging or wait and subtract an estimate of it
in the image-plane.  Note that neither method is ideal, and the choice
depends primarily upon the distribution and strength of the continuum
emission.  Subtraction in the $uv$-plane is desirable if continuum emission dominates
the source, since deconvolution of the line emission will be more robust
if it not subject to the deconvolution errors of the brighter continuum.
There is also a performance benefit since the continuum is nearly the same
in each channel of the observation, and it is desirable to avoid repeating
its deconvolution in each channel.  However, doing the continuum
estimation in the $uv$-plane has the serious drawback that interpolating
visibilities between channels is only a good approximation for emission
from near the phase center.  Thus, {\tt uvcontsub} will do an increasingly
poor job for emission distributed further from the phase center. 
If the continuum emission is relatively weak,
it is usually adequate to subtract it in the image plane; this is
described in the Image Analysis section of this cookbook.  Here, we
describe how to do continuum subtraction in the $uv$-plane.

The $uv$-plane continuum subtraction is performed by the {\tt
  uvcontsub} task.  First, determine which channels in your data cube
do not have line emission, perhaps by forming a preliminary image as
described in the next chapter.  This image will also help you decide
whether or not you need to come back and do $uv$-plane continuum
subtraction at all. 


The inputs to {\tt uvcontsub} are:
\small
\begin{verbatim}
#  uvcontsub :: Continuum fitting and subtraction in the uv plane
vis                 =         ''        #  Name of input MS.  Output goes to vis + ".contsub"
                                        #   (will be overwritten if already exists)
field               =         ''        #  Select field(s) using id(s) or name(s)
fitspw              =         ''        #  Spectral window:channel selection for fitting the
                                        #   continuum
combine             =         ''        #  Data axes to combine for the continuum estimation
                                        #   (none, or spw and/or scan)
solint              =      'int'        #  Continuum fit timescale (int recommended!)
fitorder            =          0        #  Polynomial order for the fits
spw                 =         ''        #  Spectral window selection for output
want_cont           =      False        #  Create vis + ".cont" to hold the continuum estimate.
async               =      False        #  If true the taskname must be started using
                                        #   uvcontsub(...)
\end{verbatim}
\normalsize

For each baseline, and over the timescale specified in {\tt solint},
{\tt uvcontsub} will provide a polynomial fit to the real and
imaginary parts of the (continuum-only) channels specified in {\tt
fitspw} (using the standard {\tt spw} selection syntax), 
and then subtract this model from all channels specified in {\tt spw}, or
from all channels in spectral windows of {\tt fitspw} if {\tt
  spw=''}. By setting the subparameter {\tt excludechannels=True}, the
channel selection in {\tt fitspw} will be inverted. In that case one
can select the line channels themselves and/or corrupted channels that
are not used in the continuum fit to the data. {\tt fitspw} can also
take frequency ranges, e.g. 

\small
\begin{verbatim}
fitspw='*:113.767~114.528GHz;114.744~115.447GHz'
\end{verbatim}
\normalsize

where '*' indicates to go across all spws. 


Typically, low orders for the polynomial work best, like 0$^{\rm th}$ (a
constant), or 1$^{\rm st}$ order (a linear fit). Use higher orders with caution
and check your results carefully.

Usually, one should set {\it solint='int'} which does no averaging and fits
each integration. However, if the continuum emission comes from a small
region around the phase center and {\it fitorder = 0}, then you can set solint
larger (as long as it is shorter than the timescale for changes in the
visibility function of the continuum).  If your scans are short enough you
can also use scan averaging with {\it combine='scan'} and {\it solint='inf'}. 
Be warned, setting solint too
large will introduce ``time smearing'' in the estimated continuum and thus
not properly subtract emission not at the phase center.  Increasing solint
speeds up the calculation but it does not improve the overall result
quality of {\tt uvcontsub} - although the continuum estimates of each baseline
may be noisy (just like each visibility in a continuum MS may be noisy),
it is better to use the ensemble of individual fits than to average the
ensemble before fitting.  Note that {\tt plotms} can do time and baseline
averaging on the fly to help you examine noisy data.


So, the recommended procedure is as follows:
\begin{itemize}
\item Finish calibration as described in the previous chapter.
\item Use the invert or clean task on the split result to form an exploratory
  image that is useful for determining the line-free channels.
\item Use {\tt uvcontsub} with as low fit orders as possible to estimate and subtract the
  continuum from {\it vis}, and write the continuum-subtracted dataset
  to {\it vis + '.contsub'}.
\item Use {\tt clean} with {\it vis + '.contsub'} to make an image cube of the line emission.
\item If a continuum image is desired, clean the line-free channels of the
  original MS with {\it mode='mfs'} and {\it spw=fitspw}. Note that
  using the line free channels directly is preferred over the imaging
  the 'continuum' model fitted by {\tt uvcontsub}. The fitting
  procedure will also fit noise and artifacts which produce a nice
  line cube when subtracted, but the model may not represent the true
  underlying continuum.   
\end{itemize}

For example, we perform $uv$-plane continuum subtraction on our NGC5921
dataset:
\small
\begin{verbatim}
# Want to use channels 4-6 and 50-59 for continuum
uvcontsub(vis='ngc5921.usecase.msâ€™,
field='N5921',
spw='',              # all spw (only 0 in this data)
fitspw='0:4~7;50~59' # channels 4-6 and 50-59
solint='int',        # leave it at the default
fitorder=0)          # mean only

# You will see it made a new MS:
# ngc5921.usecase.ms.contsub"
\end{verbatim}
\normalsize

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Spectral regridding of the MS ({\tt cvel})}
\label{section:cal.other.cvel}

Although not strictly a calibration operation, spectral regridding
of a MS is available to aid in calibration operations (e.g.\ continuum
subtraction) and preparation for imaging.  For this purpose, the
{\tt cvel} task has been developed.  

The inputs are:
\small
\begin{verbatim}
#  cvel :: regrid an MS to a new spectral window / channel structure or frame
vis                 =         ''        #  Name of input measurement set
outputvis           =         ''        #  Name of output measurement set
passall             =      False        #  Pass through (write to output MS) non-selected data
                                        #   with no change
field               =         ''        #  Select field using field id(s) or field name(s)
spw                 =         ''        #  Select spectral window/channels
selectdata          =      False        #  Other data selection parameters
mode                = 'velocity'        #   Regridding mode
     nchan          =         -1        #  Number of channels in output spw (-1=all)
     start          =          0        #  Velocity of first image channel: e.g. '0.0km/s'
     width          =          1        #  image channel width in velocity units: e.g. '-1.0km/s'
     interpolation  =   'linear'        #  Spectral interpolation method

phasecenter         =         ''        #  Image phase center: position or field index
restfreq            =         ''        #  rest frequency (see help)
outframe            =         ''        #  Output frame (''=keep input frame)
veltype             =    'radio'        #  velocity definition
hanning             =      False        #   If true, Hanning smooth data before regridding to
                                        #   remove Gibbs ringing.
async               =      False        #  If true the taskname must be started using cvel(...)

\end{verbatim}
\normalsize

The key parameters for the operation of {\tt cvel} are the regridding
{\tt mode}, the output reference {\tt outframe} and {\tt veltype}, and
the standard selection parameters (in particular {\tt spw} and {\tt field}).

The syntax for {\tt mode} options 
({\tt 'channel'},{\tt 'velocity'},{\tt 'frequency'},{\tt 'channel\_b'})
has been made compatible with the
respective modes of {\tt clean} (\S~\ref{section:im.pars.mode}).  The
combination of selected {\tt spw} and {\tt mode} will determine the
output channels and spw(s):
\small
\begin{verbatim}
    spw = ’0,1’; mode = ’channel’  
       # will produce a single spw containing all channels in spw 0 and 1  
    spw=’0:5~28^2’; mode = ’channel’  
       # will produce a single spw made with channels (5,7,9,...,25,27)  
    spw = ’0’; mode = ’channel’: nchan=3; start=5; width=4  
       # will produce an spw with 3 output channels  
       # new channel 1 contains data from channels (5+6+7+8)  
       # new channel 2 contains data from channels (9+10+11+12)  
       # new channel 3 contains data from channels (13+14+15+16)  
    spw = ’0:0~63^3’; mode=’channel’; nchan=21; start = 0; width = 1  
       # will produce an spw with 21 channels  
       # new channel 1 contains data from channel 0  
       # new channel 2 contains data from channel 2  
       # new channel 21 contains data from channel 61  
    spw = ’0:0~40^2’; mode = ’channel’; nchan = 3; start = 5; width = 4  
       # will produce an spw with three output channels  
       # new channel 1 contains channels (5,7)  
       # new channel 2 contains channels (13,15)  
       # new channel 3 contains channels (21,23) 
\end{verbatim}
\normalsize

The simplest use of {\tt cvel} is to shift a single spectral window
into an output frame without regridding.  This is done with 
{\tt mode='channel'}.  For example:
\small
\begin{verbatim}
cvel(vis='test_w3oh_nohann.ms',
     outputvis ='test_w3oh_nohann_chanbary.ms',
     mode='channel',nchan=-1,start=0,width=1,
     interpolation='linear',
     phasecenter='',
     spw='',
     restfreq='1665.4018MHz',
     outframe='BARY')
\end{verbatim}
\normalsize
does this for an observation of the OH line.

There is also a special {\tt mode='channel\_b'} that does not force a
linear output frequency grid, e.g.\ for irregularly spaced/overlapping
spectral windows), but is nominally faster.  This is not equivalent to
a {\tt clean} output gridding mode, although {\tt clean} will
work on this spectral lattice.

Mode {\tt channel} is intended to not interpolate between channels. It
will perform binning if needed.  For most scientific applications we
recommend using the {\tt mode='velocity''} and {\tt mode='frequency'}
options, as it is easiest to determine what the resulting
channelization will be. For example: \small
\begin{verbatim}
cvel(vis='test_w3oh_nohann.ms',
     outputvis ='test_w3oh_nohann_cvellsrk.ms',
     mode='velocity',nchan=45,start='-35.0km/s',width='-0.55km/s',
     interpolation='linear',
     phasecenter='',
     spw='',
     restfreq='1665.4018MHz',
     outframe='LSRK')

cvel(vis='test_w3oh_nohann.ms',
     outputvis ='test_w3oh_nohann_cvelbary.ms',
     mode='velocity',nchan=45,start='-35.0km/s',width='-0.55km/s',
     interpolation='linear',
     phasecenter='',
     spw='',
     restfreq='1665.4018MHz',
     outframe='BARY')

\end{verbatim}
\normalsize will transform a MS into the LSRK and BARYcenter frames
respectively.  

The sign of the {\tt width} parameter determines whether the channels
run along increasing or decreasing values of frequency or velocity (i.e.
if the cube is reversed or not). 



The intent of {\tt cvel} regridding is to transform channel labels and
the visibilities to a spectral reference frame which is appropriate
for the science analysis, e.g. from TOPO to LSRK, e.g. to correct for
Doppler shifts throughout the time of the observation. Naturally,
this will change the shape of the spectral features to some
extent. According to the Nyquist theorem you should oversample a
spectrum with twice the numbers of channels to retain the
shape. Based on some tests, however, we recommend to observe with at
least 3-4 times the number of channels for each significant spectral
feature (like 3-4 channels per linewidth). This will minimize
regridding artifacts in {\tt cvel}.  

If {\tt cvel} has already established the grid that is desired for the
imaging, {\tt clean} should be run with the default channel mode ({\tt
  > width=1}) or with exactly the same frequency/velocity parameters
as was used in {\tt cvel}. This will avoid additional regridding in {\tt
  clean}. Hanning smoothing is optionally offered in {\tt cvel}, but
tests have shown that already the regridding process itself, if it
involved a transformation from TOPO to a non-terrestrial reference
frame, implies some smoothing (due to channel interpolation) such that
Hanning smoothing may not be necessary.


The interpolation method {\tt fftshift} calculates the transformed
visibilities by applying a FFT, then a phase ramp, and then an inverse
FFT.  Note that if you want to use this interpolation method, your
frequency grid needs to be equidistant, i.e. it only works in mode
{\tt velocity} with {\tt veltype}=radio, in mode {\tt frequency}, and
in mode {\tt channel} (in the latter only if the input grid is itself
equidistant in frequency). Note also that, as opposed to all other
interpolation methods, this method will apply a {\it constant}
(frequency-independent) shift in frequency which is not fully correct
in the case of large fractional bandwidth of the given spectral
window.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{UV-Plane Model Fitting ({\tt uvmodelfit})}
\label{section:cal.other.uvmodelfit}

It is often desirable to fit simple analytic source component models
directly to visibility data.  Such fitting has its origins in early
interferometry, especially VLBI, where arrays consisted of only a few
antennas and the calibration and deconvolution problems were poorly
constrained.  These methods overcame the calibration uncertainties by
fitting the models to calibration-independent closure quantities and
the deconvolution problem by drastically limiting the number of free
parameters required to describe the visibilities.  Today, even with
larger and better calibrated arrays, it is still desirable to use
visibility model fitting in order to extract geometric properties such
as the positions and sizes of discrete components in radio sources.
Fits for physically meaningful component shapes such as disks, rings,
and optically thin spheres, though idealized, enable connecting source
geometry directly to the physics of the emission regions.

Visibility model fitting is carried out by the {\tt uvmodelfit} task.
The inputs are:
\small
\begin{verbatim}
#  uvmodelfit :: Fit a single component source model to the uv data:

vis         =         ''   #  Name of input visibility file
field       =         ''   #  field name or index
spw         =         ''   #  spectral window
selectdata  =      False   #  Activate data selection details
niter       =          5   #  Number of fitting iterations to execute
comptype    =        'P'   #  Component type (P=pt source,G=ell. gauss,D=ell. disk)
sourcepar   =  [1, 0, 0]   #  Starting guess (flux,xoff,yoff,bmajaxrat,bpa)
varypar     =         []   #  Which parameters can vary in fit
outfile     =         ''   #  Optional output component list table
async       =      False   #  if True run in the background, prompt is freed
\end{verbatim}
\normalsize
{\bf ALERT:} This task currently only fits a single component.

The user specifies the number of non-linear solution
iterations ({\tt niter}), the component type ({\tt comptype}), an
initial guess for the component parameters ({\tt sourcepar}), and
optionally, a vector of Booleans selecting which component parameters
should be allowed to vary ({\tt varypar}), and a filename in which to
store a CASA componentlist for use in other applications ({\tt file}).
Allowed {\tt comptype}s are currently point {\tt 'P'} or
Gaussian {\tt 'G'}.

The function returns a vector containing the resulting parameter list.
This vector can be edited at the command line, and specified as input
({\tt sourcepar}) for another round of fitting.

The {\tt sourcepar} parameter is currently the only way to specify the
starting parameters for the fit.  For points, there are three
parameters: I (total flux density), and relative direction (RA, Dec)
offsets (in arcsec) from the observation's phase center.  For
Gaussians, there are three additional parameters: the Gaussian's
semi-major axis width (arcsec), the aspect ratio, and position angle
(degrees).  It should be understood that the quality of the result is
very sensitive to the starting parameters provided by the user.  If
this first guess is not sufficiently close to the global $\chi^2$
minimum, the algorithm will happily converge to an incorrect local
minimum.  In fact, the $\chi^2$ surface, as a function of the
component's relative direction parameters, has a shape very much like
the inverse of the absolute value of the dirty image of the field.
Any peak in this image (positive or negative) corresponds to a local
$\chi^2$ minimum that could conceivable capture the fit.  It is the
user's responsibility to ensure that the correct minimum does the
capturing.

Currently, {\tt uvmodelfit} relies on the likelihood that the source
is very near the phase center (within a beamwidth) and/or the user's
savvy in specifying the starting parameters.  This fairly serious
constraint will soon be relieved somewhat by enabling a rudimentary
form of uv-plane weighting to increase the likelihood that the
starting guess is on a slope in the correct $\chi^2$ valley.

Improvements in the works for visibility model fitting include:

\begin{itemize}
   \item User-specifiable uv-plane weighting
   \item Additional component shapes, including elliptical disks, rings,
         and optically thin spheroids.
   \item Optional calibration pre-application
   \item Multiple components.  The handling of more than one component
         depends mostly on efficient means of managing the list itself (not easy in
         command line options), which are currently under development.
   \item Combined component and calibration fitting.
\end{itemize}

Example (see Figure~\ref{fig:modelfit}):
\small
\begin{verbatim}
  #
  # Note: It's best to channel average the data if many channels
  # before running a modelfit
  #
  split('ngc5921.ms','1445+099_avg.ms',
           datacolumn='corrected',field='1445*',width='63')

  # Initial guess is that it's close to the phase center
  # and has a flux of 2.0 (a priori we know it's 2.47)
  uvmodelfit('1445+099_avg.ms',       # use averaged data
           niter=5,               # Do 5 iterations
           comptype='P',          # P=Point source, G=Gaussian, D=Disk
           sourcepar=[2.0,.1,.1], # Source parameters for a point source
           spw='0',               # 
           outfile='gcal.cl')     # Output component list file
  
  # Output looks like:
   There are 19656 - 3 = 19653 degrees of freedom.
    iter=0:   reduced chi2=0.0418509:  I=2,  dir=[0.1, 0.1] arcsec
    iter=1:   reduced chi2=0.003382:  I=2.48562,  dir=[-0.020069, -0.0268826] arcsec
    iter=2:   reduced chi2=0.00338012:  I=2.48614,  dir=[0.00323428, -0.00232235] arcsec
    iter=3:   reduced chi2=0.00338012:  I=2.48614,  dir=[0.00325324, -0.00228963] arcsec
    iter=4:   reduced chi2=0.00338012:  I=2.48614,  dir=[0.00325324, -0.00228963] arcsec
    iter=5:   reduced chi2=0.00338012:  I=2.48614,  dir=[0.00325324, -0.00228963] arcsec
   If data weights are arbitrarily scaled, the following formal errors
    will be underestimated by at least a factor sqrt(reduced chi2). If 
    the fit is systematically poor, the errors are much worse.
   I = 2.48614 +/- 0.0176859
   x = 0.00325324 +/- 0.163019 arcsec
   y = -0.00228963 +/- 0.174458 arcsec
   Writing componentlist to file: /home/sandrock/smyers/Testing/Patch2/N5921/gcal.cl

  # Fourier transform the component list to a model of the MS
  ft('1445+099_avg.ms', complist='gcal.cl')           

  # Plot data versus uv distance
  plotxy('1445+099_avg.ms', xaxis='uvdist', datacolumn='corrected')

  # Specify green circles for model data (overplotted)
  plotxy('1445+099_avg.ms', xaxis='uvdist', datacolumn='model',
         overplot=True, plotsymbol='go') 
\end{verbatim}
\normalsize

\begin{figure}[h!]
\begin{center}
%\gname{modelfit}{5}
\pngname{plotxy_modelfit}{6}
\caption{\label{fig:modelfit} Use of plotxy to display corrected data
  (red and blue points) and uv model fit data (green circles).} 
\hrulefill
\end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reweighing visibilities based on their scatter ({\tt
    statwgt})}
\label{section:cal.other.statwt}

{\bf Alert:} {\tt statwgt} is still an experimental task. Please check
the results carefully and report any problems to the NRAO CASA
helpdesk. 

I most cases, the data that comes from the telescopes have the correct
absolute or relative weights associated (absolute weights will be
supplied once the VLA switched power application becomes standard; for
ALMA the Tsys application is already in place). However, there are
data sets where one would like to adjust the weights based on the
scatter of the visibilities (typically as a function of time, antenna,
and/or baseline). This calculation is performed by the task {\tt
  statwt} that updates the WEIGHT and SIGMA columns of the measurement
set. {\tt statwt} inputs are:

\small
\begin{verbatim} 
#  statwt :: Reweight visibilities according to their scatter
vis                 =         ''        #  Name of measurement set
dorms               =      False        #  Use rms instead of stddev?
byantenna           =      False        #  Estimate the noise per antenna -not
                                        #   implemented (vs. per baseline)
fitspw              =         ''        #  The signal-free spectral window:channels
                                        #   to estimate the scatter from
fitcorr             =         ''        #  The signal-free correlation(s) to estimate
                                        #   the scatter from (not implemented)
combine             =         ''        #  Let estimates span changes in spw, corr,
                                        #   scan and/or state
timebin             =       '0s'        #  Bin length for estimates (not implemented)
minsamp             =          2        #  Minimum number of unflagged visibilities
                                        #   for estimating the scatter
field               =         ''        #  Select field using ID(s) or name(s)
spw                 =         ''        #  Select spectral window/channels
antenna             =         ''        #  Select data based on antenna/baseline
timerange           =         ''        #  Select data by time range
scan                =         ''        #  Select data by scan numbers
intent              =         ''        #  Select data by scan intents
array               =         ''        #  Select (sub)array(s) by array ID number
correlation         =         ''        #  Select correlations to reweight
observation         =         ''        #  Select by observation ID(s)
datacolumn          = 'corrected'       #  Which data column to calculate the scatter
                                        #   from
async               =      False        #  If true the taskname must be started using
                                        #   statwt(...)
\end{verbatim}

{\tt statwt} should only be run after all calibration steps have been
performed. The parameter {\tt dorms} switches from a scatter standard
deviation to a root mean square scatter estimator. {\tt datacolumn}
specifies the column on which the task operates and the usual data
selection parameters apply. Channels with strong RFI or a spectral
line should be avoided for the calculation and good channel range
should be specified via {\tt fitspw}. In its current implementation,
{\tt statwgt} uses data samples of an integration time interval but
eventually wider sample intervals can be specified by the {\tt
  timebin} parameter. Those samples are contained within a scan, spw,
and polarization product but using the {\tt combine} can relax this
restriction. {\tt minsamp} sets the minimum number of unflagged
visibilities used for the calculation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Change the signs of visibility phases ({\tt
    conjugatevis})}
\label{section:cal.other.conjugatevis}

{\tt conjugatevis} is an easy task to flip the signs of the phases of
visibilities, thus creating the complex conjugate numbers. The inputs
are like:

\small
\begin{verbatim} 
#  conjugatevis :: Change the sign of the phases in all visibility columns.
vis                 =         ''        #  Name of input visibility file.
spwlist             =         ''        #  Spectral window selection
outputvis           =         ''        #  Name of output visibility file
overwrite           =      False        #  Overwrite the outputvis if it exists.
async               =      False        #  If true the taskname must
                                        #   be started using conjugatevis(...)
\end{verbatim}
\normalsize

The task works on all scratch columns. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Manipulation of Ephemeris Objects}
\label{section:cal.eph.manip}

When an astronomical object has a proper motion, in particular objects
in our solar system, a static (RA,Dec) position in the FIELD table of
the MeasurementSet will not accurately describe the time-dependent
position. Prior to CASA~4.2, there was no support for ephemeris objects
other than the built-in reference frames for the Sun, the Moon, and the
planets out to PLUTO. With CASA~4.2, several new features were
introduced which help the user to attach an arbitrary ephemeris to a
given field and work with the object from calibration to imaging.

\subsubsection{Ephemeris tables}

The CASA format for ephemeris tables has been defined was introduced
in the early development stages of CASA in connection with the
Measures module. The {\tt me} tool permits position calculations based
on ephemerides in this format. Two examples for such tables can be
found in the distribution directory in subdirectory {\tt
  data/ephemerides}: {\tt VGEO} is an ephemeris of Venus in the
geocentric reference frame while {\tt VTOP} is an ephemeris for the
same object in the {\tt TOPO} reference fame for the observatory
location of the VLA. With the introduction of solar system source
models (Butler) in the {\tt setjy} task, a nearly complete set of
ephemerides for the larger bodies in our solar system had to be made
available. These are stored in nearly the same format as the above
examples {\tt VGEO} and {\tt VTOP} (but with a few enhancements) in
directory {\tt data/ephemerides/JPL-Horizons}. If your object's
ephemeris is among those stored in {\tt
  data/ephemerides/JPL-Horizons}, you can simply copy the ephemeris
from there. Otherwise, you can request the ephemeris from the
JPL-Horizons using the CASA commands (for example)

\small
\begin{verbatim}   
import recipes.ephemerides.request as jplreq
jplreq.request_from_JPL(objnam='Mars',startdate='2012-01-01',enddate='2013-12-31',
date_incr='0.1 d', get_axis_orientation=False,
get_axis_ang_orientation=True,
get_sub_long=True, use_apparent=False, get_sep=False,
return_address='YOUR_EMAIL_ADDESS',
mailserver='YOUR_MAIL_SERVER_ADDRESS')
\end{verbatim}
\normalsize

where you need to fill in the parameters {\tt objnam}, {\tt
  startdate}, {\tt enddate},{\tt date\_incr} (the time interval between
individual ephemeris table entries), {\tt return\_address} (your email
address where you want to receive the ephemeris), and {\tt mailserver}
(the smtp server through which you want to send the request email). The
other parameters should be set as shown. Within a short time, you
should receive the requested ephemeris as an email from NASA's
JPL-Horizonssystem. Save the email into a file with the ``save as''
function of your mail client. See the next section on how to attach it
to your dataset.

\subsubsection{Using {\tt fixplanets} to attach ephemerides to a field
  of a MeasurementSet}
In order to set the ephemeris of a given field in a MeasurementSet,
you can use the task {\tt fixplanets} as in the following example:

\small
\begin{verbatim}    
fixplanets(vis='uid___A002_X1c6e54_X223.ms',
field='Titan', fixuvw=True, direction='mytitanephemeris')
\end{verbatim}
\normalsize

where you need to set the parameters {\tt vis} to the name of your MS
and the parameter {\tt field} to the name of the field to which you
want to attach the ephemeris. The parameter {\tt direction} must be set
to the name of your ephemeris table. Accepted formats are (a) the CASA
format (as in {\tt VGEO} or the ephemerides in {\tt
  data/ephemerides/JPL-Horizons} as described above)  and (b) the
JPL-Horizons mail format which you obtain by saving an ephemeris
email you received from JPL-Horizons. The parameter {\tt fixuvw} should
be set to True in order to trigger a recalculation of the UVW
coordinates in your MS based on the new ephemeris. The task {\tt
  fixplanets} can also be used for other field direction
modifications. Please refer to the help text of the task.


Note that among the ephemerides in the directory data {\it
  /ephemerides/JPL-Horizons/} you should only use those ending in {\it
  '\_J2000.tab'}. They are the ones in J2000 coordinates.

\subsubsection{Use of the ephemeris after attachment}
\label{section:cal.eph.manip.attach}
Once you have attached the ephemeris to a field of an MS, it will
automatically be handled in tasks like {\tt split} and {\tt concat}
which need to hand on the ephemeris to their output MSs. In particular
{\tt concat} recognizes when fields of the MSs to be concatenated use
the same ephemeris and merges these fields {\bf if} the time coverage
of the provided ephemeris in the first MS also covers the observation
time of the second MS. The ephemeris of the field in the first MS will
then be used for the merged field. In order to inspect the ephemeris
attached to a field, you can find it inside the {\tt FIELD}
subdirectory of your MS. The optional column {\tt EPHEMERIS\_ID} in the
FIELD table points to the running number of the ephemeris table. A
value of $-1$ indicates that no ephemeris is attached. Note that in case
an ephemeris is attached to a field, the direction column entries for
that field in the FIELD table will be interpreted as an {\bf offset}
to the ephemeris direction and are therefore set to (0.,0.) by default.
This offset feature is used in mosaic observations where several
fields share the same ephemeris with different offsets. The {\tt TIME}
column in the FIELD table should be set to the beginning of the
observation for that field and serves as the nominal time for ephemeris
queries.

\subsubsection{Spectral frame transformation to the rest frame
  of the ephemeris object in task {\tt cvel}} 

The ephemerides contain radial velocity information. The task {\tt
  cvel} can be used to transform spectral windows into the rest frame
of the ephemeris object by setting the parameter {\tt outframe} to
``SOURCE'' as in the following example:

\small
\begin{verbatim}
  cvel(vis='europa.ms',
  outputvis='cvel_europa.ms', outframe='SOURCE', mode = 'velocity',
  width = '0.3km/s', restfreq = '354.50547GHz')
\end{verbatim}
\normalsize

This will
make {\tt cvel} perform a transformation to the GEO reference frame followed
by an additional Doppler correction for the radial velocity given by
the ephemeris for the each field. (Typically, this should happen after
calibration and after splitting out the spectral widows and the target
of interest). The result is an MS with a single combined spectral
window in reference frame REST. From this frame, further
transformations to other reference frames are not possible.


\subsubsection{Ephemerides in ALMA datasets}

The ALMA Science Data
Model (the raw data format for ALMA data) now foresees an ephemeris
table. However, this feature is not yet used in ALMA Cycle 1. Once this
table will be filled at the observatory, the task {\tt importasdm} will
automatically translate it into an ephemeris table in CASA format and
attach it to the respective fields.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples of Calibration}
\label{section:cal.examples}

The data reduction tutorials on \url{casaguides.nrao.edu} provide
walkthroughs for
high and low frequency, spectral line and polarization calibration
techniques. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
