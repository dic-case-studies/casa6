##################### generated by xml-casa (v2) from ms.xml ########################
##################### b03336ca7c9c8dad85593010d995ae09 ##############################
from __future__ import absolute_import 
from .__casac__ import ms as _ms
from .platform import str_encode as _str_ec
from .platform import str_decode as _str_dc
from .platform import dict_encode as _dict_ec
from .platform import dict_decode as _dict_dc
from .platform import dict_encode as _quant_ec
from .platform import dict_decode as _quant_dc
from .platform import encode as _any_ec
from .platform import decode as _any_dc
from .typecheck import validator as _pc
from .coercetype import coerce as _coerce
from .table import table as _wrap_table
from .msmetadata import msmetadata as _wrap_msmetadata

class ms:
    ### self
    def __init__(self, *args, **kwargs):
        """This is the most commonly used constructor. It creates an ms tool
        which is attached to the specified measurement set table.
        
        By default the table is opened read only to prevent you from
        accidently making changes to the measurement set. Set nomodify to
        False you you do intend to make changes.
        
        Setting the lock argument to True will permanently lock the table
        preventing other processes from writing to the measurement set.
        Unless you expect this to happen, and want to prevent it, you
        should leave the lock argument at the default value which implies
        auto-locking.
        
        The host argument specifies which machine the precompiled ms
        process should be started on. The default value starts it on the
        same machine as the one that casapy is running on.
        
        In order to run the ms tool on a remote machine you need to
        satisfy all the following conditions.
        begin{itemize}
        item It must be possible to start casa on the remote machine
        item You must be able to log onto the remote machine without
        having to type a password
        item The CASAPATH environment variable must be defined on the
        remote machine. You may want to set this up in the relevant
        ``dot'' file eg., adding a line like
        texttt{source~/usr/local/aips++/aipsinit.csh} in your
        .cshrc file (for csh).
        end{itemize}
        One quick way to check if all three conditions are met is to type,
        on your local machine (rsh host 'echo $CASAPATH') where host is
        replaced by the name of the remote machine. If the value of the
        CASAPATH variable that is printed does not contain something like
        {aips-root~architecture~site~host} and that all the values are
        correct for the remote machine you can be certain that starting the
        ms tool, or any casa server, on the remote host will not work.
        
        Each ms tool can only run one function at a time. To solve this
        you start two servers. The forcenewserver argument allows you to
        do this by overriding the default behaviour of having each ms tool
        share the same server.
        
        This function returns an ms tool or fail if something went wrong,
        like an error in the measurement set name.
        
        
        """
        self._swigobj = kwargs.get('swig_object',None)
        if self._swigobj is None:
            self._swigobj = _ms()

    def open(self, thems='', nomodify=True, lock=False, check=False):
        """Use this function when you have detached (using the close function)
        the ms tool from a measurement set table and wish to reattach to another
        measurement set table.
        
        If check=true, additional referential integrity checks on the MS
        are run. If any of these fail, an exception is thrown and the MS
        is not open (since it is not a valid MS).
        
        """
        schema = {'thems': {'type': 'cStr'}, 'nomodify': {'type': 'cBool'}, 'lock': {'type': 'cBool'}, 'check': {'type': 'cBool'}}
        doc = {'thems': thems, 'nomodify': nomodify, 'lock': lock, 'check': check}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _open_result = self._swigobj.open(_str_ec(_pc.document['thems']), _pc.document['nomodify'], _pc.document['lock'], _pc.document['check'])
        return _open_result

    def reset(self):
        """This function re-attaches the ms tool to the original MS,
        effectively discarding any prior operations, in particular any
        data selection operations using msselect function.
        
        """
        _reset_result = self._swigobj.reset()
        return _reset_result

    def close(self):
        """This function detaches the ms tool from the associated
        measurement set table after flushing all the cached changes.
        After calling this function the ms tool is not associated with
        any measurement set and using any function other than open or fromfits
        will result in an error message being sent to the logger.
        
        This function can be useful to avoid synchronization problems
        which can occur when different processes have the same ms open.
        
        """
        _close_result = self._swigobj.close()
        return _close_result

    def done(self):
        """You should call close() when you are finished using the ms tool
        to close the measurement set table and free any associated file
        locks. The measurement set is not deleted.
        
        
        """
        _done_result = self._swigobj.done()
        return _done_result

    def name(self):
        """This function returns the name of the measurement set table that
        is being manipulated. If the ms tool is not attached to any
        measurement set, this function will return the string ``none''.
        
        """
        _name_result = _str_dc(self._swigobj.name())
        return _name_result

    def iswritable(self):
        """This function returns True if the underlying MeasurementSet
        was opened for writing/update.
        
        """
        _iswritable_result = self._swigobj.iswritable()
        return _iswritable_result

    def nrow(self, selected=False):
        """This function returns the number of rows in the measurement set.
        If the optional argument selected is set to True, it returns the
        number of currently selected rows, otherwise it returns the
        number of rows in the original measurement set.
        
        """
        schema = {'selected': {'type': 'cBool'}}
        doc = {'selected': selected}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _nrow_result = self._swigobj.nrow(_pc.document['selected'])
        return _nrow_result

    def getdata(self, items=[  ], ifraxis=False, ifraxisgap=int(0), increment=int(1), average=False):
        """This function reads the specified items from the currently
        selected measurement set and returns them in fields of a record.
        The main difference between this and direct access of the table,
        using the table tool, is that this function reads data from the
        selected measurement set, provides access to derived
        quantities like amplitude and flag_sum, and can reorder the
        data.
        
        As with the ms.range function, the items to read are specified
        using a vector of strings. Allowable items include: 'amplitude',
        'corrected_amplitude', 'model_amplitude', 'ratio_amplitude',
        'residual_amplitude', 'obs_residual_amplitude', 'antenna1',
        'antenna2', 'axis_info', 'data', 'corrected_data', 'float_data',
        'model_data', 'ratio_data', 'residual_data',
        'obs_residual_data', 'feed1', 'feed2', 'field_id', 'flag',
        'flag_row', 'flag_sum', 'ha' (added to 'axis_info'),
        'ifr_number', 'imaginary', 'corrected_imaginary',
        'model_imaginary', 'ratio_imaginary', 'residual_imaginary',
        'obs_residual_imaginary', 'last' (added to 'axis_info'),
        'phase', 'corrected_phase', 'model_phase', 'ratio_phase',
        'residual_phase', 'obs_residual_phase', 'real',
        'corrected_real', 'ratio_real', 'residual_real',
        'obs_residual_real', 'scan_number', 'sigma', 'data_desc_id',
        'time', 'ut' (added to 'axis_info'), 'uvw', 'u', 'v', 'w',
        'uvdist', and 'weight'. Unrecognized items will result in a
        warning being sent to the logger.  Corrected, model, and float
        visibilities will result in a warning if these columns do not
        exist.  Duplicate items are silently ignored.
        
        Note that 'ha', 'last', and 'ut' must be requested along with
        'axis_info' and ifraxis=True.  This data will be found in a
        subrecord of the returned record's 'axis_info' with the key in
        uppercase.  For example, for 'ut', the data is found in:
        rec['axis_info']['time_axis']['UT'].  See more information
        about 'axis_info' below.
        
        The record that is returned contains fields that correspond to
        each of the specified items. Most fields will contain an array.
        The array may be one, two or three dimensional depending on
        whether the corresponding row in the measurement set is a
        scalar, one-, or two-dimensional. Unless the ifraxis argument
        is set to True, the length of the last axis on these arrays
        will correspond to the number of rows in the selected
        measurement set.
        
        If the ifraxis argument is set to True, the row axis is split
        into an interferometer axis and a time axis. For example, a
        measurement set with 90 rows, in an array with 6 telescopes (so
        that there are 15 interferometers), may have a data array of
        shape [4,32,90] if ifraxis is False, or [4,32,15,6] if ifraxis
        is True (assuming there are 4 correlations and 32 channels). If
        there are missing rows, as will happen if not all
        interferometers were used for all time-slots, then a default
        value will be inserted.
        
        This splitting of the row axis may not happen for items where
        there is only a single value per row. For some items the
        returned vector will contain only as many values as there are
        interferometers and it is implicit that the same value should
        be used for all time slots. The antenna1, antenna2, feed1,
        feed2 and ifr_number items fall in this category. For other
        items, the returned vector will have as many values as there
        are time slots and it is implicit that the same value should be
        used for all interferometers. The field_id, scan_number,
        data_desc_id, and time items fall into this category.
        
        The 'axis_info' item provides data labelling information. It
        returns a record with the following fields: corr_axis,
        freq_axis, ifr_axis, and time_axis. The latter two fields are
        not present if ifraxis is set to False.
        
        1. The corr_axis field contains a string vector with elements like
        'RR' or 'XY' that indicates which polarizations were correlated
        together to produce the data. The length of this vector will
        always be the same as the length of the first axis of the data
        array.
        
        2. The freq_axis field contains a record with two fields, chan_freq
        and resolution. Each of these fields contains vectors which
        indicate the centre frequency and spectral resolution (FWHM)
        of each channel. The length of these vectors will be the same
        as the length of the second axis in the data.
        
        3. The ifr_axis field contains fields: ifr_number, ifr_name,
        ifr_shortname and baseline. The ifr_number is the same as
        returned by the 'ifr_number' item, 1000*antenna1+antenna2.
        The ifr_name and ifr_shortname are string vectors containing
        descriptions of the interferometer; ifr_name contains the names
        of the antenna pair separated by a hyphen, and ifr_shortname
        contains the ids of the antenna pair separated by a hyphen.
        The baseline is the Euclidian distance in meters between the two
        antennas. All of these vectors have a length equal to the number
        of interferometers in the selected measurement set, i.e., to the
        length of the third axis in the data when ifraxis is True.
        
        4. The time_axis field contains the MJD seconds field and
        optionally the HA, UT, and LAST fields. To include the optional
        fields, you need to add 'ha', 'last' or 'ut' strings to the list
        of requested items. All of the fields in the time_axis record
        contain vectors that indicate the time at the midpoint of the
        observation and are in seconds. The MJD seconds field is since
        0 hours on the day having a modified julian day number of zero
        and the rest are since midnight prior to the start of the
        observation.
        
        An optional gap size can be specified to visually separate
        groups of interferometers with the same antenna1 index (handy
        for identifying antennas in an interferometer vs time display).
        The default is no gap.
        
        An optional increment can be specified to return data from every
        row matching the increment only.
        
        When the average flag is set, the data will be averaged over the
        time axis if the ifraxis is True or the row axis i.e., different
        interferometers and times may be averaged together. In the
        latter case, some of the coordinate information, like
        antenna_id, will no longer make sense. When all data to be
        averaged is unflagged, the result is the averaged value and the
        corresponding flag is False. When all data is flagged, the
        result is set to zero and the corresponding flag is True.  When
        data to be averaged is mixed (unflagged and flagged), only the
        unflagged values are averaged and the flag is set to False.
        
        You need to call selectinit before calling this function.
        If you haven't then selectinit will be called for you with
        default arguments.
        
        Items prefixed with corrected, model, residual or obs_residual
        are not available unless your measurement set has been processed
        either with the imager or calibrator tools.
        
        """
        schema = {'items': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'ifraxis': {'type': 'cBool'}, 'ifraxisgap': {'type': 'cInt'}, 'increment': {'type': 'cInt'}, 'average': {'type': 'cBool'}}
        doc = {'items': items, 'ifraxis': ifraxis, 'ifraxisgap': ifraxisgap, 'increment': increment, 'average': average}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _getdata_result = _dict_dc(self._swigobj.getdata([_str_ec(_x) for _x in _pc.document['items']], _pc.document['ifraxis'], _pc.document['ifraxisgap'], _pc.document['increment'], _pc.document['average']))
        return _getdata_result

    def putdata(self, items={ }):
        """This function allows you to write values from casapy variables
        back into the measurement set table. The main difference between
        this and directly accessing the table using the table tool is
        that this function writes data to the selected measurement set.
        
        Unlike the getdata function, you can only put items that
        correspond to actual table columns. You cannot change the data
        shape either so that the number of correlations, channels and
        rows (or interferometers/time slots) must match the values in
        the selected measurement set. If the values were obtained using
        the getdata function with ifraxis argument set to True, then
        any default values added to fill in missing
        interferometer/timeslots pairs will be ignored when writing
        the modified values back using this function.
        
        Allowable items include:  'data', 'corrected_data',
        'model_data', 'flag', 'flag_row', 'sigma', and 'weight'.
        'float_data' is currently not implemented for putdata.
        
        The measurement set has to be opened for read/write access
        (nomodify=False) to be able to use this function.
        
        You need to call selectinit before calling this function.
        If you haven't then selectinit will be called for you with
        default arguments.
        
        Items prefixed with corrected, model, residual or obs_residual
        are not available unless your measurement set has been processed
        either with the imager or calibrator tools.
        
        """
        schema = {'items': {'type': 'cDict'}}
        doc = {'items': items}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _putdata_result = self._swigobj.putdata(_dict_ec(_pc.document['items']))
        return _putdata_result

    def fromfits(self, msfile='', fitsfile='', nomodify=True, lock=False, obstype=int(0), host='', forcenewserver=False, antnamescheme='old'):
        """This function will convert a uvfits file to a measurement set table
        and then open the measurement set table. The newly created
        measurement set table will continue to exist after the tool has
        been closed.
        
        Setting the lock argument to True will permanently lock the table
        preventing other processes from writing to the measurement set.
        Unless you expect this to happen, and want to prevent it, you
        should leave the lock argument at the default value which implies
        auto-locking.
        
        Note that the variety of fits files that fromfits is able to
        interpret correctly is limited mostly to files similar to those
        produced by classic AIPS. In particular, it understands only binary
        table extensions for the antenna (AN), frequency (FQ) and source
        (SU) information and ignores other extensions.
        
        This function returns True if it successfully attaches the ms tool
        to a newly created Measurement Set or False if something went
        wrong, like an error in a file name.
        
        NOTE ON WEIGHTS
        
        ms.fromfits() will generate a WEIGHT_SPECTRUM column in which it
        will fill the absolute value of the weight associated with each
        visibility in the uvfits file.  Negative weights will have the
        associated FLAGs set to True. It will compute the associated WEIGHT
        value for that MS row to be the sum of the absolute values of the
        associated WEIGHT_SPECTRUM values.
        
        """
        schema = {'msfile': {'type': 'cStr'}, 'fitsfile': {'type': 'cReqPath', 'coerce': _coerce.expand_path}, 'nomodify': {'type': 'cBool'}, 'lock': {'type': 'cBool'}, 'obstype': {'type': 'cInt'}, 'host': {'type': 'cStr'}, 'forcenewserver': {'type': 'cBool'}, 'antnamescheme': {'type': 'cStr'}}
        doc = {'msfile': msfile, 'fitsfile': fitsfile, 'nomodify': nomodify, 'lock': lock, 'obstype': obstype, 'host': host, 'forcenewserver': forcenewserver, 'antnamescheme': antnamescheme}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _fromfits_result = self._swigobj.fromfits(_str_ec(_pc.document['msfile']), _str_ec(_pc.document['fitsfile']), _pc.document['nomodify'], _pc.document['lock'], _pc.document['obstype'], _str_ec(_pc.document['host']), _pc.document['forcenewserver'], _str_ec(_pc.document['antnamescheme']))
        return _fromfits_result

    def fromfitsidi(self, msfile='', fitsfile='', nomodify=True, lock=False, obstype=int(0)):
        """This function will convert a uvfits file to a measurement set table
        and then open the measurement set table. The newly created
        measurement set table will continue to exist after the tool has
        been closed.
        
        Setting the lock argument to True will permanently lock the table
        preventing other processes from writing to the measurement set.
        Unless you expect this to happen, and want to prevent it, you
        should leave the lock argument at the default value which implies
        auto-locking.
        
        Note that the variety of fits files that fromfits is able to
        interpret correctly is limited mostly to files similar to those
        produced by classic AIPS. In particular, it understands only binary
        table extensions for the antenna (AN), frequency (FQ) and source
        (SU) information and ignores other extensions.
        
        This function returns True if it successfully attachs the ms tool
        to a newly created Measurement Set or False if something went
        wrong, like an error in a file name.
        
        """
        schema = {'msfile': {'type': 'cStr'}, 'fitsfile': {'type': 'cStr'}, 'nomodify': {'type': 'cBool'}, 'lock': {'type': 'cBool'}, 'obstype': {'type': 'cInt'}}
        doc = {'msfile': msfile, 'fitsfile': fitsfile, 'nomodify': nomodify, 'lock': lock, 'obstype': obstype}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _fromfitsidi_result = self._swigobj.fromfitsidi(_str_ec(_pc.document['msfile']), _str_ec(_pc.document['fitsfile']), _pc.document['nomodify'], _pc.document['lock'], _pc.document['obstype'])
        return _fromfitsidi_result

    def tofits(self, fitsfile='', column='corrected', field=[ ], spw=[ ], baseline=[ ], time='', scan=[ ], uvrange=[ ], taql='', writesyscal=False, multisource=False, combinespw=False, writestation=False, padwithflags=False, overwrite=False):
        """This function writes a uvfits file that contains the data in the
        measurement set associated with this tool. The fits file is always
        written in floating point format and the data are always stored in
        the primary array of the fits file.
        
        IMPORTANT NOTE: In general, some of the data averaging features of
        this method have never worked properly. In general, users should
        run mstransform to select and average data prior to running
        tofits(). The associated input parameters are slowly being
        deprecated and removed.
        
        If the measurement set has been imaged or calibrated in CASA, it
        may contain additional data columns. You need to select ONE of
        these columns to be written to the fits file. The possible
        options are:
        1. observed     This is the raw data as collected by the telescope. All
        interferometric measurement sets must contain this column.
        A synonym for 'observed' is 'data'.
        2. corrected    This is the calibrated data. A synonym for 'corrected' is
        'corrected_data'.
        3. model        This is the visibilites that would be measured using
        the current model of the sky. A synonym for 'model' is
        'model_data'.
        
        The parsing of these strings is case insensitive. If any other
        option is specified then the observed data will be written.
        
        By default a single-source uvfits file is written, but if the
        measurement set contains more than one field or if you set the
        multisource argument to True a multi-source uvfits file will be
        written. Because of limitations in the uvfits format you have to
        ensure that the data shape is fixed for all the data you intend to
        write to one fits file. See the general description of this tool
        for how you can select data to meet this condition.
        
        The combinespw argument is used to control whether data from
        different spectral windows will be written as different entries in
        the fits FQ (frequency) table or combined as different IF's
        within one entry in the FQ table. You should normally only set
        this to True if you know that the data from different spectral
        windows were observed simultaneously, and the data in the
        measurement set can be equally divided between all the spectral
        windows (i.e. each window should have the same width).  Use of
        this switch is recommended for data to be processed in classic
        AIPS and difmap (if possible, e.g., standard dual IF observations).
        
        The padwithflags argument is only relevant if combinespw is True.
        If true, it will fill in data that is 'missing' with flags to fit
        the IF structure.  This is appropriate if the MS had a few
        frequency-dependent flags applied, and was then time-averaged by
        split.  If the spectral windows were observed at different times,
        padwithflags=True will add a large number of flags, making the
        output file significantly longer.  It does not yet support spectral
        windows with different widths.
        
        The fits GC (gain curve) and TY (system temperature) tables can
        be optionally written by setting the writesyscal argument to True.
        This is a rather WSRT-specific operation at the moment and may not
        work correctly for measurement sets from other telescopes.
        
        One may overwrite the specified output file if it exists by
        specifying overwrite=True.
        
        NOTE ON WEIGHTS
        
        If the MS has no WEIGHT_SPECTRUM column, or if it does, but that
        column does not contain any data, ms.tofits() will compute the
        associated weight it writes to the uvfits file by taking the
        associated WEIGHT column value in the MS and dividing it by the
        number of channels associated with the spectral window of that
        visibility.
        
        """
        schema = {'fitsfile': {'type': 'cStr'}, 'column': {'type': 'cStr'}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'baseline': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'time': {'type': 'cStr'}, 'scan': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'uvrange': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'taql': {'type': 'cStr'}, 'writesyscal': {'type': 'cBool'}, 'multisource': {'type': 'cBool'}, 'combinespw': {'type': 'cBool'}, 'writestation': {'type': 'cBool'}, 'padwithflags': {'type': 'cBool'}, 'overwrite': {'type': 'cBool'}}
        doc = {'fitsfile': fitsfile, 'column': column, 'field': field, 'spw': spw, 'baseline': baseline, 'time': time, 'scan': scan, 'uvrange': uvrange, 'taql': taql, 'writesyscal': writesyscal, 'multisource': multisource, 'combinespw': combinespw, 'writestation': writestation, 'padwithflags': padwithflags, 'overwrite': overwrite}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _tofits_result = self._swigobj.tofits(_str_ec(_pc.document['fitsfile']), _str_ec(_pc.document['column']), _any_ec(_pc.document['field']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['baseline']), _str_ec(_pc.document['time']), _any_ec(_pc.document['scan']), _any_ec(_pc.document['uvrange']), _str_ec(_pc.document['taql']), _pc.document['writesyscal'], _pc.document['multisource'], _pc.document['combinespw'], _pc.document['writestation'], _pc.document['padwithflags'], _pc.document['overwrite'])
        return _tofits_result

    def listfits(self, fitsfile):
        """List HDU and typical data rows in a uvfits file
        
        """
        schema = {'fitsfile': {'type': 'cStr'}}
        doc = {'fitsfile': fitsfile}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _listfits_result = self._swigobj.listfits(_str_ec(_pc.document['fitsfile']))
        return _listfits_result

    def asdmref(self, abspath=''):
        """If the MS is imported from an ASDM  with option lazy=True, the DATA
        or FLOAT_DATA column of the MS is virtual and directly reads the
        visibilities from the ASDM. A reference to the original ASDM is
        stored with the MS. If the ASDM needs to be moved to a different
        path, the reference to it in the MS needs to be updated. This can
        be achieved with ms.asdmref().
        
        When called with an empty string (default), the method just reports
        the currently set ASDM path.
        
        Return value is a string containing the new path if the path was
        successfully set or (in the case abspath was empty) the MS indeed
        contains a ASDM reference, i.e. was lazily imported.
        
        If the ASDM does not contain an ASDM reference, the method returns
        an empty string. If abspath is not empty and there was an error
        setting the new reference, the method throws an exception.
        
        """
        schema = {'abspath': {'type': 'cStr'}}
        doc = {'abspath': abspath}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _asdmref_result = _str_dc(self._swigobj.asdmref(_str_ec(_pc.document['abspath'])))
        return _asdmref_result

    def concatenate(self, msfile='', freqtol=[ ], dirtol=[ ], weightscale=float(1.), handling=int(0), destmsfile='', respectname=False):
        """This function concatenates two measurement sets together.
        
        The data is copied from the measurement set specified in the
        msfile arguement to the end of the measurement set attached to the
        ms tool. If a lot of data needs to be copied this function may
        take some time. You need to open the measurement set for writing
        in order to use this function.
        
        """
        schema = {'msfile': {'type': 'cStr'}, 'freqtol': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'dirtol': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'weightscale': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'handling': {'type': 'cInt'}, 'destmsfile': {'type': 'cStr'}, 'respectname': {'type': 'cBool'}}
        doc = {'msfile': msfile, 'freqtol': freqtol, 'dirtol': dirtol, 'weightscale': weightscale, 'handling': handling, 'destmsfile': destmsfile, 'respectname': respectname}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _concatenate_result = self._swigobj.concatenate(_str_ec(_pc.document['msfile']), _any_ec(_pc.document['freqtol']), _any_ec(_pc.document['dirtol']), _pc.document['weightscale'], _pc.document['handling'], _str_ec(_pc.document['destmsfile']), _pc.document['respectname'])
        return _concatenate_result

    def testconcatenate(self, msfile='', freqtol=[ ], dirtol=[ ], respectname=False):
        """This function acts like ms.concatenate() with handling==3 (do not
        concatenate the MAIN and POINTING tables). This is useful for
        generating, e.g., SPECTRAL_WINDOW and FIELD tables which contain
        all used SPW and FIELD ids for a set of MSs without having to
        actually carry out a time-consuming concatenation on disk. The MAIN
        table in the resulting output MS is that of the original MS, i.e.
        it is not touched.
        
        """
        schema = {'msfile': {'type': 'cStr'}, 'freqtol': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'dirtol': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'respectname': {'type': 'cBool'}}
        doc = {'msfile': msfile, 'freqtol': freqtol, 'dirtol': dirtol, 'respectname': respectname}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _testconcatenate_result = self._swigobj.testconcatenate(_str_ec(_pc.document['msfile']), _any_ec(_pc.document['freqtol']), _any_ec(_pc.document['dirtol']), _pc.document['respectname'])
        return _testconcatenate_result

    def virtconcatenate(self, msfile='', auxfilename='', freqtol=[ ], dirtol=[ ], weightscale=float(1.), respectname=True):
        """This function virtually concatenates two measurement sets together
        such that they can later be turned into a multi-MS with
        createmultims().
        
        You need to open the measurement set for writing in order to use
        this function.
        
        """
        schema = {'msfile': {'type': 'cStr'}, 'auxfilename': {'type': 'cStr'}, 'freqtol': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'dirtol': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'weightscale': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'respectname': {'type': 'cBool'}}
        doc = {'msfile': msfile, 'auxfilename': auxfilename, 'freqtol': freqtol, 'dirtol': dirtol, 'weightscale': weightscale, 'respectname': respectname}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _virtconcatenate_result = self._swigobj.virtconcatenate(_str_ec(_pc.document['msfile']), _str_ec(_pc.document['auxfilename']), _any_ec(_pc.document['freqtol']), _any_ec(_pc.document['dirtol']), _pc.document['weightscale'], _pc.document['respectname'])
        return _virtconcatenate_result

    def createmultims(self, outputTableName, tables, subtables, nomodify=True, lock=False, copysubtables=False, omitsubtables=[  ]):
        """
        """
        schema = {'outputTableName': {'type': 'cStr'}, 'tables': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'subtables': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'nomodify': {'type': 'cBool'}, 'lock': {'type': 'cBool'}, 'copysubtables': {'type': 'cBool'}, 'omitsubtables': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}}
        doc = {'outputTableName': outputTableName, 'tables': tables, 'subtables': subtables, 'nomodify': nomodify, 'lock': lock, 'copysubtables': copysubtables, 'omitsubtables': omitsubtables}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _createmultims_result = self._swigobj.createmultims(_str_ec(_pc.document['outputTableName']), [_str_ec(_x) for _x in _pc.document['tables']], [_str_ec(_x) for _x in _pc.document['subtables']], _pc.document['nomodify'], _pc.document['lock'], _pc.document['copysubtables'], [_str_ec(_x) for _x in _pc.document['omitsubtables']])
        return _createmultims_result

    def ismultims(self):
        """
        """
        _ismultims_result = self._swigobj.ismultims()
        return _ismultims_result

    def split(self, outputms='', field=[ ], spw=[ ], step=[ int(1) ], baseline=[ ], timebin=[ ], time='', scan=[ ], uvrange=[ ], taql='', whichcol='DATA', tileshape=[ ], subarray=[ ], combine='', correlation='', intent='', obs=''):
        """This function splits out part of the MS into a new MS. Time and
        channel averaging can be performed in the process (but not in
        the same call).
        
        When splitting multiple spectral windows, the parameters nchan,
        start, and step can be vectors, so that each spectral window has
        its own selection on averaging and number of output channels. But
        the option of using only one value for each of these parameters
        means that it will be replicated for all the spectral windows
        selected.
        
        """
        schema = {'outputms': {'type': 'cStr'}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'step': {'type': 'cIntVec', 'coerce': [_coerce.to_list,_coerce.to_intvec]}, 'baseline': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'timebin': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'time': {'type': 'cStr'}, 'scan': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'uvrange': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'taql': {'type': 'cStr'}, 'whichcol': {'type': 'cStr'}, 'tileshape': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'subarray': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'combine': {'type': 'cStr'}, 'correlation': {'type': 'cStr'}, 'intent': {'type': 'cStr'}, 'obs': {'type': 'cStr'}}
        doc = {'outputms': outputms, 'field': field, 'spw': spw, 'step': step, 'baseline': baseline, 'timebin': timebin, 'time': time, 'scan': scan, 'uvrange': uvrange, 'taql': taql, 'whichcol': whichcol, 'tileshape': tileshape, 'subarray': subarray, 'combine': combine, 'correlation': correlation, 'intent': intent, 'obs': obs}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _split_result = self._swigobj.split(_str_ec(_pc.document['outputms']), _any_ec(_pc.document['field']), _any_ec(_pc.document['spw']), _pc.document['step'], _any_ec(_pc.document['baseline']), _any_ec(_pc.document['timebin']), _str_ec(_pc.document['time']), _any_ec(_pc.document['scan']), _any_ec(_pc.document['uvrange']), _str_ec(_pc.document['taql']), _str_ec(_pc.document['whichcol']), _any_ec(_pc.document['tileshape']), _any_ec(_pc.document['subarray']), _str_ec(_pc.document['combine']), _str_ec(_pc.document['correlation']), _str_ec(_pc.document['intent']), _str_ec(_pc.document['obs']))
        return _split_result

    def partition(self, outputms='', field=[ ], spw=[ ], baseline=[ ], timebin=[ ], time='', scan=[ ], uvrange=[ ], taql='', whichcol='DATA', tileshape=[ ], subarray=[ ], combine='', intent='', obs=''):
        """This function splits out part of the MS into a new MS. Time
        averaging can be performed in the process.  Unlike split, the
        subtables and IDs (ANTENNA1, DATA_DESCRIPTION_ID, etc.) are never
        changed to account for the selection.
        
        As a side effect of that property, partition cannot select by
        channel or correlation, or average channels.  It CAN select by
        spectral window(s).
        
        """
        schema = {'outputms': {'type': 'cStr'}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'baseline': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'timebin': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'time': {'type': 'cStr'}, 'scan': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'uvrange': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'taql': {'type': 'cStr'}, 'whichcol': {'type': 'cStr'}, 'tileshape': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'subarray': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'combine': {'type': 'cStr'}, 'intent': {'type': 'cStr'}, 'obs': {'type': 'cStr'}}
        doc = {'outputms': outputms, 'field': field, 'spw': spw, 'baseline': baseline, 'timebin': timebin, 'time': time, 'scan': scan, 'uvrange': uvrange, 'taql': taql, 'whichcol': whichcol, 'tileshape': tileshape, 'subarray': subarray, 'combine': combine, 'intent': intent, 'obs': obs}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _partition_result = self._swigobj.partition(_str_ec(_pc.document['outputms']), _any_ec(_pc.document['field']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['baseline']), _any_ec(_pc.document['timebin']), _str_ec(_pc.document['time']), _any_ec(_pc.document['scan']), _any_ec(_pc.document['uvrange']), _str_ec(_pc.document['taql']), _str_ec(_pc.document['whichcol']), _any_ec(_pc.document['tileshape']), _any_ec(_pc.document['subarray']), _str_ec(_pc.document['combine']), _str_ec(_pc.document['intent']), _str_ec(_pc.document['obs']))
        return _partition_result

    def summary(self, verbose=False, listfile='', listunfl=False, cachesize=float(50), overwrite=False, wantreturn=True):
        """This method will print a summary of the measurement set to the
        system logger. The verbose argument provides some control on how
        much information is displayed.
        
        For especially large datasets, the cachesize parameter can be
        increased for possibly better performance.
        
        This method can also return, in the header argument, a record
        containing the following fields:
        1. nrow     Number of rows in the measurement set
        2. name     Name of the measurement set
        
        DESCRIPTION OF ALGORITHM TO CALCULATE THE NUMBER OF UNFLAGGED ROWS
        
        The number of unflagged rows will only be computed if listunflis
        True. The number of unflagged rows (the nUnflRows columns in the
        scans and fields portions of the listing) is calculated by summing
        the fractional unflagged bandwidth for each row (and hence why the
        number of unflagged rows, in general, is not an integer). Thus a
        row which has half of its total bandwidth flagged contributes 0.5
        rows to the unflagged row count. A row with 20 of 32 channels of
        homogeneous width contributes 20/32 = 0.625 rows to the unflagged
        row count. A row with a value of False in the FLAG_ROW column is
        not counted in the number of unflagged rows.
        
        """
        schema = {'verbose': {'type': 'cBool'}, 'listfile': {'type': 'cStr'}, 'listunfl': {'type': 'cBool'}, 'cachesize': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'overwrite': {'type': 'cBool'}, 'wantreturn': {'type': 'cBool'}}
        doc = {'verbose': verbose, 'listfile': listfile, 'listunfl': listunfl, 'cachesize': cachesize, 'overwrite': overwrite, 'wantreturn': wantreturn}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _summary_result = _dict_dc(self._swigobj.summary(_pc.document['verbose'], _str_ec(_pc.document['listfile']), _pc.document['listunfl'], _pc.document['cachesize'], _pc.document['overwrite'], _pc.document['wantreturn']))
        return _summary_result

    def getscansummary(self):
        """This function will return a summary of the main table as a
        structure
        
        """
        _getscansummary_result = _dict_dc(self._swigobj.getscansummary())
        return _getscansummary_result

    def getspectralwindowinfo(self):
        """This method will get a summary of the spectral window actually
        used in this ms. To be precise those reference by the data
        description table.
        
        """
        _getspectralwindowinfo_result = _dict_dc(self._swigobj.getspectralwindowinfo())
        return _getspectralwindowinfo_result

    def getreferencedtables(self):
        """
        """
        _getreferencedtables_result = [_str_dc(_x) for _x in self._swigobj.getreferencedtables()]
        return _getreferencedtables_result

    def getfielddirmeas(self, dircolname='PHASE_DIR', fieldid=int(0), time=float(0), format='measure'):
        """This function returns the direction measures from the given
        direction column of the MS FIELD table as a either a measure
        dictionary or sexigesimal string representation.
        If there is an ephemeris attached, this will give you the time dependent
        direction for the given direction column including the offset which each
        field may have to the ephemeris it is referencing. You can use the value
        "EPHEMERIS_DIR" for parameter "dircolname" to access the unaltered ephemeris
        direction without any potential mosaic offsets.
        
        """
        schema = {'dircolname': {'type': 'cStr'}, 'fieldid': {'type': 'cInt'}, 'time': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'format': {'type': 'cStr'}}
        doc = {'dircolname': dircolname, 'fieldid': fieldid, 'time': time, 'format': format}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _getfielddirmeas_result = _any_dc(self._swigobj.getfielddirmeas(_str_ec(_pc.document['dircolname']), _pc.document['fieldid'], _pc.document['time'], _str_ec(_pc.document['format'])))
        return _getfielddirmeas_result

    def listhistory(self):
        """This function lists the contents of the measurement set history
        table.
        
        """
        _listhistory_result = self._swigobj.listhistory()
        return _listhistory_result

    def writehistory(self, message='', parms='', origin='MSHistoryHandler::addMessage()', msname='', app='ms'):
        """This function adds a row to the history table of the specified
        measurement set containing any message that the user wishes to
        record.  By default the history entry is written to the history
        table of the measurement set that is currently open, the message
        origin is recorded as 'MSHistoryHandler::addMessage()', the
        originating application is 'ms' and the input parameters field is
        empty.
        
        """
        schema = {'message': {'type': 'cStr'}, 'parms': {'type': 'cStr'}, 'origin': {'type': 'cStr'}, 'msname': {'type': 'cStr'}, 'app': {'type': 'cStr'}}
        doc = {'message': message, 'parms': parms, 'origin': origin, 'msname': msname, 'app': app}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _writehistory_result = self._swigobj.writehistory(_str_ec(_pc.document['message']), _str_ec(_pc.document['parms']), _str_ec(_pc.document['origin']), _str_ec(_pc.document['msname']), _str_ec(_pc.document['app']))
        return _writehistory_result

    def statistics(self, column='', complex_value='', useflags=True, useweights=False, spw='', field='', baseline='', uvrange='', time='', correlation='', scan='', intent='', array='', obs='', reportingaxes='', timeaverage=False, timebin='0s', timespan='', maxuvwdistance=float(0.0)):
        """This function computes descriptive statistics on the measurement
        set. It returns the statistical values as a python dictionary.  The
        given column name must be a numerical column. If it is a complex
        valued column, the parameter complex_value defines which derived
        real value is used for the statistics computation.
        
        """
        schema = {'column': {'type': 'cStr'}, 'complex_value': {'type': 'cStr'}, 'useflags': {'type': 'cBool'}, 'useweights': {'type': 'cBool'}, 'spw': {'type': 'cStr'}, 'field': {'type': 'cStr'}, 'baseline': {'type': 'cStr'}, 'uvrange': {'type': 'cStr'}, 'time': {'type': 'cStr'}, 'correlation': {'type': 'cStr'}, 'scan': {'type': 'cStr'}, 'intent': {'type': 'cStr'}, 'array': {'type': 'cStr'}, 'obs': {'type': 'cStr'}, 'reportingaxes': {'type': 'cStr'}, 'timeaverage': {'type': 'cBool'}, 'timebin': {'type': 'cStr'}, 'timespan': {'type': 'cStr'}, 'maxuvwdistance': {'type': 'cFloat', 'coerce': _coerce.to_float}}
        doc = {'column': column, 'complex_value': complex_value, 'useflags': useflags, 'useweights': useweights, 'spw': spw, 'field': field, 'baseline': baseline, 'uvrange': uvrange, 'time': time, 'correlation': correlation, 'scan': scan, 'intent': intent, 'array': array, 'obs': obs, 'reportingaxes': reportingaxes, 'timeaverage': timeaverage, 'timebin': timebin, 'timespan': timespan, 'maxuvwdistance': maxuvwdistance}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _statistics_result = _dict_dc(self._swigobj.statistics(_str_ec(_pc.document['column']), _str_ec(_pc.document['complex_value']), _pc.document['useflags'], _pc.document['useweights'], _str_ec(_pc.document['spw']), _str_ec(_pc.document['field']), _str_ec(_pc.document['baseline']), _str_ec(_pc.document['uvrange']), _str_ec(_pc.document['time']), _str_ec(_pc.document['correlation']), _str_ec(_pc.document['scan']), _str_ec(_pc.document['intent']), _str_ec(_pc.document['array']), _str_ec(_pc.document['obs']), _str_ec(_pc.document['reportingaxes']), _pc.document['timeaverage'], _str_ec(_pc.document['timebin']), _str_ec(_pc.document['timespan']), _pc.document['maxuvwdistance']))
        return _statistics_result

    def statisticsold(self, column='', complex_value='', useflags=True, spw='', field='', baseline='', uvrange='', time='', correlation='', scan='', array='', obs=''):
        """DEPRECATED: Please use the ms::statistics() function in place of
        ms::statisticsold().
        
        This function computes descriptive statistics on the measurement
        set. It returns the statistical values as a python dictionary.  The
        given column name must be a numerical column. If it is a complex
        valued column, the parameter complex_value defines which derived
        real value is used for the statistics computation.
        
        """
        schema = {'column': {'type': 'cStr'}, 'complex_value': {'type': 'cStr'}, 'useflags': {'type': 'cBool'}, 'spw': {'type': 'cStr'}, 'field': {'type': 'cStr'}, 'baseline': {'type': 'cStr'}, 'uvrange': {'type': 'cStr'}, 'time': {'type': 'cStr'}, 'correlation': {'type': 'cStr'}, 'scan': {'type': 'cStr'}, 'array': {'type': 'cStr'}, 'obs': {'type': 'cStr'}}
        doc = {'column': column, 'complex_value': complex_value, 'useflags': useflags, 'spw': spw, 'field': field, 'baseline': baseline, 'uvrange': uvrange, 'time': time, 'correlation': correlation, 'scan': scan, 'array': array, 'obs': obs}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _statisticsold_result = _dict_dc(self._swigobj.statisticsold(_str_ec(_pc.document['column']), _str_ec(_pc.document['complex_value']), _pc.document['useflags'], _str_ec(_pc.document['spw']), _str_ec(_pc.document['field']), _str_ec(_pc.document['baseline']), _str_ec(_pc.document['uvrange']), _str_ec(_pc.document['time']), _str_ec(_pc.document['correlation']), _str_ec(_pc.document['scan']), _str_ec(_pc.document['array']), _str_ec(_pc.document['obs'])))
        return _statisticsold_result

    def range(self, items=[  ], useflags=True, blocksize=int(10)):
        """This function returns the range of values in the currently
        selected measurement set for the items specified.
        
        Possible items include 'amplitude', 'corrected_amplitude',
        'model_amplitude', 'antenna1', 'antenna2', 'antennas',
        'array_id', 'chan_freq', 'corr_names', 'corr_types', 'feed1',
        'feed2', 'field_id', 'fields', 'float_data', 'ifr_number'
        (1000*antenna1 + antenna2), 'imaginary', 'corrected_imaginary',
        'model_imaginary', 'num_corr', 'num_chan', 'phase',
        'corrected_phase', 'model_phase', 'phase_dir', 'real',
        'corrected_real', 'model_real', 'ref_frequency', 'rows',
        'scan_number', 'sigma', 'data_desc_id', 'time', 'times', 'u',
        'v', 'w', 'uvdist', and 'weight'.  Note that corrected, model,
        and float versions are available only if these columns are
        present in the data.
        
        You specify items in which you are interested using a string
        vector where each element is a case insensitive item name.  This
        function will then return a record that has fields corresponding
        to each of the specified items. Each field will contain the
        range of the specified item. For many items the range will be
        the minimum and maximum values but for some it will be a list of
        unique values. Unrecognized items are ignored.
        
        By default the FLAG column is used to exclude flagged data
        before any ranges are determined, but you can set useflags=False
        to include flagged data in the range.  However, if you average
        in frequency, flagging will still be applied.
        
        You can influence the memory use and the reading speed using
        the blocksize argument - it specifies how big a block of data
        to read at once (in MB). For large datasets on machines with
        lots of memory you may speed things up by setting this higher
        than the default (10 MB).
        
        For some items, you need to call selectinit to select a portion
        of the data with a unique shape prior to calling this function.
        
        Items prefixed with corrected, model, residual or obs_residual
        are not available unless your measurement set has been processed
        either with the imager or calibrator tools.
        
        """
        schema = {'items': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'useflags': {'type': 'cBool'}, 'blocksize': {'type': 'cInt'}}
        doc = {'items': items, 'useflags': useflags, 'blocksize': blocksize}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _range_result = _dict_dc(self._swigobj.range([_str_ec(_x) for _x in _pc.document['items']], _pc.document['useflags'], _pc.document['blocksize']))
        return _range_result

    def lister(self, options='', datacolumn='data', field='', spw='', antenna='', timerange='', correlation='', scan='', feed='', array='', observation='', uvrange='', average='', showflags=False, msselect='', pagerows=int(50), listfile=''):
        """This tool lists measurement set visibility data under a number of
        input selection conditions.  The measurement set data columns that
        can be listed are: the raw data, corrected data, model data, and
        residual (corrected - model) data.
        
        The output table format is dynamic.  Field, Spectral Window, and
        Channel columns are not displayed if the column contents are
        uniform. For example, if ``spw = `1' '' is specified, the spw
        column will not be displayed.  When a column is not displayed, a
        message is sent to the logger and terminal indicating that the
        column values are uniform and listing the uniform value.
        
        Table column descriptions:
        
        Date/Time               Average date and time of data sample interval
        Intrf                   Interferometer baseline (antenna names)
        UVDist                  uv-distance (units of wavelength)
        Fld                     Field ID
        SpW                     Spectral Window ID
        Chn                     Channel number
        Correlated polarization Correlated polarizations (eg: RR, LL, XY)
        
        Sub-columns:
        
        Amp                     Visibility amplitude
        Phs                     Visibility phase
        Wt                      Weight of visibility measurement
        F                       Flag: `F' = flagged datum; ` ' = unflagged
        
        """
        schema = {'options': {'type': 'cStr'}, 'datacolumn': {'type': 'cStr'}, 'field': {'type': 'cStr'}, 'spw': {'type': 'cStr'}, 'antenna': {'type': 'cStr'}, 'timerange': {'type': 'cStr'}, 'correlation': {'type': 'cStr'}, 'scan': {'type': 'cStr'}, 'feed': {'type': 'cStr'}, 'array': {'type': 'cStr'}, 'observation': {'type': 'cStr'}, 'uvrange': {'type': 'cStr'}, 'average': {'type': 'cStr'}, 'showflags': {'type': 'cBool'}, 'msselect': {'type': 'cStr'}, 'pagerows': {'type': 'cInt'}, 'listfile': {'type': 'cStr'}}
        doc = {'options': options, 'datacolumn': datacolumn, 'field': field, 'spw': spw, 'antenna': antenna, 'timerange': timerange, 'correlation': correlation, 'scan': scan, 'feed': feed, 'array': array, 'observation': observation, 'uvrange': uvrange, 'average': average, 'showflags': showflags, 'msselect': msselect, 'pagerows': pagerows, 'listfile': listfile}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _lister_result = self._swigobj.lister(_str_ec(_pc.document['options']), _str_ec(_pc.document['datacolumn']), _str_ec(_pc.document['field']), _str_ec(_pc.document['spw']), _str_ec(_pc.document['antenna']), _str_ec(_pc.document['timerange']), _str_ec(_pc.document['correlation']), _str_ec(_pc.document['scan']), _str_ec(_pc.document['feed']), _str_ec(_pc.document['array']), _str_ec(_pc.document['observation']), _str_ec(_pc.document['uvrange']), _str_ec(_pc.document['average']), _pc.document['showflags'], _str_ec(_pc.document['msselect']), _pc.document['pagerows'], _str_ec(_pc.document['listfile']))
        return _lister_result

    def metadata(self, cachesize=float(50)):
        """Get the MS metadata associated with this MS.
        """
        schema = {'cachesize': {'type': 'cFloat', 'coerce': _coerce.to_float}}
        doc = {'cachesize': cachesize}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _metadata_result = _wrap_msmetadata(swig_object=self._swigobj.metadata(_pc.document['cachesize']))
        return _metadata_result

    def msselect(self, items={ }, onlyparse=False):
        """A return value of True implies that the combination of all
        selection expressions resulted in a non-Null combined TaQL
        expression. False implies that the combined TaQL could not be
        formed (i.e. it is Null, and the "selected MS" will be the same as
        the input MS).
        
        The details of selection expressions are desribed in the
        MSSelection Memo.
        
        Note that this function can be called multiple times but the
        result is cumulative.  Each selection will work on the data
        already selected from all previous calls of this function.  Use
        the function reset() to reset all selections to NULL (original
        dataset).
        
        """
        schema = {'items': {'type': 'cDict'}, 'onlyparse': {'type': 'cBool'}}
        doc = {'items': items, 'onlyparse': onlyparse}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _msselect_result = self._swigobj.msselect(_dict_ec(_pc.document['items']), _pc.document['onlyparse'])
        return _msselect_result

    def msselectedindices(self):
        """The return indices are the result of parsing the MSSelection
        expressions provided in the msselect function.
        
        """
        _msselectedindices_result = _dict_dc(self._swigobj.msselectedindices())
        return _msselectedindices_result

    def msseltoindex(self, vis='', spw=[ ], field=[ ], baseline=[ ], time=[ ], scan=[ ], uvrange=[ ], observation=[ ], polarization=[ ], taql=''):
        """Utility function that will return the ids of the selection used.
        
        """
        schema = {'vis': {'type': 'cStr'}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'baseline': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'time': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'scan': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'uvrange': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'observation': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'polarization': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'taql': {'type': 'cStr'}}
        doc = {'vis': vis, 'spw': spw, 'field': field, 'baseline': baseline, 'time': time, 'scan': scan, 'uvrange': uvrange, 'observation': observation, 'polarization': polarization, 'taql': taql}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _msseltoindex_result = _dict_dc(self._swigobj.msseltoindex(_str_ec(_pc.document['vis']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['field']), _any_ec(_pc.document['baseline']), _any_ec(_pc.document['time']), _any_ec(_pc.document['scan']), _any_ec(_pc.document['uvrange']), _any_ec(_pc.document['observation']), _any_ec(_pc.document['polarization']), _str_ec(_pc.document['taql'])))
        return _msseltoindex_result

    def selectinit(self, datadescid=int(0), reset=False):
        """A measurement set can contain data with a variety of different
        shapes (as described in the overall description to this tool).  To
        allow functions to return data in fixed shape arrays you need to
        select, using this function, rows that contain the same data shape.
        You do not need to use this function if all the data in your
        measurement set has only one shape.
        
        The DATA_DESC_ID column in the measurement set contains a value
        that maps to a particular row in the POLARIZATION and
        SPECTRAL_WINDOW subtables. Hence all rows with the same value in
        the DATA_DESC_ID column must have the same data shape. To select
        all the data where the DATA_DESC_ID value is N you call this
        function with the datadescid argument set to N.
        
        It is possible to have a measurement set with differing values in
        the DATA_DESC_ID column but where all the data is a fixed shape.
        For example this will occur if the reference frequency changes but
        the number of spectral channels is fixed. In cases like this all
        the data can be selected and this function does not need to be
        used.
        
        To return to the completely unselected measurement set, set the
        reset argument to True. This will allow you to access the full
        range of rows in the measurement set, rather than just the
        selected measurement set.
        
        The datadescid must always be a non-negative integer.
        
        """
        schema = {'datadescid': {'type': 'cInt'}, 'reset': {'type': 'cBool'}}
        doc = {'datadescid': datadescid, 'reset': reset}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectinit_result = self._swigobj.selectinit(_pc.document['datadescid'], _pc.document['reset'])
        return _selectinit_result

    def select(self, items={ }):
        """This function will select a subset of the current measurement set
        based on the range of values for each field in the input record.
        The range function will return a record that can be altered and
        used as the argument for this function.  A successful selection
        returns True. Unrecognized fields are ignored.
        
        Allowable items for select include: 'antenna1', 'antenna2',
        'array_id', 'feed1', 'feed2', 'field_id', 'ifr_number', 'rows',
        'scan_number', 'data_desc_id', 'time', 'times', 'u', 'v', 'w',
        and 'uvdist'.
        
        You need to call selectinit before
        calling this function. If you haven't then selectinit will be
        called for you with default arguments.
        
        Repeated use of this function, with different arguments, will
        further refine the selection, resulting in a successively smaller
        selected measurement set. If the selected measurement set does not
        contain any rows then this function will return False and send a
        warning message in the logger. Otherwise this function will return
        True. To undo all the selections you need to use the selectinit
        function (with reset=True).
        
        """
        schema = {'items': {'type': 'cDict'}}
        doc = {'items': items}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _select_result = self._swigobj.select(_dict_ec(_pc.document['items']))
        return _select_result

    def selecttaql(self, msselect=''):
        """This function will select a subset of the current measurement set
        based on the standard TaQL selection string given.
        
        Repeated use of this function, with different arguments, will
        further refine the selection, resulting in a successively smaller
        selected measurement set. If the selected measurement set does not
        contain any rows then this function will return False and send a
        warning message in the logger. Otherwise this function will return
        True. To undo all the selections you need to use the selectinit
        function (with reset=True).  Note that index values used in the
        TaQL string are zero-based as are all tool indices.
        
        """
        schema = {'msselect': {'type': 'cStr'}}
        doc = {'msselect': msselect}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selecttaql_result = self._swigobj.selecttaql(_str_ec(_pc.document['msselect']))
        return _selecttaql_result

    def selectchannel(self, nchan=int(1), start=int(0), width=int(1), inc=int(1)):
        """This function allows you to select a subset of the frequency
        channels in the current measurement set.  This function can also
        average, over frequency channels, prior to providing the values to
        the user.
        
        Selection on channels is not allowed using either the select or
        command functions, as they can only select entire rows in a
        measurement set. Channel selection involves accessing only some of
        the values in a row. Like all the selection functions, this
        function does not change the current measurement but updates the
        measurement set selection parameters so that functions like
        getdata will return the desired subset of the data.  Repeated use
        of this function will overwrite any previous channel selection.
        
        There are four parameters, the number of output channels, the
        first input channel to use, the number of input channels to
        average into one output channel, and the increment in the input
        spectrum for the next output channel. All four parameters need to
        be specified.
        
        When all data to be averaged is unflagged, the result is the
        averaged value and the corresponding flag is False.  When all data
        is flagged, the result is set to zero and the corresponding flag is
        True.  When data to be averaged is mixed (unflagged and flagged),
        only the unflagged values are averaged and the flag is set to
        False.
        
        This function return True if the selection was successful, and
        False if not. In the latter case an error message will also be sent
        to the logger.
        
        You need to call selectinit before calling this function.
        If you haven't then selectinit will be called for you with default
        arguments.
        
        """
        schema = {'nchan': {'type': 'cInt'}, 'start': {'type': 'cInt'}, 'width': {'type': 'cInt'}, 'inc': {'type': 'cInt'}}
        doc = {'nchan': nchan, 'start': start, 'width': width, 'inc': inc}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectchannel_result = self._swigobj.selectchannel(_pc.document['nchan'], _pc.document['start'], _pc.document['width'], _pc.document['inc'])
        return _selectchannel_result

    def selectpolarization(self, wantedpol=[  ]):
        """This function allows you to select a subset of the polarizations
        in the current measurement set.  This function can also setup
        conversion to different polarization representations.
        
        You specify the polarizations using a string vector. Allowable
        strings are include I, Q, U, V, RR, RL, LR, LL, XX, YY, XY,
        YX. These string must be specified in upper case. If the
        polarizations match those present in the measurement set they will
        be selected directly, otherwise all polarizations are read and
        then a conversion step is done. If the conversion cannot be done
        then an error will be produced when you try to access the data.
        
        This function return True if the selection was successful, and
        False if not.
        
        You need to call selectinit before calling this function.
        If you haven't then selectinit will be called for you with default
        arguments.
        
        """
        schema = {'wantedpol': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}}
        doc = {'wantedpol': wantedpol}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectpolarization_result = self._swigobj.selectpolarization([_str_ec(_x) for _x in _pc.document['wantedpol']])
        return _selectpolarization_result

    def statwt2(self, combine='', timebin=int(1), slidetimebin=False, chanbin='spw', minsamp=int(2), statalg='classic', fence=float(-1), center='mean', lside=True, zscore=float(-1), maxiter=int(-1), excludechans='', wtrange=[  ], preview=False, datacolumn='corrected'):
        """IF NOT RUN IN PREVIEW MODE, THIS APPLICATION WILL MODIFY THE
        WEIGHT, WEIGHT SPECTRUM, FLAG, AND FLAG_ROW COLUMNS OF THE INPUT
        MS. IF YOU WANT A PRISTINE COPY OF THE INPUT MS TO BE PRESERVED,
        YOU SHOULD MAKE A COPY OF IT BEFORE RUNNING THIS APPLICATION.
        
        This application computes weights for the WEIGHT and
        WEIGHT_SPECTRUM (if present) columns based on the variance of
        values in the CORRECTED_DATA or DATA column. If the MS does not
        have the specified data column, the application will fail. The
        following algorithm is used:
        
        1. For unflagged data in each sample, create two sets of values,
        one set is composed solely of the real part of the data values,
        the other set is composed solely of the imaginary part of the
        data values.
        2. Compute the variance of each of these sets, vr and vi.
        3. Compute veq = (vr + vi)/2.
        4. The associated weight is just the reciprocal of veq. The weight
        will have unit of (data unit)^(-2), eg Jy^(-2).
        
        Data are aggregated on a per-baseline, per-data description ID
        basis. Data are aggregated in bins determined by the specified
        values of the timebin and chanbin parameters. By default, data for
        separate correlations are aggregated separately. This behavior can
        be overridden by specifying combine="corr" (see below).
        
        RULES REGARDING CREATING/INITIALIZING WEIGHT_SPECTRUM COLUMN
        
        1. If run in preview mode (preview=True), no data are modified and
        no columns are added.
        2. Else if the MS already has a WEIGHT_SPECTRUM and this column has
        been initialized (has values), it will always be populated with
        the new weights.  The WEIGHT column will be populated with the
        corresponding median values of the associated WEIGHT_SPECTRUM
        array.
        3. Else if the frequency range specified for the sample is not the
        default ("spw"), the WEIGHT_SPECTRUM column will be created (if
        it doesn't already exist) and the new weights will be written to
        it.  The WEIGHT column should be populated with the
        corresponding median values of the WEIGHT_SPECTRUM array.
        4. Otherwise the single value for each spectral window will be
        written to the WEIGHT column; the WEIGHT_SPECTRUM column will
        not be added if it doesn't already exist, and if it does, it
        will remain uninitialized (no values will be written to it).
        
        TIME BINNING
        
        One of two algorithms can be used for time binning. If
        slidetimebin=True, then a sliding time bin of the specified width
        is used. If slidetimebin=False, then block time processing is used.
        The sliding time bin algorithm will generally be both more memory
        intensive and take longer than the block processing algorithm. Each
        algorithm is discussed in detail below.
        
        If the value of timebin is an integer, it means that the specified
        value should be multiplied by the representative integration time
        in the MS. This integration is the median value of all the values
        in the INTERVAL column. Flags are not considered in the integration
        time computation. If either extrema in the INTERVAL column differs
        from the median by more than 25%, the application will fail because
        the values vary too much for there to be a single, representative,
        integration time. The timebin parameter can also be specified as a
        quantity (string) that must have time conformant units.
        
        Block Time Processing
        
        The data are processed in blocks. This means that all weight
        spectrum values will be set to the same value for all points within
        the same time bin/channel bin/correlation bin (see the section on
        channel binning and description of combine="corr" for more details
        on channel binning and correlation binning). The time bins are not
        necessarily contiguous and are not necessarily the same width. The
        start of a bin is always coincident with a value from the TIME
        column, So for example, if values from the time column are [20, 60,
        100, 140, 180, 230], and the width of the bins is chosen to be
        110s, the first bin would start at 20s and run to 130s, so that
        data from timestamps 20, 60, and 100 will be included in the first
        bin. The second bin would start at 140s, so that data for
        timestamps 140, 180, and 230 would be included in the second bin.
        Also, time binning does not span scan boundaries, so that data
        associated with different scan numbers will always be binned
        separately; changes in SCAN_NUMBER will cause a new time bin to be
        created, with its starting value coincident with the time of the
        new SCAN_NUMBER. Similar behavior can be expected for changes in
        FIELD_ID and ARRAY_ID. One can override this behavior for some
        columns by specifying the combine parameter (see below).
        
        Sliding Time Window Processing
        
        In this case, the time window is always centered on the timestamp
        of the row in question and extends +/-timebin/2 around that
        timestamp, subject the the time block boundaries. Rows with the
        same baselines and data description IDs which are included in that
        window are used for determining the weight of that row. The
        boundaries of the time block to which the window is restricted are
        determined by changes in FIELD_ID, ARRAY_ID, and SCAN_NUMBER. One
        can override this behavior for FIELD_ID and/or SCAN_NUMBER by
        specifying the combine parameter (see below). Unlike the time block
        processing algorithm, this sliding time window algorithm requires
        that details all rows for the time block in question are kept in
        memory, and thus the sliding window algorithm in general requires
        more memory than the  block processing method. Also, unlike the
        block processing method which computes a single value for all
        weights within a single bin, the sliding window method requires
        that each row (along with each channel and correlation bin) be
        processed individually, so in general the sliding window method
        will take longer than the block processing method.
        
        CHANNEL BINNING
        
        The width of channel bins is specified via the chanbin parameter.
        Channel binning occurs within individual spectral windows; bins
        never span multiple spectral windows. Each channel will be included
        in exactly one bin.
        
        The default value "spw" indicates that all channels in each
        spectral window are to be included in a single bin.
        
        Any other string value is interpreted as a quantity, and so should
        have frequency units, eg "1MHz". In this case, the channel
        frequencies from the CHAN_FREQ column of the SPECTRAL_WINDOW
        subtable of the MS are used to determine the bins. The first bin
        starts at the channel frequency of the 0th channel in the spectral
        window. Channels with frequencies that differ by less than the
        value specified by the chanbin parameter are included in this bin.
        The next bin starts at the frequency of the first channel outside
        the first bin, and the process is repeated until all channels have
        been binned.
        
        If specified as an integer, the value is interpreted as the number
        of channels to include in each bin. The final bin in the spectral
        window may not necessarily contain this number of channels. For
        example, if a spectral window has 15 channels, and chanbin is
        specified to be 6, then channels 0-5 will comprise the first bin,
        channels 6-11 the second, and channels 12-14 the third, so that
        only three channels will comprise the final bin.
        
        MINIMUM REQUIRED NUMBER OF VISIBILITIES
        
        The minsamp parameter allows the user to specify the minimum number
        of unflagged visibilities that must be present in a sample for that
        sample's weight to be computed. If a sample has less than this
        number of unflagged points, the associated weights of all the
        points in the sample are set to zero, and all the points in the
        sample are flagged.
        
        AGGREGATING DATA ACROSS BOUNDARIES
        
        By default, data are not aggregated across changes in values in the
        columns ARRAY_ID, SCAN_NUMBER, STATE_ID, FIELD_ID, and
        DATA_DESC_ID. One can override this behavior for SCAN_NUMBER,
        STATE_ID, and FIELD_ID by specifying the combine parameter. For
        example, specifying combine="scan" will ignore scan boundaries when
        aggregating data. Specifying combine="field, scan" will ignore both
        scan and field boundaries when aggregating data.
        
        Also by default, data for separate correlations are aggregated
        separately. Data for all correlations within each spectral window
        can be aggregated together by specifying "corr" in the combine
        parameter.
        
        Any combination and permutation of "scan", "field", "state", and
        "corr" are supported by the combine parameter. Other values will be
        silently ignored.
        
        STATISTICS ALGORITHMS
        
        The supported statistics algorithms are described in detail in the
        imstat and ia.statistics() help. For the current application, these
        algorithms are used to compute vr and vi (see above), such that the
        set of the real parts of the visibilities and the set of the
        imaginary parts of the visibilities are treated as independent data
        sets.
        
        RANGE OF ACCEPTABLE WEIGHTS
        
        The wtrange parameter allows one to specify the acceptable range
        (inclusive, except for zero) for weights. Data with weights
        computed to be outside this range will be flagged. If not specified
        (empty array), all weights are considered to be acceptable. If
        specified, the array must contain exactly two nonnegative numeric
        values. Note that data with weights of zero are always flagged.
        
        EXCLUDING CHANNELS
        
        Channels can be excluded from the computation of the weights by
        specifying the excludechans parameter. This parameter accepts a
        valid MS channel selection string. Data associated with the
        selected channels will not be used in computing the weights.
        
        PREVIEW MODE
        
        By setting preview=True, the application is run in "preview" mode.
        In this mode, no data in the input MS are changed, although the
        amount of data that the application would have flagged is reported.
        
        DATA COLUMN
        
        The datacolumn parameter can be specified to indicate which data
        column should be used for computing the weights. The values
        "corrected" for the CORRECTED_DATA column and "data" for the DATA
        column are supported (minimum match, case insensitive). One may
        specify 'residual' in which case the values used are the result of
        the CORRECTED_DATA column - model, or 'residual_data' in which
        case the values used are the DATA column - model, where model
        is the CORRECTED_DATA column if it exists, or if it doesn't,
        the virtual source model if one exists, or if that doesn't, then
        no model is used and the 'residual' and 'residual_data' cases
        are equivalent to the 'corrected' and 'data' cases, respectively.
        The last two options are to allow for operation on timescales or
        frequency ranges which are larger than that over which the sky
        signal is expected to be constant. This situation arises in eg,
        OTF mapping, and also perhaps with sources with significant
        spectral structure. In cases where a necessary column doesn't
        exist, an exception will be thrown and no data will be changed.
        NOTE: It is the user's responsibility to ensure that a model has
        been set for all selected fields before using datacolumn='residual'
        or 'residual_data'.
        
        RETURN VALUE
        
        In all cases, the mean and variance of the set of all weights computed
        by the application is reported and returned in a dictionary with keys
        'mean' and 'variance'. Weights for which there are corresponding flags
        (=True) prior to running the application are excluded from the
        computation of these statistics. If the WEIGHT_SPECTRUM values are
        available, they are used to compute the statistics, otherwise,
        the WEIGHT values are used. The returned statistics are always computed
        using the classic algorithm; the value of statalg has no impact on how
        they are computed.
        
        OTHER CONSIDERATIONS
        
        Flagged values are not used in computing the weights, although the
        associated weights of these values are updated.
        
        If the variance for a set of data is 0, all associated flags for
        that data are set to True, and the corresponding weights are set to
        0.
        
        Because data are modified in the input MS, the nomodify parameter
        must be set to False when opening the associated MS tool.
        
        """
        schema = {'combine': {'type': 'cStr'}, 'timebin': {'anyof': [{'type': 'cStr'}, {'type': 'cInt'}]}, 'slidetimebin': {'type': 'cBool'}, 'chanbin': {'anyof': [{'type': 'cStr'}, {'type': 'cInt'}]}, 'minsamp': {'type': 'cInt'}, 'statalg': {'type': 'cStr'}, 'fence': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'center': {'type': 'cStr'}, 'lside': {'type': 'cBool'}, 'zscore': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'maxiter': {'type': 'cInt'}, 'excludechans': {'type': 'cStr'}, 'wtrange': {'type': 'cFloatVec', 'coerce': [_coerce.to_list,_coerce.to_floatvec]}, 'preview': {'type': 'cBool'}, 'datacolumn': {'type': 'cStr'}}
        doc = {'combine': combine, 'timebin': timebin, 'slidetimebin': slidetimebin, 'chanbin': chanbin, 'minsamp': minsamp, 'statalg': statalg, 'fence': fence, 'center': center, 'lside': lside, 'zscore': zscore, 'maxiter': maxiter, 'excludechans': excludechans, 'wtrange': wtrange, 'preview': preview, 'datacolumn': datacolumn}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _statwt2_result = _dict_dc(self._swigobj.statwt2(_str_ec(_pc.document['combine']), _any_ec(_pc.document['timebin']), _pc.document['slidetimebin'], _any_ec(_pc.document['chanbin']), _pc.document['minsamp'], _str_ec(_pc.document['statalg']), _pc.document['fence'], _str_ec(_pc.document['center']), _pc.document['lside'], _pc.document['zscore'], _pc.document['maxiter'], _str_ec(_pc.document['excludechans']), _pc.document['wtrange'], _pc.document['preview'], _str_ec(_pc.document['datacolumn'])))
        return _statwt2_result

    def statwt(self, dorms=False, byantenna=True, sepacs=True, fitspw=[ ], fitcorr=[ ], combine='', timebin=[ ], minsamp=int(3), field=[ ], spw=[ ], antenna='', timerange='', scan=[ ], intent='', array=[ ], correlation='', obs='', datacolumn='corrected_data'):
        """NOT IMPLEMENTED YET.
        
        This function estimates the noise from the scatter of the
        visibilities, sets SIGMA to it, and WEIGHT to SIGMA**-2.
        
        Ideally the visibilities used to estimate the scatter, as selected
        by fitspw and fitcorr, should be pure noise.  If you know for
        certain that they are, then setting dorms to True will give the
        best result.  Otherwise, use False (standard sample standard
        deviation).  More robust scatter estimates like the interquartile
        range or median absolute deviation from the median are not offered
        because they require sorting by value, which is not possible for
        complex numbers.
        
        To beat down the noise of the noise estimate, the sample size per
        estimate can be made larger than a single spw and baseline.  (Using
        combine='spw' is to interpolate between spws with line-free
        channels is recommended when an spw has no line-free channels.)
        timebin smooths the noise estimate over time.  windowtype sets the
        type of time smoothing.
        
        WEIGHT and SIGMA will not be changed for samples that have fewer
        than minsamp visibilities.  Selected visibilities for which no
        noise estimate is made will be flagged.  Note that minsamp is
        effectively at least 2 if dorms is False, and 1 if it is True.
        
        """
        schema = {'dorms': {'type': 'cBool'}, 'byantenna': {'type': 'cBool'}, 'sepacs': {'type': 'cBool'}, 'fitspw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'fitcorr': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'combine': {'type': 'cStr'}, 'timebin': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'minsamp': {'type': 'cInt'}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'antenna': {'anyof': [{'type': 'cStr'}, {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, {'type': 'cInt'}, {'type': 'cIntVec', 'coerce': [_coerce.to_list,_coerce.to_intvec]}]}, 'timerange': {'type': 'cStr'}, 'scan': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'intent': {'type': 'cStr'}, 'array': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'correlation': {'type': 'cStr'}, 'obs': {'type': 'cStr'}, 'datacolumn': {'type': 'cStr'}}
        doc = {'dorms': dorms, 'byantenna': byantenna, 'sepacs': sepacs, 'fitspw': fitspw, 'fitcorr': fitcorr, 'combine': combine, 'timebin': timebin, 'minsamp': minsamp, 'field': field, 'spw': spw, 'antenna': antenna, 'timerange': timerange, 'scan': scan, 'intent': intent, 'array': array, 'correlation': correlation, 'obs': obs, 'datacolumn': datacolumn}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _statwt_result = self._swigobj.statwt(_pc.document['dorms'], _pc.document['byantenna'], _pc.document['sepacs'], _any_ec(_pc.document['fitspw']), _any_ec(_pc.document['fitcorr']), _str_ec(_pc.document['combine']), _any_ec(_pc.document['timebin']), _pc.document['minsamp'], _any_ec(_pc.document['field']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['antenna']), _str_ec(_pc.document['timerange']), _any_ec(_pc.document['scan']), _str_ec(_pc.document['intent']), _any_ec(_pc.document['array']), _str_ec(_pc.document['correlation']), _str_ec(_pc.document['obs']), _str_ec(_pc.document['datacolumn']))
        return _statwt_result

    def regridspw(self, outframe='LSRK', mode='chan', restfreq=float(-3E30), interpolation='LINEAR', start=float(-3E30), center=float(-3E30), bandwidth=float(-1.), chanwidth=float(-1.), hanning=True):
        """This function permits you to transform the spectral data of your
        measurement set to a given reference frame. The present reference
        frame information in the MS is examined and the transformation
        performed accordingly. Since all such transformations are linear in
        frequency, a pure change of reference frame only affects the
        channel boundary definitions.
        
        In addition, the function permits you to permanently regrid the
        data, i.e. reduce the channel number and/or move the boundaries
        using several interpolation methods (selected using parameter
        "interpolation"). The new channels are equidistant in frequency (if
        parameter "mode" is chosen to be vrad or freq, or equidistant in
        wavelength if  parameter "mode" is chosen to be vopt or wave). If
        "mode" is chosen to be "chan", the regridding is performed by
        combining the existing channels, i.e. not moving but just
        eliminating channel boundaries where necessary.
        
        The regridding is applied to the channel definition and all data of
        the MS, i.e. all columns which contain arrays whose dimensions
        depend on the number of channels. The input parameters are verified
        before any modification is made to the MS.
        
        The target reference frame can be set by providing the name of a
        standard reference frame (LSRK, LSRD, BARY, GALACTO, LGROUP, CMB,
        TOPO, GEO, or SOURCE, default = no change of frame) in parameter
        "outframe". For each field in the MS, the channel frequencies are
        transformed from their present reference frame to the one given by
        parameter "outframe".
        
        If the regridding parameters are set, they are interpreted in the
        "outframe" reference frame. The regridding is applied to the data
        after the reference frame transformation.
        
        """
        schema = {'outframe': {'type': 'cStr'}, 'mode': {'type': 'cStr'}, 'restfreq': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'interpolation': {'type': 'cStr'}, 'start': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'center': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'bandwidth': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'chanwidth': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'hanning': {'type': 'cBool'}}
        doc = {'outframe': outframe, 'mode': mode, 'restfreq': restfreq, 'interpolation': interpolation, 'start': start, 'center': center, 'bandwidth': bandwidth, 'chanwidth': chanwidth, 'hanning': hanning}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _regridspw_result = self._swigobj.regridspw(_str_ec(_pc.document['outframe']), _str_ec(_pc.document['mode']), _pc.document['restfreq'], _str_ec(_pc.document['interpolation']), _pc.document['start'], _pc.document['center'], _pc.document['bandwidth'], _pc.document['chanwidth'], _pc.document['hanning'])
        return _regridspw_result

    def cvel(self, mode='channel', nchan=int(-1), start=[ ], width=[ ], interp='linear', phasec=[ ], restfreq=[ ], outframe='', veltype='radio', hanning=True):
        """This function permits you to transform the spectral data of your
        measurement set to a given reference frame and/or regrid it. It
        will combine all spectral windows of the MS into one.
        
        """
        schema = {'mode': {'type': 'cStr'}, 'nchan': {'type': 'cInt'}, 'start': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'width': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'interp': {'type': 'cStr'}, 'phasec': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'restfreq': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'outframe': {'type': 'cStr'}, 'veltype': {'type': 'cStr'}, 'hanning': {'type': 'cBool'}}
        doc = {'mode': mode, 'nchan': nchan, 'start': start, 'width': width, 'interp': interp, 'phasec': phasec, 'restfreq': restfreq, 'outframe': outframe, 'veltype': veltype, 'hanning': hanning}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _cvel_result = self._swigobj.cvel(_str_ec(_pc.document['mode']), _pc.document['nchan'], _any_ec(_pc.document['start']), _any_ec(_pc.document['width']), _str_ec(_pc.document['interp']), _any_ec(_pc.document['phasec']), _any_ec(_pc.document['restfreq']), _str_ec(_pc.document['outframe']), _str_ec(_pc.document['veltype']), _pc.document['hanning'])
        return _cvel_result

    def hanningsmooth(self, datacolumn='corrected'):
        """This function Hanning smooths the frequency channels with a
        weighted running average of:
        smoothedData[i] = 0.25*correctedData[i-1] + 0.50*correctedData[i]
        + 0.25*correctedData[i-1]
        The first and last channels are flagged. Inclusion of a flagged
        value in an average causes that averaged data value to be flagged.
        
        """
        schema = {'datacolumn': {'type': 'cStr'}}
        doc = {'datacolumn': datacolumn}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _hanningsmooth_result = self._swigobj.hanningsmooth(_str_ec(_pc.document['datacolumn']))
        return _hanningsmooth_result

    def cvelfreqs(self, spwids=[ int(0) ], fieldids=[ int(0) ], obstime='', mode='channel', nchan=int(-1), start=[ ], width=[ ], phasec=[ ], restfreq=[ ], outframe='', veltype='radio', verbose=True):
        """Take the spectral grid of a given spectral window, tranform and
        regrid it as prescribed by the given grid parameters (same as in
        cvel and clean) and return the transformed values as a list. The MS
        is not modified. Useful for tests of gridding parameters before
        using them in cvel or clean.
        
        """
        schema = {'spwids': {'type': 'cIntVec', 'coerce': [_coerce.to_list,_coerce.to_intvec]}, 'fieldids': {'type': 'cIntVec', 'coerce': [_coerce.to_list,_coerce.to_intvec]}, 'obstime': {'type': 'cStr'}, 'mode': {'type': 'cStr'}, 'nchan': {'type': 'cInt'}, 'start': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'width': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'phasec': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'restfreq': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'outframe': {'type': 'cStr'}, 'veltype': {'type': 'cStr'}, 'verbose': {'type': 'cBool'}}
        doc = {'spwids': spwids, 'fieldids': fieldids, 'obstime': obstime, 'mode': mode, 'nchan': nchan, 'start': start, 'width': width, 'phasec': phasec, 'restfreq': restfreq, 'outframe': outframe, 'veltype': veltype, 'verbose': verbose}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _cvelfreqs_result = self._swigobj.cvelfreqs(_pc.document['spwids'], _pc.document['fieldids'], _str_ec(_pc.document['obstime']), _str_ec(_pc.document['mode']), _pc.document['nchan'], _any_ec(_pc.document['start']), _any_ec(_pc.document['width']), _any_ec(_pc.document['phasec']), _any_ec(_pc.document['restfreq']), _str_ec(_pc.document['outframe']), _str_ec(_pc.document['veltype']), _pc.document['verbose'])
        return _cvelfreqs_result

    def contsub(self, outputms='', fitspw=[ ], fitorder=int(1), combine='', spw='*', unionspw='*', field='', scan='', intent='', correlation='', obs='', whichcol='CORRECTED_DATA'):
        """NOT FULLY IMPLEMENTED YET.  uvcontsub uses the cb tool for now.
        (The only reason to implement it in ms is to save time and disk
        space.)
        
        This function estimates the continuum emission of the MS and writes
        a MS with that estimate subtracted, using the ms tool.  The
        estimate is made, separately for the real and imaginary parts of
        each baseline, by fitting a low order polynomial to the unflagged
        visibilities selected by fitspw (depending on combine).
        
        """
        schema = {'outputms': {'type': 'cStr'}, 'fitspw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'fitorder': {'type': 'cInt'}, 'combine': {'type': 'cStr'}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'unionspw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'scan': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'intent': {'type': 'cStr'}, 'correlation': {'type': 'cStr'}, 'obs': {'type': 'cStr'}, 'whichcol': {'type': 'cStr'}}
        doc = {'outputms': outputms, 'fitspw': fitspw, 'fitorder': fitorder, 'combine': combine, 'spw': spw, 'unionspw': unionspw, 'field': field, 'scan': scan, 'intent': intent, 'correlation': correlation, 'obs': obs, 'whichcol': whichcol}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _contsub_result = self._swigobj.contsub(_str_ec(_pc.document['outputms']), _any_ec(_pc.document['fitspw']), _pc.document['fitorder'], _str_ec(_pc.document['combine']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['unionspw']), _any_ec(_pc.document['field']), _any_ec(_pc.document['scan']), _str_ec(_pc.document['intent']), _str_ec(_pc.document['correlation']), _str_ec(_pc.document['obs']), _str_ec(_pc.document['whichcol']))
        return _contsub_result

    def continuumsub(self, field=[ ], fitspw=[ ], spw=[ ], solint=[ ], fitorder=int(0), mode='subtract'):
        """This function provides a means of continuum determination and
        subtraction by fitting a polynomial of desired order to a subset of
        channels in each time-averaged uv spectrum.  The fit is used to
        model the continuum in all channels (not just those used in the
        fit), for subtraction, if desired.
        
        Use the fitspw parameter to limit the spectral windows processed and
        the range of channels used to estimate the continuum  in each (avoid
        channels containing spectral lines).
        
        The default solution interval 'int' will result in per-integration
        continuum fits for each baseline.
        
        The mode parameter indicates how the continuum model (the result of
        the fit) should be used:
        - 'subtract' will store the continuum model in the MODEL_DATA column
        and subtract it from the CORRECTED_DATA column
        - 'replace' will replace the CORRECTED_DATA column with the
        continuum model (useful if you want to image the continuum model
        result)
        - 'model' will only store the continuum model in the MODEL_DATA
        column (the CORRECTED_DATA is unaffected).
        
        It is important to start the ms tool with nomodify=False so that
        changes to the dataset will be allowed (see example below).  For
        now, the only way to recover the un-subtracted CORRECTED_DATA
        column is to use calibrater.correct() again.
        
        Note that the MODEL_DATA and CORRECTED_DATA columns must be present
        for continuumsub to work correctly.  The function will warn the
        user if they are not present, and abort.  To add these scratch
        columns, close the ms tool, then start a calibrater or an imager
        tool, which will add the scratch columns.  Then restart the ms
        tool, and try continuumsub again.
        
        """
        schema = {'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'fitspw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'solint': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'fitorder': {'type': 'cInt'}, 'mode': {'type': 'cStr'}}
        doc = {'field': field, 'fitspw': fitspw, 'spw': spw, 'solint': solint, 'fitorder': fitorder, 'mode': mode}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _continuumsub_result = self._swigobj.continuumsub(_any_ec(_pc.document['field']), _any_ec(_pc.document['fitspw']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['solint']), _pc.document['fitorder'], _str_ec(_pc.document['mode']))
        return _continuumsub_result

    def uvsub(self, reverse=False):
        """This function subtracts model visibility data from corrected
        visibility data leaving the residuals in the corrected data column.
        If the parameter reverse is set True, this process is reversed.
        
        """
        schema = {'reverse': {'type': 'cBool'}}
        doc = {'reverse': reverse}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _uvsub_result = self._swigobj.uvsub(_pc.document['reverse'])
        return _uvsub_result

    def addephemeris(self, id=int(-1), ephemerisname='', comment='', field=[ ]):
        """
        """
        schema = {'id': {'type': 'cInt'}, 'ephemerisname': {'type': 'cStr'}, 'comment': {'type': 'cStr'}, 'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}}
        doc = {'id': id, 'ephemerisname': ephemerisname, 'comment': comment, 'field': field}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _addephemeris_result = self._swigobj.addephemeris(_pc.document['id'], _str_ec(_pc.document['ephemerisname']), _str_ec(_pc.document['comment']), _any_ec(_pc.document['field']))
        return _addephemeris_result

    def timesort(self, newmsname=''):
        """This function sorts the main table of the measurement set by the
        contents of the column TIME in ascending order and writes a copy of
        the MS with the sorted main table into newmsfile.
        
        If no newmsname is given, a sorted copy of the MS is written into a
        new MS under the name x.sorted (where x is the name of the original
        MS). The original MS is then closed and deleted. The new MS is
        renamed to the name of the original MS and then reopened.
        
        """
        schema = {'newmsname': {'type': 'cStr'}}
        doc = {'newmsname': newmsname}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _timesort_result = self._swigobj.timesort(_str_ec(_pc.document['newmsname']))
        return _timesort_result

    def sort(self, newmsname='', columns=[  ]):
        """This function sorts the main table of the measurement set by the
        contents of the input set of columns in ascending order and writes
        a copy of the MS with the sorted main table into newmsfile.
        
        If no newmsname is given, a sorted copy of the MS is written into a
        new MS under the name x.sorted (where x is the name of the original
        MS). The original MS is then closed and deleted. The new MS is
        renamed to the name of the original MS and then reopened.
        
        """
        schema = {'newmsname': {'type': 'cStr'}, 'columns': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}}
        doc = {'newmsname': newmsname, 'columns': columns}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _sort_result = self._swigobj.sort(_str_ec(_pc.document['newmsname']), [_str_ec(_x) for _x in _pc.document['columns']])
        return _sort_result

    def iterinit(self, columns=[  ], interval=float(0.0), maxrows=int(0), adddefaultsortcolumns=True):
        """Specify the columns to iterate over and the time interval to use
        for the TIME column iteration.  The columns are specified by their
        MS column name and must contain scalar values.
        
        Note that the following default sort columns are always added to
        the specified columns: array_id, field_id, data_desc_id and time.
        This is so that the iterator can keep track of the coordinates
        associated with the data (field direction, frequency, etc.). If you
        want to sort on these columns last instead of first, you need to
        include them in the columns specified. If you don't want to sort on
        these columns at all, you can set adddefaultsortcolumns to False.
        
        You may want to use iteration for a large dataset. After calling
        iterinit, you must call iterorigin before attempting to retrieve
        data with getdata.
        
        You need to call selectinit before calling this.
        
        """
        schema = {'columns': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'interval': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'maxrows': {'type': 'cInt'}, 'adddefaultsortcolumns': {'type': 'cBool'}}
        doc = {'columns': columns, 'interval': interval, 'maxrows': maxrows, 'adddefaultsortcolumns': adddefaultsortcolumns}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _iterinit_result = self._swigobj.iterinit([_str_ec(_x) for _x in _pc.document['columns']], _pc.document['interval'], _pc.document['maxrows'], _pc.document['adddefaultsortcolumns'])
        return _iterinit_result

    def iterorigin(self):
        """Set or reset the iterator to the start of the currently specified
        iteration.  You need to call this after iterinit, before attempting
        to retrieve data with getdata. You may also use iterorigin to set
        the iterator back to the start before you reach the end of the
        data.
        
        """
        _iterorigin_result = self._swigobj.iterorigin()
        return _iterorigin_result

    def iternext(self):
        """This sets the currently selected table (as accessed with getdata)
        to the next iteration. If there is no more data, the function
        returns False and the selection is reset to that before the
        iteration started.  You need to call iterinit and iterorigin
        before calling this.
        
        """
        _iternext_result = self._swigobj.iternext()
        return _iternext_result

    def iterend(self):
        """This sets the currently selected table (as accessed with
        getdata) to the table that was selected before iteration
        started.  Use this to end the iteration prematurely. There is no
        need to call this if you continue iterating until iternext
        returns False.
        
        See the example below.
        
        """
        _iterend_result = self._swigobj.iterend()
        return _iterend_result

    def ngetdata(self, items=[  ], ifraxis=False, ifraxisgap=int(0), increment=int(1), average=False):
        """DEPRECATED: Please use the ms::getdata() function in place
        of ms::ngetdata().
        
        This method extracts the data as specified in the items
        parameter.  The data is returned as a record with each item
        as a separate key in the record (all in lower case).
        
        Unless the iterator was initialized with a niterinit(), this
        method initializes the iterator as niterinit([".."],0.0,0,False).
        
        """
        schema = {'items': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'ifraxis': {'type': 'cBool'}, 'ifraxisgap': {'type': 'cInt'}, 'increment': {'type': 'cInt'}, 'average': {'type': 'cBool'}}
        doc = {'items': items, 'ifraxis': ifraxis, 'ifraxisgap': ifraxisgap, 'increment': increment, 'average': average}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _ngetdata_result = _dict_dc(self._swigobj.ngetdata([_str_ec(_x) for _x in _pc.document['items']], _pc.document['ifraxis'], _pc.document['ifraxisgap'], _pc.document['increment'], _pc.document['average']))
        return _ngetdata_result

    def niterinit(self, columns=[  ], interval=float(0.0), maxrows=int(0), adddefaultsortcolumns=True):
        """DEPRECATED: Please use the ms::iterinit() function in place
        of ms::niterinit().
        
        """
        schema = {'columns': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'interval': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'maxrows': {'type': 'cInt'}, 'adddefaultsortcolumns': {'type': 'cBool'}}
        doc = {'columns': columns, 'interval': interval, 'maxrows': maxrows, 'adddefaultsortcolumns': adddefaultsortcolumns}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _niterinit_result = self._swigobj.niterinit([_str_ec(_x) for _x in _pc.document['columns']], _pc.document['interval'], _pc.document['maxrows'], _pc.document['adddefaultsortcolumns'])
        return _niterinit_result

    def niterorigin(self):
        """DEPRECATED: Please use the ms::iterorigin() function in place
        of ms::niterorigin().
        
        Set or reset the iterator to the start of the currently
        specified iteration. You need to call this before attempting to
        iteratively retrieve data with ngetdata. You can set the
        iteration back to the start before you reach the end of the
        data.  You need to call iterinit before calling this.  See the
        example below.
        
        """
        _niterorigin_result = self._swigobj.niterorigin()
        return _niterorigin_result

    def niternext(self):
        """DEPRECATED: Please use the ms::iternext() function in place
        of ms::niternext().
        
        This sets the currently selected table (as accessed with
        ngetdata) to the next iteration. If there is no more data, the
        function returns False.  You need to call iterinit and
        iterorigin before calling this.  See the example below.
        
        """
        _niternext_result = self._swigobj.niternext()
        return _niternext_result

    def niterend(self):
        """DEPRECATED: Please use the ms::iterend() function in place
        of ms::niterend().
        
        The serves redundant purpose and is here only for backward
        compatibility.
        
        This method returns True if there are no more iterations left.
        I.e., the iterations have ended.  This same information is also
        returned by niternext().
        
        With the use of the VisibilityIterator in the niterinit(),
        niterorigin(), niternext() methods, the iterator is set to the
        original state by calling niterinit() at any time.
        
        See the example below.
        
        """
        _niterend_result = self._swigobj.niterend()
        return _niterend_result

    def nrowold(self, selected=False):
        """DEPRECATED: Please use the ms::nrow() function in place of
        ms::nrowold().
        
        This function returns the number of rows in the measurement
        set. If the optional argument selected is set to True, it returns
        the number of currently selected rows, otherwise it returns the
        the number of rows in the original measurement.
        
        """
        schema = {'selected': {'type': 'cBool'}}
        doc = {'selected': selected}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _nrowold_result = self._swigobj.nrowold(_pc.document['selected'])
        return _nrowold_result

    def rangeold(self, items=[  ], useflags=True, blocksize=int(10)):
        """DEPRECATED: Please use the ms::range() function in place of
        ms::rangeold().
        
        This function will return the range of values in the currently
        selected measurement set for the items specified.  Possible items
        include most scalar columns, interferometer number
        (1000*antenna1+antenna2), uvdist(ance), u, v, w, amplitude, phase,
        real and imaginary components of the data (and corrected and model
        versions of these - if these columns are present). See the table
        at the top of the document to find the exact list.
        
        You specify items in which you are interested using a string
        vector where each element is a case insensitive item name.  This
        function will then return a record that has fields corresponding
        to each of the specified items. Each field will contain the range
        of the specified item. For many items the range will be the
        minimum and maximum values but for some it will be a list of
        unique values. Unrecognized items are ignored.
        
        By default the FLAG column is used to exclude flagged data before
        any ranges are determined, but you can set useflags=False to
        include flagged data in the range.  However, if you average in
        frequency, flagging will still be applied.
        
        You can influence the memory use and the reading speed using
        the blocksize argument - it specifies how big a block of data
        to read at once (in MB). For large datasets on machines with lots
        of memory you may speed things up by setting this higher than the
        default (10 MB).
        
        For some items, you need to call selectinitold to select a portion
        of the data with a unique shape prior to calling this function.
        
        Items prefixed with corrected, model, residual or obs_residual
        are not available unless your measurement set has been processed
        either with the imager or calibrator tools.
        
        """
        schema = {'items': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'useflags': {'type': 'cBool'}, 'blocksize': {'type': 'cInt'}}
        doc = {'items': items, 'useflags': useflags, 'blocksize': blocksize}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _rangeold_result = _dict_dc(self._swigobj.rangeold([_str_ec(_x) for _x in _pc.document['items']], _pc.document['useflags'], _pc.document['blocksize']))
        return _rangeold_result

    def selectinitold(self, datadescid=int(0), reset=False):
        """DEPRECATED: Please use the ms::selectinit() function in place of
        ms::selectinitold().
        
        A measurement set can contain data with a variety of different
        shapes (as described in the overall description to this tool).  To
        allow functions to return data in fixed shape arrays you need to
        select, using this function, rows that contain the same data
        shape. You do not need to use this function if all the data in
        your measurement set has only one shape.
        
        The DATA_DESC_ID column in the measurement set contains a
        value that maps to a particular row in the POLARIZATION and
        SPECTRAL_WINDOW subtables. Hence all rows with the same
        value in the DATA_DESC_ID column must have the same data
        shape. To select all the data where the DATA_DESC_ID value
        is $N$ you call this function with the datadescid argument set to
        $N$.
        
        It is possible to have a measurement set with differing values in
        the DATA_DESC_ID column but where all the data is a fixed
        shape. For example this will occur if the reference frequency
        changes but the number of spectral channels is fixed. In cases
        like this all the data can be selected, using this function with
        an argument of zero.  If the data shape does change and you call
        this function with an datadescid set to zero  the return value will be False. In all other cases it
        will be True.
        
        To return to the completely unselected measurement set, set the
        reset argument to True. This will allow you to access the full
        range of rows in the measurement set, rather than just the
        selected measurement set.
        
        The datadescid must always be a non-negative integer.
        
        """
        schema = {'datadescid': {'type': 'cInt'}, 'reset': {'type': 'cBool'}}
        doc = {'datadescid': datadescid, 'reset': reset}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectinitold_result = self._swigobj.selectinitold(_pc.document['datadescid'], _pc.document['reset'])
        return _selectinitold_result

    def selectold(self, items={ }):
        """DEPRECATED: Please use the ms::select() function in place of
        ms::selectold().
        
        This function will select a subset of the current measurement set
        based on the range of values for each field in the input record.
        The rangeold function will return a record that can be altered and
        used as the argument for this function.  A successful selection
        returns True. Unrecognized fields are ignored.
        
        Allowable items for selectold include:  antenna1, antenna2,
        array_id, feed1, feed2, field_id, ifr_number, rows, scan_number,
        data_desc_id, time, times, u, v, w, and uvdist.
        
        You need to call selectinitold before
        calling this function. If you haven't then selectinitold will be
        called for you with default arguments.
        
        Repeated use of this function, with different arguments, will
        further refine the selection, resulting in a successively smaller
        selected measurement set. If the selected measurement set does not
        contain any rows then this function will return False and send a
        warning message in the logger. Otherwise this function will return
        True. To undo all the selections you need to use the selectinitold
        function (with reset=True).
        
        """
        schema = {'items': {'type': 'cDict'}}
        doc = {'items': items}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectold_result = self._swigobj.selectold(_dict_ec(_pc.document['items']))
        return _selectold_result

    def selecttaqlold(self, msselect=''):
        """DEPRECATED: Please use the ms::selecttaql() function in place of
        ms::selecttaqlold().
        
        This function will select a subset of the current measurement set
        based on the standard TaQL selection string given.
        
        Repeated use of this function, with different arguments, will
        further refine the selection, resulting in a successively smaller
        selected measurement set. If the selected measurement set does not
        contain any rows then this function will return False and send a
        warning message in the logger. Otherwise this function will return
        True. To undo all the selections you need to use the selectinitold
        function (with reset=True). Note that index values used in the TaQL
        string are zero-based as are all tool indices.
        
        
        """
        schema = {'msselect': {'type': 'cStr'}}
        doc = {'msselect': msselect}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selecttaqlold_result = self._swigobj.selecttaqlold(_str_ec(_pc.document['msselect']))
        return _selecttaqlold_result

    def selectchannelold(self, nchan=int(1), start=int(0), width=int(1), inc=int(1)):
        """DEPRECATED: Please use the ms::selectchannel() function in place of
        ms::selectchannelold().
        
        This function allows you to select a subset of the frequency
        channels in the current measurement set.  This function can also
        average, over frequency channels, prior to providing the values to
        the user.
        
        Selection on channels is not allowed using either the select or
        command functions as they can only select entire rows in a
        measurement set. Channel selection involves accessing only some of
        the values in a row. Like all the selection functions this
        function does not change the current measurement but updates the
        measurement set selection parameters so that functions like
        getdataold will return the desired subset of the data.  Repeated
        use of this function will overwrite any previous channel selection.
        
        There are four parameters, the number of output channels, the
        first input channel to use, the number of input channels to
        average into one output channel, and the increment in the input
        spectrum for the next output channel. All four parameters need to
        be specified.
        
        When all data to be averaged is unflagged, the result is the
        averaged value and the corresponding flag is False.  When all data
        is flagged, the result is set to zero and the corresponding flag is
        True.  When data to be averaged is mixed (unflagged and flagged),
        only the unflagged values are averaged and the flag is set to
        False.
        
        This function return True if the selection was successful, and
        False if not. In the latter case an error message will also be sent
        to the logger.
        
        You need to call selectinitold before
        calling this function. If you haven't then selectinitold will be
        called for you with default arguments.
        
        """
        schema = {'nchan': {'type': 'cInt'}, 'start': {'type': 'cInt'}, 'width': {'type': 'cInt'}, 'inc': {'type': 'cInt'}}
        doc = {'nchan': nchan, 'start': start, 'width': width, 'inc': inc}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectchannelold_result = self._swigobj.selectchannelold(_pc.document['nchan'], _pc.document['start'], _pc.document['width'], _pc.document['inc'])
        return _selectchannelold_result

    def selectpolarizationold(self, wantedpol=[  ]):
        """DEPRECATED: Please use the ms::selectpolarization() function in
        place of ms::selectpolarizationold().
        
        This function allows you to select a subset of the polarizations
        in the current measurement set.  This function can also setup
        conversion to different polarization representations.
        
        You specify the polarizations using a string vector. Allowable
        strings are include I, Q, U, V, RR, RL, LR, LL, XX, YY, XY,
        YX. These string must be specified in upper case. If the
        polarizations match those present in the measurement set they will
        be selected directly, otherwise all polarizations are read and
        then a conversion step is done. If the conversion cannot be done
        then an error will be produced when you try to access the data.
        
        This function return True if the selection was successful, and
        False if not.
        
        You need to call selectinitold before
        calling this function. If you haven't then selectinitold will be
        called for you with default arguments.
        
        """
        schema = {'wantedpol': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}}
        doc = {'wantedpol': wantedpol}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _selectpolarizationold_result = self._swigobj.selectpolarizationold([_str_ec(_x) for _x in _pc.document['wantedpol']])
        return _selectpolarizationold_result

    def getdataold(self, items=[  ], ifraxis=False, ifraxisgap=int(0), increment=int(1), average=False):
        """DEPRECATED: Please use the ms::getdata() function in place
        of ms::getdataold().
        
        This function will read the specified items from the currently
        selected measurement set and returns them in fields of a record.
        The main difference between this and direct access of the table,
        using the table tool, is that this function reads data from the
        selected measurement set, it provides access to derived
        quantities like amplitude and flag_sum and it can reorder the
        data.
        
        The items to read are specified, as with the rangeold function,
        using a vector of strings. Allowable items include:  amplitude,
        corrected_amplitude, model_amplitude, ratio_amplitude,
        residual_amplitude, obs_residual_amplitude, antenna1, antenna2,
        axis_info, data, corrected_data, model_data, ratio_data,
        residual_data, obs_residual_data, feed1, feed2, field_id, flag,
        flag_row, flag_sum, ha (added to axis_info), ifr_number, imaginary,
        corrected_imaginary, model_imaginary, ratio_imaginary,
        residual_imaginary, obs_residual_imaginary, last (added to
        axis_info), phase, corrected_phase, model_phase, ratio_phase,
        residual_phase, obs_residual_phase, real, corrected_real,
        ratio_real, residual_real, obs_residual_real, scan_number, sigma,
        data_desc_id, time, ut (added to axis_info), uvw, u, v, w, uvdist,
        and weight. Unrecognized items will result in a warning being sent
        to the logger.  Duplicate items are silently ignored.
        
        The record that is returned contains fields that correspond to
        each of the specified items. Most fields will contain an array. The
        array may be one, two or three dimensional depending on whether the
        corresponding row in the measurement set is a scalar, one or two
        dimensional. Unless the ifraxis argument is set to True the length
        of the last axis on these arrays will correspond to the number of
        rows in the selected measurement set.
        
        If the ifraxis argument is set to True, the row axis is split into
        an interferometer axis and a time axis. For example a measurement
        set with 90 rows, in an array with 6 telescopes (so that there are
        15 interferometers), may have a data array of shape [4,32,90]
        if ifraxis is False or [4,32,15,6], if ifraxis is True (assuming
        there are 4 correlations and 32 channels). If there are missing
        rows as will happen if not all interferometers where used for all
        time-slots then a default value will be inserted.
        
        This splitting of the row axis may not happen for items where
        there is only a single value per row. For some items the returned
        vector will contain only as many values as there are
        interferometers and it is implicit that the same value should be
        used for all time slots. The antenna1, antenna2, feed1, feed2, and
        ifr_number items fall in this category. For other items the
        returned vector will have as many values as there are time slots
        and it is implicit that the same value should be used for all
        interefometers. The field_id, scan_number, data_desc_id, and
        time items fall into this category.
        
        The axis_info item provides data labelling information. It
        returns a record with the following fields: corr_axis, freq_axis,
        ifr_axis and time_axis. The latter two fields are not present if
        ifr_axis is set to False. The corr_axis field contains a string
        vector with elements like 'RR' or 'XY' that indicates which
        polarizations where correlated together to produce the data. The
        length of this vector will always be the same as the length of the
        first axis of the data array. The freq_axis field contains a record
        with two fields, chan_freq and resolution. Each of these fields
        contains vectors which indicate the centre frequency and spectral
        resolution (FWHM) of each channel. The length of these vectors will
        be the same as the length of the second axis in the data. The
        ifr_axis field contains fields: ifr_number, ifr_name,
        ifr_shortname, and baseline. The ifr_number is the same as returned
        by the ifr_item, the ifr_name and ifr_shortname are string vecors
        containing descriptions of the interferometer and the baseline is
        the Euclidian distance, in meters between the two antennas. All of
        these vectors have a length equal to the number of interferometers
        in the selected measurement set ie., to the length of the third
        axis in the data when ifraxis is True. The time_axis field contains
        the MJD seconds field and optionally the HA, UT, and LAST fields.
        To include the optional fields you need to add the ha, last or ut
        strings to the list of requested items. All the fields in the
        time_axis record contain vectors that indicate the time at the
        midpoint of the observation and are in seconds. The MJD seconds
        field is since 0 hours on the day having a modified julian day
        number of zero and the rest are since midnight prior to the start
        of the observation.
        
        An optional gap size can be specified to visually separate groups
        of interferometers with the same antenna1 index (handy for
        identifying antennas in an interferometer vs time display). The
        default is no gap.
        
        An optional increment can be specified to return data from every
        row matching the increment only.
        
        When the average flag is set, the data will be averaged over the
        time axis if the ifraxis is True or the row axis i.e., different
        interferometers and times may be averaged together. In the latter
        case, some of the coordinate information, like antenna_id, will
        no longer make sense.  When all data to be averaged is unflagged,
        the result is the averaged value and the corresponding flag is
        False. When all data is flagged, the result is set to zero and the
        corresponding flag is True.  When data to be averaged is mixed
        (unflagged and flagged), only the unflagged values are averaged and
        the flag is set to False.
        
        You need to call selectinitold before
        calling this function. If you haven't then selectinitold will be
        called for you with default arguments.
        
        Items prefixed with corrected, model, residual or obs_residual are
        not available unless your measurement set has been processed either
        with the imager or calibrator tools.
        
        """
        schema = {'items': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'ifraxis': {'type': 'cBool'}, 'ifraxisgap': {'type': 'cInt'}, 'increment': {'type': 'cInt'}, 'average': {'type': 'cBool'}}
        doc = {'items': items, 'ifraxis': ifraxis, 'ifraxisgap': ifraxisgap, 'increment': increment, 'average': average}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _getdataold_result = _dict_dc(self._swigobj.getdataold([_str_ec(_x) for _x in _pc.document['items']], _pc.document['ifraxis'], _pc.document['ifraxisgap'], _pc.document['increment'], _pc.document['average']))
        return _getdataold_result

    def putdataold(self, items={ }):
        """DEPRECATED: Please use the ms::putdata() function in place
        of ms::putdataold().
        
        This function allows you to write values from casapy variables back
        into the measurement set table. The main difference between this
        and directly accessing the table using the table tool is that this
        function writes data to the selected measurement set.
        
        Unlike the getdataold function you can only put items that
        correspond to actual table columns. You cannot change the data
        shape either so that the number of correlations, channels and rows
        (or intereferometers/time slots) must match the values in the
        selected measurement set. If the values were obtained using the
        getdataold function with ifraxis argument set to True, then any
        default values added to fill in missing intereferometer/timeslots
        pairs will be ignored when writing the modified values back using
        this function.
        
        Allowable items include:  data, corrected_data, model_data, flag,
        flag_row, sigma, and weight.
        
        The measurement set has to be opened for read/write access to be
        able to use this function.
        
        You need to call selectinitold before
        calling this function. If you haven't then selectinitold will be
        called for you with default arguments.
        
        Items prefixed with corrected, model, residual or obs_residual are
        not available unless your measurement set has been processed either
        with the imager or calibrator tools.
        
        """
        schema = {'items': {'type': 'cDict'}}
        doc = {'items': items}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _putdataold_result = self._swigobj.putdataold(_dict_ec(_pc.document['items']))
        return _putdataold_result

    def iterinitold(self, columns=[  ], interval=float(0.0), maxrows=int(0), adddefaultsortcolumns=True):
        """DEPRECATED: Please use the ms::iterinit() function in place
        of ms::iterinitold().
        
        Specify the columns to iterate over and the time interval to use
        for the TIME column iteration.  The columns are specified by their
        MS column name and must contain scalar values.
        
        Note that the following columns are always added to the specified
        columns: array_id, field_id, data_desc_id and time. This is so that
        the iterator can keep track of the coordinates associated with the
        data (field direction, frequency, etc.). If you want to sort on
        these columns last instead of first, you need to include them in
        the columns specified. If you don't want to sort on these columns
        at all, you can set adddefaultsortcolumns to False.
        
        You may want to use iteration for a large dataset. After calling
        iterinitold, you must call iteroriginold before attempting to
        retrieve data with getdataold.
        
        You need to call selectinitold before calling this.
        
        """
        schema = {'columns': {'type': 'cStrVec', 'coerce': [_coerce.to_list,_coerce.to_strvec]}, 'interval': {'type': 'cFloat', 'coerce': _coerce.to_float}, 'maxrows': {'type': 'cInt'}, 'adddefaultsortcolumns': {'type': 'cBool'}}
        doc = {'columns': columns, 'interval': interval, 'maxrows': maxrows, 'adddefaultsortcolumns': adddefaultsortcolumns}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _iterinitold_result = self._swigobj.iterinitold([_str_ec(_x) for _x in _pc.document['columns']], _pc.document['interval'], _pc.document['maxrows'], _pc.document['adddefaultsortcolumns'])
        return _iterinitold_result

    def iteroriginold(self):
        """DEPRECATED: Please use the ms::iterorigin() function in place
        of ms::iteroriginold().
        
        Set or reset the iterator to the start of the currently specified
        iteration. You need to call this after iterinitold, before
        attempting to retrieve data with getdataold. You may also use
        iteroriginold to set the iteration back to the start before you
        reach the end of the data.
        
        """
        _iteroriginold_result = self._swigobj.iteroriginold()
        return _iteroriginold_result

    def iternextold(self):
        """DEPRECATED: Please use the ms::iternext() function in place
        of ms::iternextold().
        
        This sets the currently selected table (as accessed with
        getdataold) to the next iteration. If there is no more data, the
        function returns False and the selection is reset to that before
        the iteration started.  You need to call iterinitold and
        iteroriginold before calling this.
        
        """
        _iternextold_result = self._swigobj.iternextold()
        return _iternextold_result

    def iterendold(self):
        """DEPRECATED: Please use the ms::iterend() function in place
        of ms::iterendold().
        
        This sets the currently selected table (as accessed with
        getdataold) to the table that was selected
        before iteration started.  Use this to end the iteration
        prematurely.  There is no need to call this if you continue
        iterating until iternextold returns False.
        
        See the example below.
        
        """
        _iterendold_result = self._swigobj.iterendold()
        return _iterendold_result

    def continuumsubold(self, field=[ ], fitspw=[ ], spw=[ ], solint=[ ], fitorder=int(0), mode='subtract'):
        """DEPRECATED: Please use the ms::continuumsub() function in place
        of ms::continuumsubold().
        
        This function provides a means of continuum determination and
        subtraction by fitting a polynomial of desired order to a subset
        of channels in each time-averaged uv spectrum.  The fit is used
        to model the continuum in all channels (not just those used in
        the fit), for subtraction, if desired.  Use the fitspw parameter
        to limit the spectral windows processed and the range of channels
        used to estimate the continuum  in each (avoid channels
        containing spectral lines).  The default solution interval 'int'
        will result in per-integration continuum fits for each baseline.
        The mode parameter indicates how the continuum model (the result
        of the fit) should be used: 'subtract' will store the continuum
        model in the MODEL_DATA column and subtract it from the
        CORRECTED_DATA column; 'replace' will replace the CORRECTED_DATA
        column with the continuum model (useful if you want to image the
        continuum model result); and 'model' will only store the
        continuum model in the MODEL_DATA column (the CORRECTED_DATA is
        unaffected).
        
        It is important to open the dataset with nomodify=False so that
        changes will be allowed (see example below).
        
        For now, the only way to recover the un-subtracted CORRECTED_DATA
        column is to use calibrater.correct() again.
        
        Note that the MODEL_DATA and CORRECTED_DATA columns must be
        present for continuumsubold to work correctly.  The function will
        warn the user if they are not present, and abort.  To add these
        scratch columns, close the ms tool, then start a calibrater or an
        imager tool, which will add the scratch columns.  Then restart
        the ms tool, and try continuumsubold again.
        
        """
        schema = {'field': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'fitspw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'spw': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'solint': {'type': 'cVariant', 'coerce': [_coerce.to_variant]}, 'fitorder': {'type': 'cInt'}, 'mode': {'type': 'cStr'}}
        doc = {'field': field, 'fitspw': fitspw, 'spw': spw, 'solint': solint, 'fitorder': fitorder, 'mode': mode}
        assert _pc.validate(doc,schema), str(_pc.errors)
        _continuumsubold_result = self._swigobj.continuumsubold(_any_ec(_pc.document['field']), _any_ec(_pc.document['fitspw']), _any_ec(_pc.document['spw']), _any_ec(_pc.document['solint']), _pc.document['fitorder'], _str_ec(_pc.document['mode']))
        return _continuumsubold_result

