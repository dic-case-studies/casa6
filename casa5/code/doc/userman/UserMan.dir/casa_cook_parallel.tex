%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DP 2012-09-26  First version
% DP 2012-10-15  CASA 4.0 version
% DP 2013-04-26  CASA 4.1 version

\chapter{Parallel Processing in CASA}
\label{chapter:parallel}

Since CASA 4.0.0, a {\it parallelized} execution of a full data analysis from data import
to imaging is possible.
This functionality continues to be under development and should still be regarded as experimental
but users are encouraged to try this feature if they have access to computers with
a solid state disk (SSD) or RAID arrays with a Lustre file system or other fast
file systems. On normal SATA disks, there is no benefit from parallelization as
the file system cannot feed more than one CASA instance.

The following deal with the handling of visibility data. For imaging
with {\tt pclean}, see \S\,\ref{section:im.pclean}).

\section{The CASA parallelization scheme}

In order to run {\it one} analysis on multiple processors, one can parallelize the
work by dividing the data into several parts (``partitioning'') and then
run a CASA instance on each part {\it or} have non-trivially parallelized
algorithms which make use of several processors within a single CASA instance.
Non-trivial parallelization is presently only implemented in certain sections of 
the imaging code of CASA based on OpenMP.

All other parallelization is achieved by partitioning the MeasurementSet (MS) of
interest using the task {\tt partition}. The resulting partitioned MS is called
a ``multi-MS'' or ``MMS''. Logically, an MMS has the same structure as an MS but
internally it is a group of several MSs which are virtually concatenated.
Virtual concatenation of several MSs or MMSs into an MMS can also be 
achieved via the new (in CASA 4.0) task {\tt virtualconcat}.

Due to the virtual concatenation, the main table of an MMS appears like
the union of the main tables of all the member MSs such that when the MMS
is accessed like a normal MS, processing can proceed sequentially as usual.
Each member MS or ``subMS'' of an MMS, however, is at the same time a valid MS on its own 
and can be processed as such. This is what happens when the MMS is accessed
by a parallelized task. The partitioning of the MMS is recognized and work
is started in parallel on the separate subMSs. 

The internal structure of an MMS can be inspected using the new task {\tt listpartition}.

\section{Multi-MS-compatible tasks in CASA 4.1}

The following tasks in CASA 4.1 have been tested to work with MMSs as input:
\begin{itemize}
\item {\tt applycal}
\item {\tt bandpass}
\item {\tt clean}
\item {\tt concat} (produces an output MS, not an MMS)
\item {\tt fixplanets}
\item {\tt flagdata}
\item {\tt flagmanager}
\item {\tt fluxscale}
\item {\tt gaincal}
\item {\tt gencal}
\item {\tt listobs}
\item {\tt listpartition}
\item {\tt listvis}
\item {\tt listhistory}
\item {\tt partition} (repartitioning of an MMS is also possible)
\item {\tt pclean}
\item {\tt plotms}
\item {\tt setjy}
\item {\tt split} (produces MS by default, an MMS if parameter keepmms=True)
\item {\tt uvcontsub} (produces an MMS)
\item {\tt virtualconcat} (produces an MMS)
\item {\tt vishead}
\item {\tt wvrgcal}
\end{itemize}
Of these, the following tasks will work in a parallelized way on MMSs
to speed up processing:
\begin{itemize}
\item {\tt applycal}
\item {\tt flagdata}
\item {\tt partition} (repartitioning of an MMS is also possible)
\item {\tt pclean}
\item {\tt setjy} (when parameter usescratch=True)
\item {\tt split} (when parameter keepmms=True)
\item {\tt uvcontsub}
\end{itemize}
You can find an example of a parallelized analysis in the regression script
\begin{center}
{\tt alma-m100-analysis-hpc-regression.py} 
\end{center}
in a subdirectory of your CASA distribution.

\section{Parallelization control}

\subsection{Requirements}

The following requirements are necessary for all the nodes to be included in the cluster:

\begin{itemize}

\item Password-less ssh access from the controller (user) machine into all the
 hosts to be included in the cluster 

{NOTE:} This is not necessary when using only {\tt localhost}, i.e. if 
the cluster is deployed only on the machine where {\tt casapy} is running.

\item All the input files must be located in a shared file-system, accessible from all the 
nodes comprising the cluster, and mounted in the same path of the file-system

\item Mirrored CASA installation w.r.t. the CASA installation in the controller (user) machine, 
so that the following environmental variables are pointing to valid installations: {\tt PATH, 
LD\_LIBRARY\_PATH, IPYTHONDIR, CASAPATH, CASAARCH, PYTHONHOME, \_\_CASAPY\_PYTHONDIR, 
PGPLOT\_DEV, PGPLOT\_DIR, PGPLOT\_FONT}.

\end{itemize}

\subsection{Configuration and Start-Up}

The ``cluster'', i.e. the collection of CASA instances which will run the 
jobs from parallelized tasks, is set up automatically when it is used the first time,
typically when running {\tt partition}.
The setup of this default cluster is derived from the properties of the host
on which CASA is running. Presently the settings are such that up to 90\% of
the processors and the available RAM is used. There has to be at least 512~MB per CASA engine.
But even if there are not enough resources available, at least one engine is deployed.
In that case, the cluster is bypassed and jobs are simply run sequentially. 

If the user wants to override these settings, this is possible by creating
a ``cluster configuration file'' with one line per host to be used in the
following format:

{\small
\begin{verbatim} 
<hostname>, <number of engines>, <work directory>, <RAM usage>, <RAM per engine> 
\end{verbatim}
}

The comma-separated parameters have the following meaning:

\begin{enumerate}

\item {\tt hostname}: Hostname of the target node where the cluster is deployed

{\bf NOTE:} The hostname has to be provided w/o quotes

\item {\tt number of engines}: Supports in turns 3 different formats
\begin{itemize}
\item  If provided as an integer >1: It is interpreted as the actual user-specified 
maximum number of engines
\item  If provided as an integer =0: It will deploy as maximum engines as possible 
according to the idle CPU capacity available at the target node
\item  If provided as a float between 0 and 1: It is interpreted as the percentage 
of idle CPU capacity that the cluster can use in total at the target node
\end{itemize}


\item {\tt work directory}: Area in which the cluster will put intermediate files such as log files, configuration files, and monitoring files 

{\bf NOTE1:} This area has to be accessible from the controller (user) machine, and mounted in the same path of the filesystem

{\bf NOTE2:} The path name has to be provided w/o quotes

\item {\tt RAM usage}: (optional) can be given in three different formats 
\begin{itemize}
\item  If provided as an integer =0 or not at all (default): will deploy as many engines as possible using up to 90\% of the free RAM available at target node 
\item  If provided as an integer $>$1: interpreted as the actual user-specified maximum amount of RAM (MB) to be used in total at the target node
\item  If provided as a float between 0 and 1: interpreted as the percentage of free RAM that the cluster can use in total at the target node
\end{itemize}


\item {\tt RAM per engine}: (optional) integer, the required memory per engine in MB (default is 512MB) 

\end{enumerate}
It is also possible to add comments, by using the \# character at the beginning of the line. 

\vspace{2mm}

\noindent
Example:

\begin{center}
\begin{verbatim}
# CASA cluster configuration file
orion, 10, /home/jdoe/test/myclusterhome1
m42, 4, /home/jdoe/test/myclusterhome2, 0.6, 1024
antares, 0.6, /home/jdoe/test/myclusterhome3, 0, 2048
\end{verbatim}
\end{center}
Will set up a cluster comprised of three nodes, deploying the engines per node as follows:

\begin{itemize}
\item At host ``orion'' up to 10 engines will be deployed with working
  directory\\ /home/jdoe/test/myclusterhome1 and using as much free RAM
  available as possible (up to 90\% by default), taking into account
  that each engine can use up to 512~MB (the default and minimum)

\item At host ``m42'': It will deploy up to 4 engines, with working
  directory\\ /home/jdoe/test/myclusterhome2, and using at the most 60\%
  of the free RAM available, taking into account that each engine can
  use up to 1024~MB.

\item At host ``antares'': It will deploy as many engines as possible,
  with working directory\\ /home/jdoe/test/myclusterhome3, using up to
  60\% of the idle CPU capacity / cores, and as much free RAM
  available as possible (up to 90\% by default), taking into account
  that each engine can use up to 2048~MB.
\end{itemize}

Using such a configuration file named, say, ``cluster-config.txt'', the cluster can then
be created after CASA startup using the commands
\begin{verbatim}
from simple_cluster import *
sc = simple_cluster()
sc.init_cluster('cluster-config.txt', 'mycluster')
\end{verbatim}
It will be used by all subsequent calls to parallelized tasks.

To stop an existing cluster without exiting casa type
\begin{verbatim}
sc.stop_cluster()
\end{verbatim}
Otherwise, any cluster will be terminated anyway when CASA exits.

\subsection{Monitoring}

The CASA cluster framework comes with a monitoring service that produces a monitoring.log file, in the same directory where 
the controller instance (user CASA terminal) is started.

This file is updated whilst the cluster is actively carrying out tasks, and shows information per engine, and total per 
host included in the cluster.

Therefore, it is possible to monitor the cluster by screening this file in a separated terminal, e.g. using the 
operating system command {\tt watch}:

\begin{verbatim} 
          watch cat monitoring.log
\end{verbatim}
will result in the screen output
{\scriptsize
\begin{verbatim}
Host  Engine    Status CPU[%] Memory[%] Time[s] Read[MB] Write[MB] Read[MB/s] Write[MB/s]   Job     Sub-MS
hpc01      0   running     75        2      28       334         2         11           0 flagdata X54.0005.ms
hpc01      1   running     61        2       9       197         0         21           0 flagdata X54.0007.ms
hpc01      2   running     74        2      28       386         3         13           0 flagdata X54.0003.ms
hpc01      3 scheduled     24        1       0         1         0          0           0 flagdata X54.0012.ms
hpc01      4   running     82        2      28       386         3         13           0 flagdata X54.0001.ms
hpc01      5 scheduled     21        1       0        13         0          0           0 flagdata X54.0011.ms
==============================================================================================================
hpc01  Total              337       10              1318         8          61          0
==============================================================================================================

\end{verbatim}
}

